{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/links-to-official-docs.md",
      "references/production-patterns.md",
      "references/providers-quickstart.md",
      "references/top-errors.md",
      "references/v5-breaking-changes.md",
      "VERIFICATION_REPORT.md"
    ]
  },
  "content": "Production-ready backend AI with Vercel AI SDK v5.\r\n\r\n\r\n### Installation\r\n\r\n```bash\r\nnpm install ai\r\n\r\nnpm install @ai-sdk/openai       # OpenAI (GPT-5, GPT-4, GPT-3.5)\r\nnpm install @ai-sdk/anthropic    # Anthropic (Claude Sonnet 4.5, Opus 4, Haiku 4)\r\nnpm install @ai-sdk/google       # Google (Gemini 2.5 Pro/Flash/Lite)\r\nnpm install workers-ai-provider  # Cloudflare Workers AI\r\n\r\nnpm install zod\r\n```\r\n\r\n### Environment Variables\r\n\r\n```bash\r\n\r\n### Performance\r\n\r\n**1. Always use streaming for long-form content:**\r\n```typescript\r\n// User-facing: Use streamText\r\nconst stream = streamText({ model: openai('gpt-4'), prompt: 'Long essay' });\r\nreturn stream.toDataStreamResponse();\r\n\r\n// Background tasks: Use generateText\r\nconst result = await generateText({ model: openai('gpt-4'), prompt: 'Analyze data' });\r\n```\r\n\r\n**2. Set appropriate maxOutputTokens:**\r\n```typescript\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  prompt: 'Short answer',\r\n  maxOutputTokens: 100,  // Limit tokens to save cost\r\n});\r\n```\r\n\r\n**3. Cache provider instances:**\r\n```typescript\r\n// Good: Reuse provider instances\r\nconst gpt4 = openai('gpt-4-turbo');\r\nconst result1 = await generateText({ model: gpt4, prompt: 'Hello' });\r\nconst result2 = await generateText({ model: gpt4, prompt: 'World' });\r\n```\r\n\r\n**4. Optimize Zod schemas:**\r\n```typescript\r\n// Avoid complex nested schemas at top level in Workers\r\n// Move into route handlers to prevent startup overhead\r\n```\r\n\r\n### Error Handling\r\n\r\n**1. Wrap all AI calls in try-catch:**\r\n```typescript\r\ntry {\r\n  const result = await generateText({ /* ... */ });\r\n} catch (error) {\r\n  // Handle specific errors\r\n  if (error instanceof AI_APICallError) { /* ... */ }\r\n  else if (error instanceof AI_NoContentGeneratedError) { /* ... */ }\r\n  else { /* ... */ }\r\n}\r\n```\r\n\r\n**2. Implement retry logic:**\r\n```typescript\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  prompt: 'Hello',\r\n  maxRetries: 3,\r\n});\r\n```\r\n\r\n**3. Log errors properly:**\r\n```typescript\r\nconsole.error('AI SDK Error:', {\r\n  type: error.constructor.name,\r\n  message: error.message,\r\n  statusCode: error.statusCode,\r\n  timestamp: new Date().toISOString(),\r\n});\r\n```\r\n\r\n### Cost Optimization\r\n\r\n**1. Choose appropriate models:**\r\n```typescript\r\n// Simple tasks: Use cheaper models\r\nconst simple = await generateText({ model: openai('gpt-3.5-turbo'), prompt: 'Hello' });\r\n\r\n// Complex reasoning: Use GPT-4\r\nconst complex = await generateText({ model: openai('gpt-4'), prompt: 'Analyze...' });\r\n```\r\n\r\n**2. Set maxOutputTokens appropriately:**\r\n```typescript\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  prompt: 'Summarize in 2 sentences',\r\n  maxOutputTokens: 100,  // Prevent over-generation\r\n});\r\n```\r\n\r\n**3. Cache results when possible:**\r\n```typescript\r\nconst cache = new Map();\r\n\r\nasync function getCachedResponse(prompt: string) {\r\n  if (cache.has(prompt)) return cache.get(prompt);\r\n\r\n  const result = await generateText({ model: openai('gpt-4'), prompt });\r\n  cache.set(prompt, result.text);\r\n  return result.text;\r\n}\r\n```\r\n\r\n### Cloudflare Workers Specific\r\n\r\n**1. Move imports inside handlers:**\r\n```typescript\r\n// Avoid startup overhead\r\nexport default {\r\n  async fetch(request, env) {\r\n    const { generateText } = await import('ai');\r\n    const { openai } = await import('@ai-sdk/openai');\r\n    // Use here\r\n  }\r\n}\r\n```\r\n\r\n**2. Monitor startup time:**\r\n```bash\r\nwrangler deploy",
  "name": "ai-sdk-core",
  "id": "ai-sdk-core",
  "sections": {
    "Critical v4→v5 Migration": "AI SDK v5 introduced extensive breaking changes. If migrating from v4, follow this guide.\r\n\r\n### Breaking Changes Overview\r\n\r\n1. **Parameter Renames**\r\n   - `maxTokens` → `maxOutputTokens`\r\n   - `providerMetadata` → `providerOptions`\r\n\r\n2. **Tool Definitions**\r\n   - `parameters` → `inputSchema`\r\n   - Tool properties: `args` → `input`, `result` → `output`\r\n\r\n3. **Message Types**\r\n   - `CoreMessage` → `ModelMessage`\r\n   - `Message` → `UIMessage`\r\n   - `convertToCoreMessages` → `convertToModelMessages`\r\n\r\n4. **Tool Error Handling**\r\n   - `ToolExecutionError` class removed\r\n   - Now `tool-error` content parts\r\n   - Enables automated retry\r\n\r\n5. **Multi-Step Execution**\r\n   - `maxSteps` → `stopWhen`\r\n   - Use `stepCountIs()` or `hasToolCall()`\r\n\r\n6. **Message Structure**\r\n   - Simple `content` string → `parts` array\r\n   - Parts: text, file, reasoning, tool-call, tool-result\r\n\r\n7. **Streaming Architecture**\r\n   - Single chunk → start/delta/end lifecycle\r\n   - Unique IDs for concurrent streams\r\n\r\n8. **Tool Streaming**\r\n   - Enabled by default\r\n   - `toolCallStreaming` option removed\r\n\r\n9. **Package Reorganization**\r\n   - `ai/rsc` → `@ai-sdk/rsc`\r\n   - `ai/react` → `@ai-sdk/react`\r\n   - `LangChainAdapter` → `@ai-sdk/langchain`\r\n\r\n### Migration Examples\r\n\r\n**Before (v4):**\r\n```typescript\r\nimport { generateText } from 'ai';\r\n\r\nconst result = await generateText({\r\n  model: openai.chat('gpt-4'),\r\n  maxTokens: 500,\r\n  providerMetadata: { openai: { user: 'user-123' } },\r\n  tools: {\r\n    weather: {\r\n      description: 'Get weather',\r\n      parameters: z.object({ location: z.string() }),\r\n      execute: async (args) => { /* args.location */ },\r\n    },\r\n  },\r\n  maxSteps: 5,\r\n});\r\n```\r\n\r\n**After (v5):**\r\n```typescript\r\nimport { generateText, tool, stopWhen, stepCountIs } from 'ai';\r\n\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  maxOutputTokens: 500,\r\n  providerOptions: { openai: { user: 'user-123' } },\r\n  tools: {\r\n    weather: tool({\r\n      description: 'Get weather',\r\n      inputSchema: z.object({ location: z.string() }),\r\n      execute: async ({ location }) => { /* input.location */ },\r\n    }),\r\n  },\r\n  stopWhen: stepCountIs(5),\r\n});\r\n```\r\n\r\n### Migration Checklist\r\n\r\n- [ ] Update all `maxTokens` to `maxOutputTokens`\r\n- [ ] Update `providerMetadata` to `providerOptions`\r\n- [ ] Convert tool `parameters` to `inputSchema`\r\n- [ ] Update tool execute functions: `args` → `input`\r\n- [ ] Replace `maxSteps` with `stopWhen(stepCountIs(n))`\r\n- [ ] Update message types: `CoreMessage` → `ModelMessage`\r\n- [ ] Remove `ToolExecutionError` handling\r\n- [ ] Update package imports (`ai/rsc` → `@ai-sdk/rsc`)\r\n- [ ] Test streaming behavior (architecture changed)\r\n- [ ] Update TypeScript types\r\n\r\n### Automated Migration\r\n\r\nAI SDK provides a migration tool:\r\n\r\n```bash\r\nnpx ai migrate\r\n```\r\n\r\nThis will update most breaking changes automatically. Review changes carefully.\r\n\r\n**Official Migration Guide:**\r\nhttps://ai-sdk.dev/docs/migration-guides/migration-guide-5-0\r\n\r\n---",
    "Core Functions": "### generateText()\r\n\r\nGenerate text completion with optional tools and multi-step execution.\r\n\r\n**Signature:**\r\n\r\n```typescript\r\nasync function generateText(options: {\r\n  model: LanguageModel;\r\n  prompt?: string;\r\n  messages?: Array<ModelMessage>;\r\n  system?: string;\r\n  tools?: Record<string, Tool>;\r\n  maxOutputTokens?: number;\r\n  temperature?: number;\r\n  stopWhen?: StopCondition;\r\n  // ... other options\r\n}): Promise<GenerateTextResult>\r\n```\r\n\r\n**Basic Usage:**\r\n\r\n```typescript\r\nimport { generateText } from 'ai';\r\nimport { openai } from '@ai-sdk/openai';\r\n\r\nconst result = await generateText({\r\n  model: openai('gpt-4-turbo'),\r\n  prompt: 'Explain quantum computing',\r\n  maxOutputTokens: 500,\r\n  temperature: 0.7,\r\n});\r\n\r\nconsole.log(result.text);\r\nconsole.log(`Tokens: ${result.usage.totalTokens}`);\r\n```\r\n\r\n**With Messages (Chat Format):**\r\n\r\n```typescript\r\nconst result = await generateText({\r\n  model: openai('gpt-4-turbo'),\r\n  messages: [\r\n    { role: 'system', content: 'You are a helpful assistant.' },\r\n    { role: 'user', content: 'What is the weather?' },\r\n    { role: 'assistant', content: 'I need your location.' },\r\n    { role: 'user', content: 'San Francisco' },\r\n  ],\r\n});\r\n```\r\n\r\n**With Tools:**\r\n\r\n```typescript\r\nimport { tool } from 'ai';\r\nimport { z } from 'zod';\r\n\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  tools: {\r\n    weather: tool({\r\n      description: 'Get the weather for a location',\r\n      inputSchema: z.object({\r\n        location: z.string(),\r\n      }),\r\n      execute: async ({ location }) => {\r\n        // API call here\r\n        return { temperature: 72, condition: 'sunny' };\r\n      },\r\n    }),\r\n  },\r\n  prompt: 'What is the weather in Tokyo?',\r\n});\r\n```\r\n\r\n**When to Use:**\r\n- Need final response (not streaming)\r\n- Want to wait for tool executions to complete\r\n- Simpler code when streaming not needed\r\n- Building batch/scheduled tasks\r\n\r\n**Error Handling:**\r\n\r\n```typescript\r\nimport { AI_APICallError, AI_NoContentGeneratedError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateText({\r\n    model: openai('gpt-4-turbo'),\r\n    prompt: 'Hello',\r\n  });\r\n  console.log(result.text);\r\n} catch (error) {\r\n  if (error instanceof AI_APICallError) {\r\n    console.error('API call failed:', error.message);\r\n    // Check rate limits, API key, network\r\n  } else if (error instanceof AI_NoContentGeneratedError) {\r\n    console.error('No content generated');\r\n    // Prompt may have been filtered\r\n  } else {\r\n    console.error('Unknown error:', error);\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n### streamText()\r\n\r\nStream text completion with real-time chunks.\r\n\r\n**Signature:**\r\n\r\n```typescript\r\nfunction streamText(options: {\r\n  model: LanguageModel;\r\n  prompt?: string;\r\n  messages?: Array<ModelMessage>;\r\n  system?: string;\r\n  tools?: Record<string, Tool>;\r\n  maxOutputTokens?: number;\r\n  temperature?: number;\r\n  stopWhen?: StopCondition;\r\n  // ... other options\r\n}): StreamTextResult\r\n```\r\n\r\n**Basic Streaming:**\r\n\r\n```typescript\r\nimport { streamText } from 'ai';\r\nimport { anthropic } from '@ai-sdk/anthropic';\r\n\r\nconst stream = streamText({\r\n  model: anthropic('claude-sonnet-4-5-20250929'),\r\n  prompt: 'Write a poem about AI',\r\n});\r\n\r\n// Stream to console\r\nfor await (const chunk of stream.textStream) {\r\n  process.stdout.write(chunk);\r\n}\r\n\r\n// Or get final result\r\nconst finalResult = await stream.result;\r\nconsole.log(finalResult.text);\r\n```\r\n\r\n**Streaming with Tools:**\r\n\r\n```typescript\r\nconst stream = streamText({\r\n  model: openai('gpt-4'),\r\n  tools: {\r\n    // ... tools definition\r\n  },\r\n  prompt: 'What is the weather?',\r\n});\r\n\r\n// Stream text chunks\r\nfor await (const chunk of stream.textStream) {\r\n  process.stdout.write(chunk);\r\n}\r\n```\r\n\r\n**Handling the Stream:**\r\n\r\n```typescript\r\nconst stream = streamText({\r\n  model: openai('gpt-4-turbo'),\r\n  prompt: 'Explain AI',\r\n});\r\n\r\n// Option 1: Text stream\r\nfor await (const text of stream.textStream) {\r\n  console.log(text);\r\n}\r\n\r\n// Option 2: Full stream (includes metadata)\r\nfor await (const part of stream.fullStream) {\r\n  if (part.type === 'text-delta') {\r\n    console.log(part.textDelta);\r\n  } else if (part.type === 'tool-call') {\r\n    console.log('Tool called:', part.toolName);\r\n  }\r\n}\r\n\r\n// Option 3: Wait for final result\r\nconst result = await stream.result;\r\nconsole.log(result.text, result.usage);\r\n```\r\n\r\n**When to Use:**\r\n- Real-time user-facing responses\r\n- Long-form content generation\r\n- Want to show progress\r\n- Better perceived performance\r\n\r\n**Production Pattern:**\r\n\r\n```typescript\r\n// Next.js API Route\r\nimport { streamText } from 'ai';\r\nimport { openai } from '@ai-sdk/openai';\r\n\r\nexport async function POST(request: Request) {\r\n  const { messages } = await request.json();\r\n\r\n  const stream = streamText({\r\n    model: openai('gpt-4-turbo'),\r\n    messages,\r\n  });\r\n\r\n  // Return stream to client\r\n  return stream.toDataStreamResponse();\r\n}\r\n```\r\n\r\n**Error Handling:**\r\n\r\n```typescript\r\n// Recommended: Use onError callback (added in v4.1.22)\r\nconst stream = streamText({\r\n  model: openai('gpt-4-turbo'),\r\n  prompt: 'Hello',\r\n  onError({ error }) {\r\n    console.error('Stream error:', error);\r\n    // Custom error handling\r\n  },\r\n});\r\n\r\nfor await (const chunk of stream.textStream) {\r\n  process.stdout.write(chunk);\r\n}\r\n\r\n// Alternative: Manual try-catch\r\ntry {\r\n  const stream = streamText({\r\n    model: openai('gpt-4-turbo'),\r\n    prompt: 'Hello',\r\n  });\r\n\r\n  for await (const chunk of stream.textStream) {\r\n    process.stdout.write(chunk);\r\n  }\r\n} catch (error) {\r\n  console.error('Stream error:', error);\r\n}\r\n```\r\n\r\n---\r\n\r\n### generateObject()\r\n\r\nGenerate structured output validated by Zod schema.\r\n\r\n**Signature:**\r\n\r\n```typescript\r\nasync function generateObject<T>(options: {\r\n  model: LanguageModel;\r\n  schema: z.Schema<T>;\r\n  prompt?: string;\r\n  messages?: Array<ModelMessage>;\r\n  system?: string;\r\n  mode?: 'auto' | 'json' | 'tool';\r\n  // ... other options\r\n}): Promise<GenerateObjectResult<T>>\r\n```\r\n\r\n**Basic Usage:**\r\n\r\n```typescript\r\nimport { generateObject } from 'ai';\r\nimport { openai } from '@ai-sdk/openai';\r\nimport { z } from 'zod';\r\n\r\nconst result = await generateObject({\r\n  model: openai('gpt-4'),\r\n  schema: z.object({\r\n    recipe: z.object({\r\n      name: z.string(),\r\n      ingredients: z.array(z.object({\r\n        name: z.string(),\r\n        amount: z.string(),\r\n      })),\r\n      instructions: z.array(z.string()),\r\n    }),\r\n  }),\r\n  prompt: 'Generate a recipe for chocolate chip cookies',\r\n});\r\n\r\nconsole.log(result.object.recipe);\r\n```\r\n\r\n**Nested Schemas:**\r\n\r\n```typescript\r\nconst PersonSchema = z.object({\r\n  name: z.string(),\r\n  age: z.number(),\r\n  address: z.object({\r\n    street: z.string(),\r\n    city: z.string(),\r\n    country: z.string(),\r\n  }),\r\n  hobbies: z.array(z.string()),\r\n});\r\n\r\nconst result = await generateObject({\r\n  model: openai('gpt-4'),\r\n  schema: PersonSchema,\r\n  prompt: 'Generate a person profile',\r\n});\r\n```\r\n\r\n**Arrays and Unions:**\r\n\r\n```typescript\r\n// Array of objects\r\nconst result = await generateObject({\r\n  model: openai('gpt-4'),\r\n  schema: z.object({\r\n    people: z.array(z.object({\r\n      name: z.string(),\r\n      role: z.enum(['engineer', 'designer', 'manager']),\r\n    })),\r\n  }),\r\n  prompt: 'Generate a team of 5 people',\r\n});\r\n\r\n// Union types\r\nconst result = await generateObject({\r\n  model: openai('gpt-4'),\r\n  schema: z.discriminatedUnion('type', [\r\n    z.object({ type: z.literal('text'), content: z.string() }),\r\n    z.object({ type: z.literal('image'), url: z.string() }),\r\n  ]),\r\n  prompt: 'Generate content',\r\n});\r\n```\r\n\r\n**When to Use:**\r\n- Need structured data (JSON, forms, etc.)\r\n- Validation is critical\r\n- Extracting data from unstructured input\r\n- Building AI workflows that consume JSON\r\n\r\n**Error Handling:**\r\n\r\n```typescript\r\nimport { AI_NoObjectGeneratedError, AI_TypeValidationError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateObject({\r\n    model: openai('gpt-4'),\r\n    schema: z.object({ name: z.string() }),\r\n    prompt: 'Generate a person',\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_NoObjectGeneratedError) {\r\n    console.error('Model did not generate valid object');\r\n    // Try simplifying schema or adding examples\r\n  } else if (error instanceof AI_TypeValidationError) {\r\n    console.error('Zod validation failed:', error.message);\r\n    // Schema doesn't match output\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n### streamObject()\r\n\r\nStream structured output with partial updates.\r\n\r\n**Signature:**\r\n\r\n```typescript\r\nfunction streamObject<T>(options: {\r\n  model: LanguageModel;\r\n  schema: z.Schema<T>;\r\n  prompt?: string;\r\n  messages?: Array<ModelMessage>;\r\n  mode?: 'auto' | 'json' | 'tool';\r\n  // ... other options\r\n}): StreamObjectResult<T>\r\n```\r\n\r\n**Basic Usage:**\r\n\r\n```typescript\r\nimport { streamObject } from 'ai';\r\nimport { google } from '@ai-sdk/google';\r\nimport { z } from 'zod';\r\n\r\nconst stream = streamObject({\r\n  model: google('gemini-2.5-pro'),\r\n  schema: z.object({\r\n    characters: z.array(z.object({\r\n      name: z.string(),\r\n      class: z.string(),\r\n      stats: z.object({\r\n        hp: z.number(),\r\n        mana: z.number(),\r\n      }),\r\n    })),\r\n  }),\r\n  prompt: 'Generate 3 RPG characters',\r\n});\r\n\r\n// Stream partial updates\r\nfor await (const partialObject of stream.partialObjectStream) {\r\n  console.log(partialObject);\r\n  // { characters: [{ name: \"Aria\" }] }\r\n  // { characters: [{ name: \"Aria\", class: \"Mage\" }] }\r\n  // { characters: [{ name: \"Aria\", class: \"Mage\", stats: { hp: 100 } }] }\r\n  // ...\r\n}\r\n```\r\n\r\n**UI Integration Pattern:**\r\n\r\n```typescript\r\n// Server endpoint\r\nexport async function POST(request: Request) {\r\n  const { prompt } = await request.json();\r\n\r\n  const stream = streamObject({\r\n    model: openai('gpt-4'),\r\n    schema: z.object({\r\n      summary: z.string(),\r\n      keyPoints: z.array(z.string()),\r\n    }),\r\n    prompt,\r\n  });\r\n\r\n  return stream.toTextStreamResponse();\r\n}\r\n\r\n// Client (with useObject hook from ai-sdk-ui)\r\nconst { object, isLoading } = useObject({\r\n  api: '/api/analyze',\r\n  schema: /* same schema */,\r\n});\r\n\r\n// Render partial object as it streams\r\n{object?.summary && <p>{object.summary}</p>}\r\n{object?.keyPoints?.map(point => <li key={point}>{point}</li>)}\r\n```\r\n\r\n**When to Use:**\r\n- Real-time structured data (forms, dashboards)\r\n- Show progressive completion\r\n- Large structured outputs\r\n- Better UX for slow generations\r\n\r\n---",
    "Provider Setup & Configuration": "### OpenAI\r\n\r\n```typescript\r\nimport { openai } from '@ai-sdk/openai';\r\nimport { generateText } from 'ai';\r\n\r\n// API key from environment (recommended)\r\n// OPENAI_API_KEY=sk-...\r\nconst model = openai('gpt-4-turbo');\r\n\r\n// Or explicit API key\r\nconst model = openai('gpt-4', {\r\n  apiKey: process.env.OPENAI_API_KEY,\r\n});\r\n\r\n// Available models\r\nconst gpt5 = openai('gpt-5');           // Latest (released August 2025)\r\nconst gpt4 = openai('gpt-4-turbo');\r\nconst gpt35 = openai('gpt-3.5-turbo');\r\n\r\nconst result = await generateText({\r\n  model: gpt4,\r\n  prompt: 'Hello',\r\n});\r\n```\r\n\r\n**Common Errors:**\r\n- `AI_LoadAPIKeyError`: Check `OPENAI_API_KEY` environment variable\r\n- `429 Rate Limit`: Implement exponential backoff, upgrade tier\r\n- `401 Unauthorized`: Invalid API key format\r\n\r\n**Rate Limiting:**\r\nOpenAI enforces RPM (requests per minute) and TPM (tokens per minute) limits. Implement retry logic:\r\n\r\n```typescript\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  prompt: 'Hello',\r\n  maxRetries: 3,  // Built-in retry\r\n});\r\n```\r\n\r\n---\r\n\r\n### Anthropic\r\n\r\n```typescript\r\nimport { anthropic } from '@ai-sdk/anthropic';\r\n\r\n// ANTHROPIC_API_KEY=sk-ant-...\r\nconst claude = anthropic('claude-sonnet-4-5-20250929');\r\n\r\n// Available models (Claude 4.x family, released 2025)\r\nconst sonnet45 = anthropic('claude-sonnet-4-5-20250929');  // Latest, recommended\r\nconst sonnet4 = anthropic('claude-sonnet-4-20250522');     // Released May 2025\r\nconst opus4 = anthropic('claude-opus-4-20250522');         // Highest quality\r\n\r\n// Legacy models (Claude 3.x, deprecated)\r\n// const sonnet35 = anthropic('claude-3-5-sonnet-20241022');  // Use Claude 4.x instead\r\n// const opus3 = anthropic('claude-3-opus-20240229');\r\n// const haiku3 = anthropic('claude-3-haiku-20240307');\r\n\r\nconst result = await generateText({\r\n  model: sonnet45,\r\n  prompt: 'Explain quantum entanglement',\r\n});\r\n```\r\n\r\n**Common Errors:**\r\n- `AI_LoadAPIKeyError`: Check `ANTHROPIC_API_KEY` environment variable\r\n- `overloaded_error`: Retry with exponential backoff\r\n- `rate_limit_error`: Wait and retry\r\n\r\n**Best Practices:**\r\n- Claude excels at long-context tasks (200K+ tokens)\r\n- **Claude 4.x recommended**: Anthropic deprecated Claude 3.x in 2025\r\n- Use Sonnet 4.5 for balance of speed/quality (latest model)\r\n- Use Sonnet 4 for production stability (if avoiding latest)\r\n- Use Opus 4 for highest quality reasoning and complex tasks\r\n\r\n---\r\n\r\n### Google\r\n\r\n```typescript\r\nimport { google } from '@ai-sdk/google';\r\n\r\n// GOOGLE_GENERATIVE_AI_API_KEY=...\r\nconst gemini = google('gemini-2.5-pro');\r\n\r\n// Available models (all GA since June-July 2025)\r\nconst pro = google('gemini-2.5-pro');\r\nconst flash = google('gemini-2.5-flash');\r\nconst lite = google('gemini-2.5-flash-lite');\r\n\r\nconst result = await generateText({\r\n  model: pro,\r\n  prompt: 'Analyze this data',\r\n});\r\n```\r\n\r\n**Common Errors:**\r\n- `AI_LoadAPIKeyError`: Check `GOOGLE_GENERATIVE_AI_API_KEY`\r\n- `SAFETY`: Content filtered by safety settings\r\n- `QUOTA_EXCEEDED`: Rate limit hit\r\n\r\n**Best Practices:**\r\n- Gemini Pro: Best for reasoning and analysis\r\n- Gemini Flash: Fast, cost-effective for most tasks\r\n- Free tier has generous limits\r\n- Good for multimodal tasks (combine with image inputs)\r\n\r\n---\r\n\r\n### Cloudflare Workers AI\r\n\r\n```typescript\r\nimport { Hono } from 'hono';\r\nimport { generateText } from 'ai';\r\nimport { createWorkersAI } from 'workers-ai-provider';\r\n\r\ninterface Env {\r\n  AI: Ai;\r\n}\r\n\r\nconst app = new Hono<{ Bindings: Env }>();\r\n\r\napp.post('/chat', async (c) => {\r\n  // Create provider inside handler (avoid startup overhead)\r\n  const workersai = createWorkersAI({ binding: c.env.AI });\r\n\r\n  const result = await generateText({\r\n    model: workersai('@cf/meta/llama-3.1-8b-instruct'),\r\n    prompt: 'What is Cloudflare?',\r\n  });\r\n\r\n  return c.json({ response: result.text });\r\n});\r\n\r\nexport default app;\r\n```\r\n\r\n**wrangler.jsonc:**\r\n\r\n```jsonc\r\n{\r\n  \"name\": \"ai-sdk-worker\",\r\n  \"compatibility_date\": \"2025-10-21\",\r\n  \"ai\": {\r\n    \"binding\": \"AI\"\r\n  }\r\n}\r\n```\r\n\r\n**Important Notes:**\r\n\r\n**Startup Optimization:**\r\nAI SDK v5 + Zod can cause >270ms startup time in Workers. Solutions:\r\n\r\n1. **Move imports inside handler:**\r\n```typescript\r\n// BAD (startup overhead)\r\nimport { createWorkersAI } from 'workers-ai-provider';\r\nconst workersai = createWorkersAI({ binding: env.AI });\r\n\r\n// GOOD (lazy init)\r\napp.post('/chat', async (c) => {\r\n  const { createWorkersAI } = await import('workers-ai-provider');\r\n  const workersai = createWorkersAI({ binding: c.env.AI });\r\n  // ...\r\n});\r\n```\r\n\r\n2. **Minimize top-level Zod schemas:**\r\n```typescript\r\n// Move complex schemas into route handlers\r\n```\r\n\r\n**When to Use workers-ai-provider:**\r\n- Multi-provider scenarios (OpenAI + Workers AI)\r\n- Using AI SDK UI hooks with Workers AI\r\n- Need consistent API across providers\r\n\r\n**When to Use Native Binding:**\r\nFor Cloudflare-only deployments without multi-provider support, use the `cloudflare-workers-ai` skill instead for maximum performance.\r\n\r\n---",
    "Production Best Practices": "```\r\n\r\n**3. Handle streaming properly:**\r\n```typescript\r\n// Return ReadableStream for streaming responses\r\nconst stream = streamText({ model: openai('gpt-4'), prompt: 'Hello' });\r\nreturn new Response(stream.toTextStream(), {\r\n  headers: { 'Content-Type': 'text/plain; charset=utf-8' },\r\n});\r\n```\r\n\r\n### Next.js / Vercel Specific\r\n\r\n**1. Use Server Actions for mutations:**\r\n```typescript\r\n'use server';\r\n\r\nexport async function generateContent(input: string) {\r\n  const result = await generateText({\r\n    model: openai('gpt-4'),\r\n    prompt: input,\r\n  });\r\n  return result.text;\r\n}\r\n```\r\n\r\n**2. Use Server Components for initial loads:**\r\n```typescript\r\n// app/page.tsx\r\nexport default async function Page() {\r\n  const result = await generateText({\r\n    model: openai('gpt-4'),\r\n    prompt: 'Welcome message',\r\n  });\r\n\r\n  return <div>{result.text}</div>;\r\n}\r\n```\r\n\r\n**3. Implement loading states:**\r\n```typescript\r\n'use client';\r\n\r\nimport { useState } from 'react';\r\nimport { generateContent } from './actions';\r\n\r\nexport default function Form() {\r\n  const [loading, setLoading] = useState(false);\r\n\r\n  async function handleSubmit(formData: FormData) {\r\n    setLoading(true);\r\n    const result = await generateContent(formData.get('input'));\r\n    setLoading(false);\r\n  }\r\n\r\n  return (\r\n    <form action={handleSubmit}>\r\n      <input name=\"input\" />\r\n      <button disabled={loading}>\r\n        {loading ? 'Generating...' : 'Submit'}\r\n      </button>\r\n    </form>\r\n  );\r\n}\r\n```\r\n\r\n**4. For deployment:**\r\nSee Vercel's official deployment documentation: https://vercel.com/docs/functions\r\n\r\n---",
    "When to Use This Skill": "### Use ai-sdk-core when:\r\n\r\n- Building backend AI features (server-side text generation)\r\n- Implementing server-side text generation (Node.js, Workers, Next.js)\r\n- Creating structured AI outputs (JSON, forms, data extraction)\r\n- Building AI agents with tools (multi-step workflows)\r\n- Integrating multiple AI providers (OpenAI, Anthropic, Google, Cloudflare)\r\n- Migrating from AI SDK v4 to v5\r\n- Encountering AI SDK errors (AI_APICallError, AI_NoObjectGeneratedError, etc.)\r\n- Using AI in Cloudflare Workers (with workers-ai-provider)\r\n- Using AI in Next.js Server Components/Actions\r\n- Need consistent API across different LLM providers\r\n\r\n### Don't use this skill when:\r\n\r\n- Building React chat UIs (use **ai-sdk-ui** skill instead)\r\n- Need frontend hooks like useChat (use **ai-sdk-ui** skill instead)\r\n- Need advanced topics like embeddings or image generation (check official docs)\r\n- Building native Cloudflare Workers AI apps without multi-provider (use **cloudflare-workers-ai** skill instead)\r\n- Need Generative UI / RSC (see https://ai-sdk.dev/docs/ai-sdk-rsc)\r\n\r\n---",
    "Templates & References": "This skill includes:\r\n\r\n- **13 Templates:** Ready-to-use code examples in `templates/`\r\n- **5 Reference Docs:** Detailed guides in `references/`\r\n- **1 Script:** Version checker in `scripts/`\r\n\r\nAll files are optimized for copy-paste into your project.\r\n\r\n---\r\n\r\n**Last Updated:** 2025-10-29\r\n**Skill Version:** 1.1.0\r\n**AI SDK Version:** 5.0.81+",
    "Top 12 Errors & Solutions": "### 1. AI_APICallError\r\n\r\n**Cause:** API request failed (network, auth, rate limit).\r\n\r\n**Solution:**\r\n```typescript\r\nimport { AI_APICallError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateText({\r\n    model: openai('gpt-4'),\r\n    prompt: 'Hello',\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_APICallError) {\r\n    console.error('API call failed:', error.message);\r\n    console.error('Status code:', error.statusCode);\r\n    console.error('Response:', error.responseBody);\r\n\r\n    // Check common causes\r\n    if (error.statusCode === 401) {\r\n      // Invalid API key\r\n    } else if (error.statusCode === 429) {\r\n      // Rate limit - implement backoff\r\n    } else if (error.statusCode >= 500) {\r\n      // Provider issue - retry\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Validate API keys at startup\r\n- Implement retry logic with exponential backoff\r\n- Monitor rate limits\r\n- Handle network errors gracefully\r\n\r\n---\r\n\r\n### 2. AI_NoObjectGeneratedError\r\n\r\n**Cause:** Model didn't generate valid object matching schema.\r\n\r\n**Solution:**\r\n```typescript\r\nimport { AI_NoObjectGeneratedError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateObject({\r\n    model: openai('gpt-4'),\r\n    schema: z.object({ /* complex schema */ }),\r\n    prompt: 'Generate data',\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_NoObjectGeneratedError) {\r\n    console.error('No valid object generated');\r\n\r\n    // Solutions:\r\n    // 1. Simplify schema\r\n    // 2. Add more context to prompt\r\n    // 3. Provide examples in prompt\r\n    // 4. Try different model (gpt-4 better than gpt-3.5 for complex objects)\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Start with simple schemas, add complexity incrementally\r\n- Include examples in prompt: \"Generate a person like: { name: 'Alice', age: 30 }\"\r\n- Use GPT-4 for complex structured output\r\n- Test schemas with sample data first\r\n\r\n---\r\n\r\n### 3. Worker Startup Limit (270ms+)\r\n\r\n**Cause:** AI SDK v5 + Zod initialization overhead in Cloudflare Workers exceeds startup limits.\r\n\r\n**Solution:**\r\n```typescript\r\n// BAD: Top-level imports cause startup overhead\r\nimport { createWorkersAI } from 'workers-ai-provider';\r\nimport { complexSchema } from './schemas';\r\n\r\nconst workersai = createWorkersAI({ binding: env.AI });\r\n\r\n// GOOD: Lazy initialization inside handler\r\nexport default {\r\n  async fetch(request, env) {\r\n    const { createWorkersAI } = await import('workers-ai-provider');\r\n    const workersai = createWorkersAI({ binding: env.AI });\r\n\r\n    // Use workersai here\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Move AI SDK imports inside route handlers\r\n- Minimize top-level Zod schemas\r\n- Monitor Worker startup time (must be <400ms)\r\n- Use Wrangler's startup time reporting\r\n\r\n**GitHub Issue:** Search for \"Workers startup limit\" in Vercel AI SDK issues\r\n\r\n---\r\n\r\n### 4. streamText Fails Silently\r\n\r\n**Cause:** Stream errors can be swallowed by `createDataStreamResponse`.\r\n\r\n**Status:** ✅ **RESOLVED** - Fixed in ai@4.1.22 (February 2025)\r\n\r\n**Solution (Recommended):**\r\n```typescript\r\n// Use the onError callback (added in v4.1.22)\r\nconst stream = streamText({\r\n  model: openai('gpt-4'),\r\n  prompt: 'Hello',\r\n  onError({ error }) {\r\n    console.error('Stream error:', error);\r\n    // Custom error logging and handling\r\n  },\r\n});\r\n\r\n// Stream safely\r\nfor await (const chunk of stream.textStream) {\r\n  process.stdout.write(chunk);\r\n}\r\n```\r\n\r\n**Alternative (Manual try-catch):**\r\n```typescript\r\n// Fallback if not using onError callback\r\ntry {\r\n  const stream = streamText({\r\n    model: openai('gpt-4'),\r\n    prompt: 'Hello',\r\n  });\r\n\r\n  for await (const chunk of stream.textStream) {\r\n    process.stdout.write(chunk);\r\n  }\r\n} catch (error) {\r\n  console.error('Stream error:', error);\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- **Use `onError` callback** for proper error capture (recommended)\r\n- Implement server-side error monitoring\r\n- Test stream error handling explicitly\r\n- Always log on server side in production\r\n\r\n**GitHub Issue:** #4726 (RESOLVED)\r\n\r\n---\r\n\r\n### 5. AI_LoadAPIKeyError\r\n\r\n**Cause:** Missing or invalid API key.\r\n\r\n**Solution:**\r\n```typescript\r\nimport { AI_LoadAPIKeyError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateText({\r\n    model: openai('gpt-4'),\r\n    prompt: 'Hello',\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_LoadAPIKeyError) {\r\n    console.error('API key error:', error.message);\r\n\r\n    // Check:\r\n    // 1. .env file exists and loaded\r\n    // 2. Correct env variable name (OPENAI_API_KEY)\r\n    // 3. Key format is valid (starts with sk-)\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Validate API keys at application startup\r\n- Use environment variable validation (e.g., zod)\r\n- Provide clear error messages in development\r\n- Document required environment variables\r\n\r\n---\r\n\r\n### 6. AI_InvalidArgumentError\r\n\r\n**Cause:** Invalid parameters passed to function.\r\n\r\n**Solution:**\r\n```typescript\r\nimport { AI_InvalidArgumentError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateText({\r\n    model: openai('gpt-4'),\r\n    maxOutputTokens: -1,  // Invalid!\r\n    prompt: 'Hello',\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_InvalidArgumentError) {\r\n    console.error('Invalid argument:', error.message);\r\n    // Check parameter types and values\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Use TypeScript for type checking\r\n- Validate inputs before calling AI SDK functions\r\n- Read function signatures carefully\r\n- Check official docs for parameter constraints\r\n\r\n---\r\n\r\n### 7. AI_NoContentGeneratedError\r\n\r\n**Cause:** Model generated no content (safety filters, etc.).\r\n\r\n**Solution:**\r\n```typescript\r\nimport { AI_NoContentGeneratedError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateText({\r\n    model: openai('gpt-4'),\r\n    prompt: 'Some prompt',\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_NoContentGeneratedError) {\r\n    console.error('No content generated');\r\n\r\n    // Possible causes:\r\n    // 1. Safety filters blocked output\r\n    // 2. Prompt triggered content policy\r\n    // 3. Model configuration issue\r\n\r\n    // Handle gracefully:\r\n    return { text: 'Unable to generate response. Please try different input.' };\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Sanitize user inputs\r\n- Avoid prompts that may trigger safety filters\r\n- Have fallback messaging\r\n- Log occurrences for analysis\r\n\r\n---\r\n\r\n### 8. AI_TypeValidationError\r\n\r\n**Cause:** Zod schema validation failed on generated output.\r\n\r\n**Solution:**\r\n```typescript\r\nimport { AI_TypeValidationError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateObject({\r\n    model: openai('gpt-4'),\r\n    schema: z.object({\r\n      age: z.number().min(0).max(120),  // Strict validation\r\n    }),\r\n    prompt: 'Generate person',\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_TypeValidationError) {\r\n    console.error('Validation failed:', error.message);\r\n\r\n    // Solutions:\r\n    // 1. Relax schema constraints\r\n    // 2. Add more guidance in prompt\r\n    // 3. Use .optional() for unreliable fields\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Start with lenient schemas, tighten gradually\r\n- Use `.optional()` for fields that may not always be present\r\n- Add validation hints in field descriptions\r\n- Test with various prompts\r\n\r\n---\r\n\r\n### 9. AI_RetryError\r\n\r\n**Cause:** All retry attempts failed.\r\n\r\n**Solution:**\r\n```typescript\r\nimport { AI_RetryError } from 'ai';\r\n\r\ntry {\r\n  const result = await generateText({\r\n    model: openai('gpt-4'),\r\n    prompt: 'Hello',\r\n    maxRetries: 3,  // Default is 2\r\n  });\r\n} catch (error) {\r\n  if (error instanceof AI_RetryError) {\r\n    console.error('All retries failed');\r\n    console.error('Last error:', error.lastError);\r\n\r\n    // Check root cause:\r\n    // - Persistent network issue\r\n    // - Provider outage\r\n    // - Invalid configuration\r\n  }\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Investigate root cause of failures\r\n- Adjust retry configuration if needed\r\n- Implement circuit breaker pattern for provider outages\r\n- Have fallback providers\r\n\r\n---\r\n\r\n### 10. Rate Limiting Errors\r\n\r\n**Cause:** Exceeded provider rate limits (RPM/TPM).\r\n\r\n**Solution:**\r\n```typescript\r\n// Implement exponential backoff\r\nasync function generateWithBackoff(prompt: string, retries = 3) {\r\n  for (let i = 0; i < retries; i++) {\r\n    try {\r\n      return await generateText({\r\n        model: openai('gpt-4'),\r\n        prompt,\r\n      });\r\n    } catch (error) {\r\n      if (error instanceof AI_APICallError && error.statusCode === 429) {\r\n        const delay = Math.pow(2, i) * 1000;  // Exponential backoff\r\n        console.log(`Rate limited, waiting ${delay}ms`);\r\n        await new Promise(resolve => setTimeout(resolve, delay));\r\n      } else {\r\n        throw error;\r\n      }\r\n    }\r\n  }\r\n  throw new Error('Rate limit retries exhausted');\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Monitor rate limit headers\r\n- Queue requests to stay under limits\r\n- Upgrade provider tier if needed\r\n- Implement request throttling\r\n\r\n---\r\n\r\n### 11. TypeScript Performance with Zod\r\n\r\n**Cause:** Complex Zod schemas slow down TypeScript type checking.\r\n\r\n**Solution:**\r\n```typescript\r\n// Instead of deeply nested schemas at top level:\r\n// const complexSchema = z.object({ /* 100+ fields */ });\r\n\r\n// Define inside functions or use type assertions:\r\nfunction generateData() {\r\n  const schema = z.object({ /* complex schema */ });\r\n  return generateObject({ model: openai('gpt-4'), schema, prompt: '...' });\r\n}\r\n\r\n// Or use z.lazy() for recursive schemas:\r\ntype Category = { name: string; subcategories?: Category[] };\r\nconst CategorySchema: z.ZodType<Category> = z.lazy(() =>\r\n  z.object({\r\n    name: z.string(),\r\n    subcategories: z.array(CategorySchema).optional(),\r\n  })\r\n);\r\n```\r\n\r\n**Prevention:**\r\n- Avoid top-level complex schemas\r\n- Use `z.lazy()` for recursive types\r\n- Split large schemas into smaller ones\r\n- Use type assertions where appropriate\r\n\r\n**Official Docs:**\r\nhttps://ai-sdk.dev/docs/troubleshooting/common-issues/slow-type-checking\r\n\r\n---\r\n\r\n### 12. Invalid JSON Response (Provider-Specific)\r\n\r\n**Cause:** Some models occasionally return invalid JSON.\r\n\r\n**Solution:**\r\n```typescript\r\n// Use built-in retry and mode selection\r\nconst result = await generateObject({\r\n  model: openai('gpt-4'),\r\n  schema: mySchema,\r\n  prompt: 'Generate data',\r\n  mode: 'json',  // Force JSON mode (supported by GPT-4)\r\n  maxRetries: 3,  // Retry on invalid JSON\r\n});\r\n\r\n// Or catch and retry manually:\r\ntry {\r\n  const result = await generateObject({\r\n    model: openai('gpt-4'),\r\n    schema: mySchema,\r\n    prompt: 'Generate data',\r\n  });\r\n} catch (error) {\r\n  // Retry with different model\r\n  const result = await generateObject({\r\n    model: openai('gpt-4-turbo'),\r\n    schema: mySchema,\r\n    prompt: 'Generate data',\r\n  });\r\n}\r\n```\r\n\r\n**Prevention:**\r\n- Use `mode: 'json'` when available\r\n- Prefer GPT-4 for structured output\r\n- Implement retry logic\r\n- Validate responses\r\n\r\n**GitHub Issue:** #4302 (Imagen 3.0 Invalid JSON)\r\n\r\n---\r\n\r\n**For More Errors:**\r\nSee complete error reference at https://ai-sdk.dev/docs/reference/ai-sdk-errors\r\n\r\n---",
    "Quick Start (5 Minutes)": "OPENAI_API_KEY=sk-...\r\nANTHROPIC_API_KEY=sk-ant-...\r\nGOOGLE_GENERATIVE_AI_API_KEY=...\r\n```\r\n\r\n### First Example: Generate Text\r\n\r\n```typescript\r\nimport { generateText } from 'ai';\r\nimport { openai } from '@ai-sdk/openai';\r\n\r\nconst result = await generateText({\r\n  model: openai('gpt-4-turbo'),\r\n  prompt: 'What is TypeScript?',\r\n});\r\n\r\nconsole.log(result.text);\r\n```\r\n\r\n### First Example: Streaming Chat\r\n\r\n```typescript\r\nimport { streamText } from 'ai';\r\nimport { anthropic } from '@ai-sdk/anthropic';\r\n\r\nconst stream = streamText({\r\n  model: anthropic('claude-sonnet-4-5-20250929'),\r\n  messages: [\r\n    { role: 'user', content: 'Tell me a story' },\r\n  ],\r\n});\r\n\r\nfor await (const chunk of stream.textStream) {\r\n  process.stdout.write(chunk);\r\n}\r\n```\r\n\r\n### First Example: Structured Output\r\n\r\n```typescript\r\nimport { generateObject } from 'ai';\r\nimport { openai } from '@ai-sdk/openai';\r\nimport { z } from 'zod';\r\n\r\nconst result = await generateObject({\r\n  model: openai('gpt-4'),\r\n  schema: z.object({\r\n    name: z.string(),\r\n    age: z.number(),\r\n    skills: z.array(z.string()),\r\n  }),\r\n  prompt: 'Generate a person profile for a software engineer',\r\n});\r\n\r\nconsole.log(result.object);\r\n// { name: \"Alice\", age: 28, skills: [\"TypeScript\", \"React\"] }\r\n```\r\n\r\n---",
    "Dependencies & Versions": "```json\r\n{\r\n  \"dependencies\": {\r\n    \"ai\": \"^5.0.81\",\r\n    \"@ai-sdk/openai\": \"^2.0.56\",\r\n    \"@ai-sdk/anthropic\": \"^2.0.38\",\r\n    \"@ai-sdk/google\": \"^2.0.24\",\r\n    \"workers-ai-provider\": \"^2.0.0\",\r\n    \"zod\": \"^3.23.8\"\r\n  },\r\n  \"devDependencies\": {\r\n    \"@types/node\": \"^20.11.0\",\r\n    \"typescript\": \"^5.3.3\"\r\n  }\r\n}\r\n```\r\n\r\n**Version Notes:**\r\n- AI SDK v5.0.81+ (stable, latest as of October 2025)\r\n- v6 is in beta - not covered in this skill\r\n- **Zod compatibility**: This skill uses Zod 3.x, but AI SDK 5 officially supports both Zod 3.x and Zod 4.x (4.1.12 latest)\r\n  - Zod 4 recommended for new projects (released August 2025)\r\n  - Zod 4 has breaking changes: error APIs, `.default()` behavior, `ZodError.errors` removed\r\n  - Some peer dependency warnings may occur with `zod-to-json-schema` when using Zod 4\r\n  - See https://zod.dev/v4/changelog for migration guide\r\n- Provider packages at 2.0+ for v5 compatibility\r\n\r\n**Check Latest Versions:**\r\n```bash\r\nnpm view ai version\r\nnpm view @ai-sdk/openai version\r\nnpm view @ai-sdk/anthropic version\r\nnpm view @ai-sdk/google version\r\nnpm view workers-ai-provider version\r\nnpm view zod version  # Check for Zod 4.x updates\r\n```\r\n\r\n---",
    "Links to Official Documentation": "### Core Documentation\r\n\r\n- **AI SDK Introduction:** https://ai-sdk.dev/docs/introduction\r\n- **AI SDK Core Overview:** https://ai-sdk.dev/docs/ai-sdk-core/overview\r\n- **Generating Text:** https://ai-sdk.dev/docs/ai-sdk-core/generating-text\r\n- **Generating Structured Data:** https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\r\n- **Tools and Tool Calling:** https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\r\n- **Agents Overview:** https://ai-sdk.dev/docs/agents/overview\r\n- **Foundations:** https://ai-sdk.dev/docs/foundations/overview\r\n\r\n### Advanced Topics (Not Replicated in This Skill)\r\n\r\n- **Embeddings:** https://ai-sdk.dev/docs/ai-sdk-core/embeddings\r\n- **Image Generation:** https://ai-sdk.dev/docs/ai-sdk-core/generating-images\r\n- **Transcription:** https://ai-sdk.dev/docs/ai-sdk-core/generating-transcriptions\r\n- **Speech:** https://ai-sdk.dev/docs/ai-sdk-core/generating-speech\r\n- **MCP Tools:** https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\r\n- **Telemetry:** https://ai-sdk.dev/docs/ai-sdk-core/telemetry\r\n- **Generative UI:** https://ai-sdk.dev/docs/ai-sdk-rsc\r\n\r\n### Migration & Troubleshooting\r\n\r\n- **v4→v5 Migration Guide:** https://ai-sdk.dev/docs/migration-guides/migration-guide-5-0\r\n- **All Error Types (28 total):** https://ai-sdk.dev/docs/reference/ai-sdk-errors\r\n- **Troubleshooting Guide:** https://ai-sdk.dev/docs/troubleshooting\r\n\r\n### Provider Documentation\r\n\r\n- **OpenAI Provider:** https://ai-sdk.dev/providers/ai-sdk-providers/openai\r\n- **Anthropic Provider:** https://ai-sdk.dev/providers/ai-sdk-providers/anthropic\r\n- **Google Provider:** https://ai-sdk.dev/providers/ai-sdk-providers/google\r\n- **All Providers (25+):** https://ai-sdk.dev/providers/overview\r\n- **Community Providers:** https://ai-sdk.dev/providers/community-providers\r\n\r\n### Cloudflare Integration\r\n\r\n- **Workers AI Provider (Community):** https://ai-sdk.dev/providers/community-providers/cloudflare-workers-ai\r\n- **Cloudflare Workers AI Docs:** https://developers.cloudflare.com/workers-ai/\r\n- **workers-ai-provider GitHub:** https://github.com/cloudflare/ai/tree/main/packages/workers-ai-provider\r\n- **Cloudflare AI SDK Configuration:** https://developers.cloudflare.com/workers-ai/configuration/ai-sdk/\r\n\r\n### Vercel / Next.js Integration\r\n\r\n- **Vercel AI SDK 5.0 Blog:** https://vercel.com/blog/ai-sdk-5\r\n- **Next.js App Router Integration:** https://ai-sdk.dev/docs/getting-started/nextjs-app-router\r\n- **Next.js Pages Router Integration:** https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\r\n- **Vercel Functions:** https://vercel.com/docs/functions\r\n- **Vercel Streaming:** https://vercel.com/docs/functions/streaming\r\n\r\n### GitHub & Community\r\n\r\n- **GitHub Repository:** https://github.com/vercel/ai\r\n- **GitHub Issues:** https://github.com/vercel/ai/issues\r\n- **Discord Community:** https://discord.gg/vercel\r\n\r\n---",
    "Tool Calling & Agents": "### Basic Tool Definition\r\n\r\n```typescript\r\nimport { generateText, tool } from 'ai';\r\nimport { openai } from '@ai-sdk/openai';\r\nimport { z } from 'zod';\r\n\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  tools: {\r\n    weather: tool({\r\n      description: 'Get the weather for a location',\r\n      inputSchema: z.object({\r\n        location: z.string().describe('The city and country, e.g. \"Paris, France\"'),\r\n        unit: z.enum(['celsius', 'fahrenheit']).optional(),\r\n      }),\r\n      execute: async ({ location, unit = 'celsius' }) => {\r\n        // Simulate API call\r\n        const data = await fetch(`https://api.weather.com/${location}`);\r\n        return { temperature: 72, condition: 'sunny', unit };\r\n      },\r\n    }),\r\n    convertTemperature: tool({\r\n      description: 'Convert temperature between units',\r\n      inputSchema: z.object({\r\n        value: z.number(),\r\n        from: z.enum(['celsius', 'fahrenheit']),\r\n        to: z.enum(['celsius', 'fahrenheit']),\r\n      }),\r\n      execute: async ({ value, from, to }) => {\r\n        if (from === to) return { value };\r\n        if (from === 'celsius' && to === 'fahrenheit') {\r\n          return { value: (value * 9/5) + 32 };\r\n        }\r\n        return { value: (value - 32) * 5/9 };\r\n      },\r\n    }),\r\n  },\r\n  prompt: 'What is the weather in Tokyo in Fahrenheit?',\r\n});\r\n\r\nconsole.log(result.text);\r\n// Model will call weather tool, potentially convertTemperature, then answer\r\n```\r\n\r\n**v5 Tool Changes:**\r\n- `parameters` → `inputSchema` (Zod schema)\r\n- Tool properties: `args` → `input`, `result` → `output`\r\n- `ToolExecutionError` removed (now `tool-error` content parts)\r\n\r\n---\r\n\r\n### Agent Class\r\n\r\nThe Agent class simplifies multi-step execution with tools.\r\n\r\n```typescript\r\nimport { Agent, tool } from 'ai';\r\nimport { anthropic } from '@ai-sdk/anthropic';\r\nimport { z } from 'zod';\r\n\r\nconst weatherAgent = new Agent({\r\n  model: anthropic('claude-sonnet-4-5-20250929'),\r\n  system: 'You are a weather assistant. Always convert temperatures to the user\\'s preferred unit.',\r\n  tools: {\r\n    getWeather: tool({\r\n      description: 'Get current weather for a location',\r\n      inputSchema: z.object({\r\n        location: z.string(),\r\n      }),\r\n      execute: async ({ location }) => {\r\n        return { temp: 72, condition: 'sunny', unit: 'fahrenheit' };\r\n      },\r\n    }),\r\n    convertTemp: tool({\r\n      description: 'Convert temperature between units',\r\n      inputSchema: z.object({\r\n        fahrenheit: z.number(),\r\n      }),\r\n      execute: async ({ fahrenheit }) => {\r\n        return { celsius: (fahrenheit - 32) * 5/9 };\r\n      },\r\n    }),\r\n  },\r\n});\r\n\r\nconst result = await weatherAgent.run({\r\n  messages: [\r\n    { role: 'user', content: 'What is the weather in SF in Celsius?' },\r\n  ],\r\n});\r\n\r\nconsole.log(result.text);\r\n// Agent will call getWeather, then convertTemp, then respond\r\n```\r\n\r\n**When to Use Agent vs Raw generateText:**\r\n- **Use Agent when:** Multiple tools, complex workflows, multi-step reasoning\r\n- **Use generateText when:** Simple single-step, one or two tools, full control needed\r\n\r\n---\r\n\r\n### Multi-Step Execution\r\n\r\nControl when multi-step execution stops with `stopWhen` conditions.\r\n\r\n```typescript\r\nimport { generateText, stopWhen, stepCountIs, hasToolCall } from 'ai';\r\nimport { openai } from '@ai-sdk/openai';\r\n\r\n// Stop after specific number of steps\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  tools: { /* ... */ },\r\n  prompt: 'Research TypeScript and create a summary',\r\n  stopWhen: stepCountIs(5),  // Max 5 steps (tool calls + responses)\r\n});\r\n\r\n// Stop when specific tool is called\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  tools: {\r\n    research: tool({ /* ... */ }),\r\n    finalize: tool({ /* ... */ }),\r\n  },\r\n  prompt: 'Research and finalize a report',\r\n  stopWhen: hasToolCall('finalize'),  // Stop when finalize is called\r\n});\r\n\r\n// Combine conditions\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  tools: { /* ... */ },\r\n  prompt: 'Complex task',\r\n  stopWhen: (step) => step.stepCount >= 10 || step.hasToolCall('finish'),\r\n});\r\n```\r\n\r\n**v5 Change:**\r\n`maxSteps` parameter removed. Use `stopWhen(stepCountIs(n))` instead.\r\n\r\n---\r\n\r\n### Dynamic Tools (v5 New Feature)\r\n\r\nAdd tools at runtime based on context:\r\n\r\n```typescript\r\nconst result = await generateText({\r\n  model: openai('gpt-4'),\r\n  tools: (context) => {\r\n    // Context includes messages, step count, etc.\r\n    const baseTool = {\r\n      search: tool({ /* ... */ }),\r\n    };\r\n\r\n    // Add tools based on context\r\n    if (context.messages.some(m => m.content.includes('weather'))) {\r\n      baseTool.weather = tool({ /* ... */ });\r\n    }\r\n\r\n    return baseTools;\r\n  },\r\n  prompt: 'Help me with my task',\r\n});\r\n```\r\n\r\n---"
  }
}