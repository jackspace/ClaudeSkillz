{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/best-practices.md",
      "references/consumer-api.md",
      "references/producer-api.md",
      "references/wrangler-commands.md"
    ]
  },
  "content": "**Status**: Production Ready ✅\r\n**Last Updated**: 2025-10-21\r\n**Dependencies**: cloudflare-worker-base (for Worker setup)\r\n**Latest Versions**: wrangler@4.43.0, @cloudflare/workers-types@4.20251014.0\r\n\r\n---\r\n\r\n\r\n### 1. Create a Queue\r\n\r\n```bash\r\nnpx wrangler queues create my-queue\r\n\r\n\r\nnpx wrangler queues list\r\n\r\nnpx wrangler queues info my-queue\r\n```\r\n\r\n### 2. Set Up Producer (Send Messages)\r\n\r\n**wrangler.jsonc:**\r\n\r\n```jsonc\r\n{\r\n  \"name\": \"my-producer\",\r\n  \"main\": \"src/index.ts\",\r\n  \"compatibility_date\": \"2025-10-11\",\r\n  \"queues\": {\r\n    \"producers\": [\r\n      {\r\n        \"binding\": \"MY_QUEUE\",           // Available as env.MY_QUEUE\r\n        \"queue\": \"my-queue\"               // Queue name from step 1\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**src/index.ts (Producer):**\r\n\r\n```typescript\r\nimport { Hono } from 'hono';\r\n\r\ntype Bindings = {\r\n  MY_QUEUE: Queue;\r\n};\r\n\r\nconst app = new Hono<{ Bindings: Bindings }>();\r\n\r\n// Send single message\r\napp.post('/send', async (c) => {\r\n  const body = await c.req.json();\r\n\r\n  await c.env.MY_QUEUE.send({\r\n    userId: body.userId,\r\n    action: 'process-order',\r\n    timestamp: Date.now(),\r\n  });\r\n\r\n  return c.json({ status: 'queued' });\r\n});\r\n\r\n// Send batch of messages\r\napp.post('/send-batch', async (c) => {\r\n  const items = await c.req.json();\r\n\r\n  await c.env.MY_QUEUE.sendBatch(\r\n    items.map((item) => ({\r\n      body: { userId: item.userId, action: item.action },\r\n    }))\r\n  );\r\n\r\n  return c.json({ status: 'queued', count: items.length });\r\n});\r\n\r\nexport default app;\r\n```\r\n\r\n### 3. Set Up Consumer (Process Messages)\r\n\r\n**Create consumer Worker:**\r\n\r\n```bash\r\nnpm create cloudflare@latest my-consumer -- --type hello-world --ts\r\ncd my-consumer\r\n```\r\n\r\n**wrangler.jsonc:**\r\n\r\n```jsonc\r\n{\r\n  \"name\": \"my-consumer\",\r\n  \"main\": \"src/index.ts\",\r\n  \"compatibility_date\": \"2025-10-11\",\r\n  \"queues\": {\r\n    \"consumers\": [\r\n      {\r\n        \"queue\": \"my-queue\",              // Queue to consume from\r\n        \"max_batch_size\": 10,             // Process up to 10 messages at once\r\n        \"max_batch_timeout\": 5            // Or wait max 5 seconds\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**src/index.ts (Consumer):**\r\n\r\n```typescript\r\nexport default {\r\n  async queue(\r\n    batch: MessageBatch,\r\n    env: Env,\r\n    ctx: ExecutionContext\r\n  ): Promise<void> {\r\n    console.log(`Processing batch of ${batch.messages.length} messages`);\r\n\r\n    for (const message of batch.messages) {\r\n      console.log('Message:', message.id, message.body, `Attempt: ${message.attempts}`);\r\n\r\n      // Your processing logic here\r\n      await processMessage(message.body);\r\n    }\r\n\r\n    // Implicit acknowledgement: if this function returns without error,\r\n    // all messages are automatically acknowledged\r\n  },\r\n};\r\n\r\nasync function processMessage(body: any) {\r\n  // Process the message\r\n  console.log('Processing:', body);\r\n}\r\n```\r\n\r\n### 4. Deploy and Test\r\n\r\n```bash\r\ncd my-producer\r\nnpm run deploy\r\n\r\ncd my-consumer\r\nnpm run deploy\r\n\r\ncurl -X POST https://my-producer.<your-subdomain>.workers.dev/send \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\"userId\": \"123\", \"action\": \"welcome-email\"}'\r\n\r\n\r\n### 1. Basic Consumer (Implicit Acknowledgement)\r\n\r\n**Best for:** Idempotent operations where retries are safe\r\n\r\n```typescript\r\nexport default {\r\n  async queue(batch: MessageBatch, env: Env): Promise<void> {\r\n    for (const message of batch.messages) {\r\n      // Process message\r\n      await sendEmail(message.body.email, message.body.content);\r\n    }\r\n\r\n    // Implicit ack: returning successfully acknowledges ALL messages\r\n  },\r\n};\r\n```\r\n\r\n**Behavior:**\r\n- If function returns successfully → all messages acknowledged\r\n- If function throws error → **all messages retried**\r\n- Simple but can cause duplicate processing on partial failures\r\n\r\n---\r\n\r\n### 2. Explicit Acknowledgement (Non-Idempotent Operations)\r\n\r\n**Best for:** Database writes, API calls, financial transactions\r\n\r\n```typescript\r\nexport default {\r\n  async queue(batch: MessageBatch, env: Env): Promise<void> {\r\n    for (const message of batch.messages) {\r\n      try {\r\n        // Non-idempotent operation\r\n        await env.DB.prepare(\r\n          'INSERT INTO orders (id, user_id, amount) VALUES (?, ?, ?)'\r\n        ).bind(message.body.orderId, message.body.userId, message.body.amount).run();\r\n\r\n        // Explicitly acknowledge success\r\n        message.ack();\r\n      } catch (error) {\r\n        console.error(`Failed to process ${message.id}:`, error);\r\n\r\n        // Don't ack - will retry (or let it fail)\r\n        // Optionally: message.retry() for explicit retry\r\n      }\r\n    }\r\n  },\r\n};\r\n```\r\n\r\n**Why explicit ack?**\r\n- Prevents duplicate writes if one message in batch fails\r\n- Only successfully processed messages are acknowledged\r\n- Failed messages retry independently\r\n\r\n---\r\n\r\n### 3. Retry with Exponential Backoff\r\n\r\n**Best for:** Rate-limited APIs, temporary failures\r\n\r\n```typescript\r\nexport default {\r\n  async queue(batch: MessageBatch, env: Env): Promise<void> {\r\n    for (const message of batch.messages) {\r\n      try {\r\n        // Call rate-limited API\r\n        await fetch('https://api.example.com/process', {\r\n          method: 'POST',\r\n          body: JSON.stringify(message.body),\r\n        });\r\n\r\n        message.ack();\r\n      } catch (error) {\r\n        if (error.status === 429) {\r\n          // Rate limited - retry with exponential backoff\r\n          const delaySeconds = Math.min(\r\n            60 * Math.pow(2, message.attempts - 1), // 60s, 120s, 240s, ...\r\n            3600 // Max 1 hour\r\n          );\r\n\r\n          console.log(`Rate limited. Retrying in ${delaySeconds}s (attempt ${message.attempts})`);\r\n          message.retry({ delaySeconds });\r\n        } else {\r\n          // Other error - retry immediately\r\n          message.retry();\r\n        }\r\n      }\r\n    }\r\n  },\r\n};\r\n```\r\n\r\n---\r\n\r\n### 4. Dead Letter Queue (DLQ) Pattern\r\n\r\n**Best for:** Handling permanently failed messages\r\n\r\n**Setup DLQ:**\r\n\r\n```bash\r\nnpx wrangler queues create my-dlq\r\n\r\n\r\n### Create Queue\r\n\r\n```bash\r\nnpx wrangler queues create my-queue\r\n\r\nnpx wrangler queues create my-queue --message-retention-period-secs 1209600\r\n\r\nnpx wrangler queues create my-queue --delivery-delay-secs 60\r\n```\r\n\r\n---\r\n\r\n### List Queues\r\n\r\n```bash\r\nnpx wrangler queues list\r\n```\r\n\r\n---\r\n\r\n### Get Queue Info\r\n\r\n```bash\r\nnpx wrangler queues info my-queue\r\n\r\n```\r\n\r\n---\r\n\r\n### Update Queue\r\n\r\n```bash\r\nnpx wrangler queues update my-queue --message-retention-period-secs 604800\r\n\r\nnpx wrangler queues update my-queue --delivery-delay-secs 3600\r\n```\r\n\r\n---\r\n\r\n### Delete Queue\r\n\r\n```bash\r\nnpx wrangler queues delete my-queue\r\n\r\n```\r\n\r\n---\r\n\r\n### Consumer Management\r\n\r\n```bash\r\nnpx wrangler queues consumer add my-queue my-consumer-worker \\\r\n  --batch-size 50 \\\r\n  --batch-timeout 10 \\\r\n  --message-retries 5 \\\r\n  --max-concurrency 20 \\\r\n  --retry-delay-secs 300\r\n\r\nnpx wrangler queues consumer remove my-queue my-consumer-worker\r\n```\r\n\r\n---\r\n\r\n### Purge Queue\r\n\r\n```bash\r\nnpx wrangler queues purge my-queue\r\n```\r\n\r\n---\r\n\r\n### Pause/Resume Delivery\r\n\r\n```bash\r\nnpx wrangler queues pause-delivery my-queue\r\n\r\n\r\n### Issue: Messages not being delivered to consumer\r\n\r\n**Possible causes:**\r\n1. Consumer not deployed\r\n2. Wrong queue name in wrangler.jsonc\r\n3. Delivery paused\r\n4. Consumer throwing errors\r\n\r\n**Solution:**\r\n\r\n```bash\r\nnpx wrangler queues info my-queue\r\n\r\nnpx wrangler queues resume-delivery my-queue\r\n\r\nnpx wrangler tail my-consumer\r\n```\r\n\r\n---\r\n\r\n### Issue: Entire batch retried when one message fails\r\n\r\n**Cause:** Using implicit acknowledgement with non-idempotent operations\r\n\r\n**Solution:** Use explicit ack()\r\n\r\n```typescript\r\n// ✅ Explicit ack\r\nfor (const message of batch.messages) {\r\n  try {\r\n    await dbWrite(message.body);\r\n    message.ack(); // Only ack on success\r\n  } catch (error) {\r\n    console.error(`Failed: ${message.id}`);\r\n    // Don't ack - will retry\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n### Issue: Messages deleted without processing\r\n\r\n**Cause:** No Dead Letter Queue configured\r\n\r\n**Solution:**\r\n\r\n```bash\r\nnpx wrangler queues create my-dlq",
  "name": "cloudflare-queues",
  "id": "cloudflare-queues",
  "sections": {
    "Related Documentation": "- [Cloudflare Queues Docs](https://developers.cloudflare.com/queues/)\r\n- [How Queues Works](https://developers.cloudflare.com/queues/reference/how-queues-works/)\r\n- [JavaScript APIs](https://developers.cloudflare.com/queues/configuration/javascript-apis/)\r\n- [Batching & Retries](https://developers.cloudflare.com/queues/configuration/batching-retries/)\r\n- [Consumer Concurrency](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/)\r\n- [Dead Letter Queues](https://developers.cloudflare.com/queues/configuration/dead-letter-queues/)\r\n- [Wrangler Commands](https://developers.cloudflare.com/queues/reference/wrangler-commands/)\r\n- [Limits](https://developers.cloudflare.com/queues/platform/limits/)\r\n- [Pricing](https://developers.cloudflare.com/queues/platform/pricing/)\r\n\r\n---\r\n\r\n**Last Updated**: 2025-10-21\r\n**Version**: 1.0.0\r\n**Maintainer**: Jeremy Dawes | jeremy@jezweb.net",
    "Never Do ❌": "1. **Never assume message ordering** - not guaranteed FIFO\r\n2. **Never rely on implicit ack for non-idempotent ops** - use explicit ack()\r\n3. **Never send messages >128 KB** - will fail\r\n4. **Never delete queues with active messages** - data loss\r\n5. **Never skip DLQ configuration** in production\r\n6. **Never exceed 5000 msg/s per queue** - rate limit error\r\n7. **Never process messages synchronously in loop** - use Promise.all()\r\n8. **Never ignore message.attempts** - use for backoff logic\r\n9. **Never set max_concurrency=1** unless you have a very specific reason\r\n10. **Never forget to ack()** in explicit acknowledgement patterns\r\n\r\n---",
    "Limits & Quotas": "| Feature | Limit |\r\n|---------|-------|\r\n| **Queues per account** | 10,000 |\r\n| **Message size** | 128 KB (includes ~100 bytes metadata) |\r\n| **Message retries** | 100 max |\r\n| **Batch size** | 1-100 messages |\r\n| **Batch timeout** | 0-60 seconds |\r\n| **Messages per sendBatch** | 100 (or 256 KB total) |\r\n| **Queue throughput** | 5,000 messages/second per queue |\r\n| **Message retention** | 4 days (default), 14 days (max) |\r\n| **Queue backlog size** | 25 GB per queue |\r\n| **Concurrent consumers** | 250 (push-based, auto-scale) |\r\n| **Consumer duration** | 15 minutes (wall clock) |\r\n| **Consumer CPU time** | 30 seconds (default), 5 minutes (max) |\r\n| **Visibility timeout** | 12 hours (pull consumers) |\r\n| **Message delay** | 12 hours (max) |\r\n| **API rate limit** | 1200 requests / 5 minutes |\r\n\r\n---",
    "Always Do ✅": "1. **Configure Dead Letter Queue** for production queues\r\n2. **Use explicit ack()** for non-idempotent operations (DB writes, API calls)\r\n3. **Validate message size** before sending (<128 KB)\r\n4. **Use sendBatch()** for multiple messages (more efficient)\r\n5. **Implement exponential backoff** for retries\r\n6. **Set appropriate batch settings** based on workload\r\n7. **Monitor queue backlog** and consumer errors\r\n8. **Use ctx.waitUntil()** for async cleanup in consumers\r\n9. **Handle errors gracefully** - log, alert, retry\r\n10. **Let concurrency auto-scale** (don't set max_concurrency unless needed)\r\n\r\n---",
    "Pricing": "**Requires Workers Paid plan** ($5/month)\r\n\r\n**Operations Pricing:**\r\n- First 1,000,000 operations/month: **FREE**\r\n- After that: **$0.40 per million operations**\r\n\r\n**What counts as an operation:**\r\n- Each 64 KB chunk written, read, or deleted\r\n- Messages >64 KB count as multiple operations:\r\n  - 65 KB message = 2 operations\r\n  - 127 KB message = 2 operations\r\n  - 128 KB message = 2 operations\r\n\r\n**Typical message lifecycle:**\r\n- 1 write + 1 read + 1 delete = **3 operations**\r\n\r\n**Retries:**\r\n- Each retry = additional **read operation**\r\n- Message retried 3 times = 1 write + 4 reads + 1 delete = **6 operations**\r\n\r\n**Dead Letter Queue:**\r\n- Writing to DLQ = additional **write operation**\r\n\r\n**Cost examples:**\r\n- 1M messages/month (no retries): ((1M × 3) - 1M) / 1M × $0.40 = **$0.80**\r\n- 10M messages/month: ((10M × 3) - 1M) / 1M × $0.40 = **$11.60**\r\n- 100M messages/month: ((100M × 3) - 1M) / 1M × $0.40 = **$119.60**\r\n\r\n---",
    "Consumer Configuration": "### Batch Settings\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [\r\n      {\r\n        \"queue\": \"my-queue\",\r\n        \"max_batch_size\": 100,        // 1-100 messages (default: 10)\r\n        \"max_batch_timeout\": 30       // 0-60 seconds (default: 5)\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**How batching works:**\r\n- Consumer called when **either** condition met:\r\n  - `max_batch_size` messages accumulated\r\n  - `max_batch_timeout` seconds elapsed (whichever comes first)\r\n\r\n**Example:**\r\n- `max_batch_size: 100`, `max_batch_timeout: 10`\r\n- If 100 messages arrive in 3 seconds → batch delivered immediately\r\n- If only 50 messages arrive in 10 seconds → batch of 50 delivered\r\n\r\n**Tuning guidelines:**\r\n- **High volume, low latency** → `max_batch_size: 100`, `max_batch_timeout: 1`\r\n- **Low volume, batch writes** → `max_batch_size: 50`, `max_batch_timeout: 30`\r\n- **Cost optimization** → Larger batches = fewer invocations\r\n\r\n---\r\n\r\n### Retry Settings\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [\r\n      {\r\n        \"queue\": \"my-queue\",\r\n        \"max_retries\": 5,             // 0-100 (default: 3)\r\n        \"retry_delay\": 300            // Seconds (default: 0)\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**`max_retries`:**\r\n- Number of times to retry failed message\r\n- After max retries:\r\n  - With DLQ → message sent to DLQ\r\n  - Without DLQ → **message deleted permanently**\r\n\r\n**`retry_delay`:**\r\n- Default delay for all retried messages (seconds)\r\n- Can be overridden with `message.retry({ delaySeconds })`\r\n- Maximum delay: 43200 seconds (12 hours)\r\n\r\n---\r\n\r\n### Concurrency Settings\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [\r\n      {\r\n        \"queue\": \"my-queue\",\r\n        \"max_concurrency\": 10         // 1-250 (default: auto-scale)\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**How concurrency works:**\r\n- Queues auto-scales consumers based on backlog\r\n- Default: Scales up to 250 concurrent invocations\r\n- Setting `max_concurrency` limits scaling\r\n\r\n**When to set max_concurrency:**\r\n- ✅ Upstream API has rate limits\r\n- ✅ Database connection limits\r\n- ✅ Want to control costs\r\n- ❌ Most cases - leave unset for best performance\r\n\r\n**Auto-scaling triggers:**\r\n- Growing backlog (messages accumulating)\r\n- High error rate\r\n- Processing speed vs. incoming rate\r\n\r\n---\r\n\r\n### Dead Letter Queue\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [\r\n      {\r\n        \"queue\": \"my-queue\",\r\n        \"max_retries\": 3,\r\n        \"dead_letter_queue\": \"my-dlq\"  // Name of DLQ\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**CRITICAL:**\r\n- DLQ must be created separately: `npx wrangler queues create my-dlq`\r\n- Without DLQ, failed messages are **deleted permanently**\r\n- Messages in DLQ persist for 4 days without consumer\r\n- Always configure DLQ for production queues\r\n\r\n---",
    "TypeScript Types": "```typescript\r\n// Queue binding (producer)\r\ninterface Queue<Body = any> {\r\n  send(body: Body, options?: QueueSendOptions): Promise<void>;\r\n  sendBatch(\r\n    messages: Iterable<MessageSendRequest<Body>>,\r\n    options?: QueueSendBatchOptions\r\n  ): Promise<void>;\r\n}\r\n\r\ninterface QueueSendOptions {\r\n  delaySeconds?: number;\r\n}\r\n\r\ninterface MessageSendRequest<Body = any> {\r\n  body: Body;\r\n  delaySeconds?: number;\r\n}\r\n\r\ninterface QueueSendBatchOptions {\r\n  delaySeconds?: number;\r\n}\r\n\r\n// Consumer handler\r\nexport default {\r\n  queue(\r\n    batch: MessageBatch,\r\n    env: Env,\r\n    ctx: ExecutionContext\r\n  ): Promise<void>;\r\n}\r\n\r\ninterface MessageBatch<Body = unknown> {\r\n  readonly queue: string;\r\n  readonly messages: Message<Body>[];\r\n  ackAll(): void;\r\n  retryAll(options?: QueueRetryOptions): void;\r\n}\r\n\r\ninterface Message<Body = unknown> {\r\n  readonly id: string;\r\n  readonly timestamp: Date;\r\n  readonly body: Body;\r\n  readonly attempts: number;\r\n  ack(): void;\r\n  retry(options?: QueueRetryOptions): void;\r\n}\r\n\r\ninterface QueueRetryOptions {\r\n  delaySeconds?: number;\r\n}\r\n```\r\n\r\n---",
    "Error Handling": "### Common Errors\r\n\r\n#### 1. Message Too Large\r\n\r\n```typescript\r\n// ❌ Bad: Message >128 KB\r\nawait env.MY_QUEUE.send({\r\n  data: largeArray, // >128 KB\r\n});\r\n\r\n// ✅ Good: Check size before sending\r\nconst message = { data: largeArray };\r\nconst size = new TextEncoder().encode(JSON.stringify(message)).length;\r\n\r\nif (size > 128000) {\r\n  // Store in R2, send reference\r\n  const key = `messages/${crypto.randomUUID()}.json`;\r\n  await env.MY_BUCKET.put(key, JSON.stringify(message));\r\n  await env.MY_QUEUE.send({ type: 'large-message', r2Key: key });\r\n} else {\r\n  await env.MY_QUEUE.send(message);\r\n}\r\n```\r\n\r\n---\r\n\r\n#### 2. Throughput Exceeded\r\n\r\n```typescript\r\n// ❌ Bad: Exceeding 5000 msg/s per queue\r\nfor (let i = 0; i < 10000; i++) {\r\n  await env.MY_QUEUE.send({ id: i }); // Too fast!\r\n}\r\n\r\n// ✅ Good: Use sendBatch\r\nconst messages = Array.from({ length: 10000 }, (_, i) => ({\r\n  body: { id: i },\r\n}));\r\n\r\n// Send in batches of 100\r\nfor (let i = 0; i < messages.length; i += 100) {\r\n  await env.MY_QUEUE.sendBatch(messages.slice(i, i + 100));\r\n}\r\n\r\n// ✅ Even better: Rate limit with delay\r\nfor (let i = 0; i < messages.length; i += 100) {\r\n  await env.MY_QUEUE.sendBatch(messages.slice(i, i + 100));\r\n  if (i + 100 < messages.length) {\r\n    await new Promise(resolve => setTimeout(resolve, 100)); // 100ms delay\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n#### 3. Consumer Timeout\r\n\r\n```typescript\r\n// ❌ Bad: Long processing without CPU limit increase\r\nexport default {\r\n  async queue(batch: MessageBatch): Promise<void> {\r\n    for (const message of batch.messages) {\r\n      await processForMinutes(message.body); // CPU timeout!\r\n    }\r\n  },\r\n};\r\n\r\n// ✅ Good: Increase CPU limit in wrangler.jsonc\r\n```\r\n\r\n**wrangler.jsonc:**\r\n\r\n```jsonc\r\n{\r\n  \"limits\": {\r\n    \"cpu_ms\": 300000  // 5 minutes (max allowed)\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n#### 4. Backlog Growing\r\n\r\n```typescript\r\n// Issue: Consumer too slow, backlog growing\r\n\r\n// ✅ Solution 1: Increase batch size\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [{\r\n      \"queue\": \"my-queue\",\r\n      \"max_batch_size\": 100  // Process more per invocation\r\n    }]\r\n  }\r\n}\r\n\r\n// ✅ Solution 2: Let concurrency auto-scale (don't set max_concurrency)\r\n\r\n// ✅ Solution 3: Optimize consumer code\r\nexport default {\r\n  async queue(batch: MessageBatch, env: Env): Promise<void> {\r\n    // Process in parallel\r\n    await Promise.all(\r\n      batch.messages.map(async (message) => {\r\n        await process(message.body);\r\n        message.ack();\r\n      })\r\n    );\r\n  },\r\n};\r\n```\r\n\r\n---",
    "Complete Producer API": "### `send()` - Send Single Message\r\n\r\n```typescript\r\ninterface QueueSendOptions {\r\n  delaySeconds?: number;  // Delay delivery (0-43200 seconds / 12 hours)\r\n}\r\n\r\nawait env.MY_QUEUE.send(body: any, options?: QueueSendOptions);\r\n```\r\n\r\n**Examples:**\r\n\r\n```typescript\r\n// Simple send\r\nawait env.MY_QUEUE.send({ userId: '123', action: 'send-email' });\r\n\r\n// Send with delay (10 minutes)\r\nawait env.MY_QUEUE.send(\r\n  { userId: '123', action: 'reminder' },\r\n  { delaySeconds: 600 }\r\n);\r\n\r\n// Send structured data\r\nawait env.MY_QUEUE.send({\r\n  type: 'order-confirmation',\r\n  orderId: 'ORD-123',\r\n  email: 'user@example.com',\r\n  items: [{ sku: 'ITEM-1', quantity: 2 }],\r\n  total: 49.99,\r\n  timestamp: Date.now(),\r\n});\r\n```\r\n\r\n**CRITICAL:**\r\n- Message body must be JSON serializable (structured clone algorithm)\r\n- Maximum message size: **128 KB** (including ~100 bytes internal metadata)\r\n- Messages >128 KB will fail - split them or store in R2 and send reference\r\n\r\n---\r\n\r\n### `sendBatch()` - Send Multiple Messages\r\n\r\n```typescript\r\ninterface MessageSendRequest<Body = any> {\r\n  body: Body;\r\n  delaySeconds?: number;\r\n}\r\n\r\ninterface QueueSendBatchOptions {\r\n  delaySeconds?: number;  // Default delay for all messages\r\n}\r\n\r\nawait env.MY_QUEUE.sendBatch(\r\n  messages: Iterable<MessageSendRequest>,\r\n  options?: QueueSendBatchOptions\r\n);\r\n```\r\n\r\n**Examples:**\r\n\r\n```typescript\r\n// Send batch of messages\r\nawait env.MY_QUEUE.sendBatch([\r\n  { body: { userId: '1', action: 'email' } },\r\n  { body: { userId: '2', action: 'email' } },\r\n  { body: { userId: '3', action: 'email' } },\r\n]);\r\n\r\n// Send batch with individual delays\r\nawait env.MY_QUEUE.sendBatch([\r\n  { body: { task: 'task1' }, delaySeconds: 60 },   // 1 min\r\n  { body: { task: 'task2' }, delaySeconds: 300 },  // 5 min\r\n  { body: { task: 'task3' }, delaySeconds: 600 },  // 10 min\r\n]);\r\n\r\n// Send batch with default delay\r\nawait env.MY_QUEUE.sendBatch(\r\n  [\r\n    { body: { task: 'task1' } },\r\n    { body: { task: 'task2' } },\r\n  ],\r\n  { delaySeconds: 3600 } // All delayed by 1 hour\r\n);\r\n\r\n// Dynamic batch from array\r\nconst tasks = await getTasks();\r\nawait env.MY_QUEUE.sendBatch(\r\n  tasks.map((task) => ({\r\n    body: {\r\n      taskId: task.id,\r\n      userId: task.userId,\r\n      priority: task.priority,\r\n    },\r\n  }))\r\n);\r\n```\r\n\r\n**Limits:**\r\n- Maximum 100 messages per batch\r\n- Maximum 256 KB total batch size\r\n- Each message still limited to 128 KB individually\r\n\r\n---",
    "Production Checklist": "Before deploying to production:\r\n\r\n- [ ] Dead Letter Queue created and configured\r\n- [ ] Explicit ack() used for non-idempotent operations\r\n- [ ] Message size validation (<128 KB)\r\n- [ ] Batch settings optimized for workload\r\n- [ ] Retry settings configured (max_retries, retry_delay)\r\n- [ ] Consumer concurrency settings appropriate\r\n- [ ] Error handling and logging implemented\r\n- [ ] Monitoring and alerting set up\r\n- [ ] DLQ consumer deployed\r\n- [ ] Exponential backoff for rate-limited APIs\r\n- [ ] CPU limits increased if needed (limits.cpu_ms)\r\n- [ ] Message retention period appropriate\r\n- [ ] Tested under load\r\n\r\n---",
    "Complete Consumer API": "### Queue Handler Function\r\n\r\n```typescript\r\nexport default {\r\n  async queue(\r\n    batch: MessageBatch,\r\n    env: Env,\r\n    ctx: ExecutionContext\r\n  ): Promise<void> {\r\n    // Process messages\r\n  },\r\n};\r\n```\r\n\r\n**Parameters:**\r\n- `batch` - MessageBatch object containing messages\r\n- `env` - Environment bindings (KV, D1, R2, etc.)\r\n- `ctx` - Execution context for `waitUntil()`, `passThroughOnException()`\r\n\r\n---\r\n\r\n### MessageBatch Interface\r\n\r\n```typescript\r\ninterface MessageBatch<Body = unknown> {\r\n  readonly queue: string;              // Queue name\r\n  readonly messages: Message<Body>[];  // Array of messages\r\n  ackAll(): void;                      // Acknowledge all messages\r\n  retryAll(options?: QueueRetryOptions): void;  // Retry all messages\r\n}\r\n```\r\n\r\n**Properties:**\r\n\r\n- **`queue`** - Name of the queue this batch came from\r\n  - Useful when one consumer handles multiple queues\r\n\r\n- **`messages`** - Array of Message objects\r\n  - Ordering is **best effort**, not guaranteed\r\n  - Process order should not be assumed\r\n\r\n**Methods:**\r\n\r\n- **`ackAll()`** - Mark all messages as successfully delivered\r\n  - Even if handler throws error, these messages won't retry\r\n  - Use when you've safely processed all messages\r\n\r\n- **`retryAll(options?)`** - Mark all messages for retry\r\n  - Messages re-queued immediately (or after delay)\r\n  - Counts towards max_retries limit\r\n\r\n---\r\n\r\n### Message Interface\r\n\r\n```typescript\r\ninterface Message<Body = unknown> {\r\n  readonly id: string;          // Unique message ID\r\n  readonly timestamp: Date;     // When message was sent\r\n  readonly body: Body;          // Message content\r\n  readonly attempts: number;    // Retry count (starts at 1)\r\n  ack(): void;                  // Acknowledge this message\r\n  retry(options?: QueueRetryOptions): void;  // Retry this message\r\n}\r\n```\r\n\r\n**Properties:**\r\n\r\n- **`id`** - System-generated unique ID (UUID)\r\n- **`timestamp`** - Date object when message was sent to queue\r\n- **`body`** - Your message content (any JSON serializable type)\r\n- **`attempts`** - Number of times consumer has processed this message\r\n  - Starts at 1 on first delivery\r\n  - Increments on each retry\r\n  - Use for exponential backoff: `delaySeconds: 60 * message.attempts`\r\n\r\n**Methods:**\r\n\r\n- **`ack()`** - Mark message as successfully delivered\r\n  - Message won't be retried even if handler fails later\r\n  - **Critical for non-idempotent operations** (DB writes, API calls)\r\n\r\n- **`retry(options?)`** - Mark message for retry\r\n  - Re-queued immediately or after `delaySeconds`\r\n  - Counts towards max_retries (default 3)\r\n\r\n---\r\n\r\n### QueueRetryOptions\r\n\r\n```typescript\r\ninterface QueueRetryOptions {\r\n  delaySeconds?: number;  // Delay retry (0-43200 seconds / 12 hours)\r\n}\r\n```\r\n\r\n**Example:**\r\n\r\n```typescript\r\n// Retry immediately\r\nmessage.retry();\r\n\r\n// Retry after 5 minutes\r\nmessage.retry({ delaySeconds: 300 });\r\n\r\n// Exponential backoff based on attempts\r\nmessage.retry({\r\n  delaySeconds: Math.min(60 * Math.pow(2, message.attempts - 1), 3600),\r\n});\r\n```\r\n\r\n---",
    "Troubleshooting": "```\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [{\r\n      \"queue\": \"my-queue\",\r\n      \"dead_letter_queue\": \"my-dlq\"\r\n    }]\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n### Issue: Consumer not auto-scaling\r\n\r\n**Possible causes:**\r\n1. `max_concurrency` set to 1\r\n2. Consumer returning errors (not processing)\r\n3. Batch processing too fast (no backlog)\r\n\r\n**Solution:**\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [{\r\n      \"queue\": \"my-queue\",\r\n      // Don't set max_concurrency - let it auto-scale\r\n      \"max_batch_size\": 50  // Increase batch size instead\r\n    }]\r\n  }\r\n}\r\n```\r\n\r\n---",
    "Wrangler Commands": "npx wrangler queues resume-delivery my-queue\r\n```\r\n\r\n**Use cases:**\r\n- Maintenance on consumer Workers\r\n- Temporarily stop processing\r\n- Debug issues without losing messages\r\n\r\n---",
    "Quick Start (10 Minutes)": "npx wrangler tail my-consumer\r\n```\r\n\r\n---",
    "Consumer Patterns": "```\r\n\r\n**wrangler.jsonc:**\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [\r\n      {\r\n        \"queue\": \"my-queue\",\r\n        \"max_batch_size\": 10,\r\n        \"max_retries\": 3,\r\n        \"dead_letter_queue\": \"my-dlq\"  // Failed messages go here\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**DLQ Consumer:**\r\n\r\n```typescript\r\n// Consumer for dead letter queue\r\nexport default {\r\n  async queue(batch: MessageBatch, env: Env): Promise<void> {\r\n    for (const message of batch.messages) {\r\n      // Log failed message\r\n      console.error('PERMANENTLY FAILED MESSAGE:', {\r\n        id: message.id,\r\n        attempts: message.attempts,\r\n        body: message.body,\r\n        timestamp: message.timestamp,\r\n      });\r\n\r\n      // Store in database for manual review\r\n      await env.DB.prepare(\r\n        'INSERT INTO failed_messages (id, body, attempts, failed_at) VALUES (?, ?, ?, ?)'\r\n      ).bind(\r\n        message.id,\r\n        JSON.stringify(message.body),\r\n        message.attempts,\r\n        new Date().toISOString()\r\n      ).run();\r\n\r\n      // Optionally: send alert to ops team\r\n      await sendAlert({\r\n        type: 'queue-dlq',\r\n        messageId: message.id,\r\n        queue: batch.queue,\r\n      });\r\n\r\n      // Acknowledge to remove from DLQ\r\n      message.ack();\r\n    }\r\n  },\r\n};\r\n```\r\n\r\n**How it works:**\r\n1. Message fails in main queue\r\n2. Retries up to `max_retries` (default 3)\r\n3. After max retries, sent to DLQ\r\n4. DLQ consumer processes failed messages\r\n5. Without DLQ, messages are **deleted permanently**\r\n\r\n---\r\n\r\n### 5. Multiple Queues, Single Consumer\r\n\r\n**Best for:** Centralized processing logic\r\n\r\n```typescript\r\nexport default {\r\n  async queue(batch: MessageBatch, env: Env): Promise<void> {\r\n    // Switch based on queue name\r\n    switch (batch.queue) {\r\n      case 'high-priority-queue':\r\n        await processHighPriority(batch.messages, env);\r\n        break;\r\n\r\n      case 'low-priority-queue':\r\n        await processLowPriority(batch.messages, env);\r\n        break;\r\n\r\n      case 'email-queue':\r\n        await processEmails(batch.messages, env);\r\n        break;\r\n\r\n      default:\r\n        console.warn(`Unknown queue: ${batch.queue}`);\r\n        // Log to DLQ or monitoring\r\n    }\r\n  },\r\n};\r\n\r\nasync function processHighPriority(messages: Message[], env: Env) {\r\n  for (const message of messages) {\r\n    // Process with urgency\r\n    await fastProcess(message.body);\r\n    message.ack();\r\n  }\r\n}\r\n\r\nasync function processLowPriority(messages: Message[], env: Env) {\r\n  for (const message of messages) {\r\n    // Can take longer\r\n    await slowProcess(message.body);\r\n    message.ack();\r\n  }\r\n}\r\n```\r\n\r\n**wrangler.jsonc:**\r\n\r\n```jsonc\r\n{\r\n  \"queues\": {\r\n    \"consumers\": [\r\n      { \"queue\": \"high-priority-queue\" },\r\n      { \"queue\": \"low-priority-queue\" },\r\n      { \"queue\": \"email-queue\" }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n---"
  }
}