{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/best-practices.md",
      "references/models-catalog.md"
    ]
  },
  "content": "Production-ready knowledge domain for building AI-powered applications with Cloudflare Workers AI.\r\n\r\n**Status**: Production Ready ✅\r\n**Last Updated**: 2025-10-21\r\n**Dependencies**: cloudflare-worker-base (for Worker setup)\r\n**Latest Versions**: wrangler@4.43.0, @cloudflare/workers-types@4.20251014.0\r\n\r\n---",
  "name": "cloudflare-workers-ai",
  "id": "cloudflare-workers-ai",
  "sections": {
    "OpenAI Compatibility": "Workers AI supports OpenAI-compatible endpoints.\r\n\r\n**Using OpenAI SDK:**\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI({\r\n  apiKey: env.CLOUDFLARE_API_KEY,\r\n  baseURL: `https://api.cloudflare.com/client/v4/accounts/${env.CLOUDFLARE_ACCOUNT_ID}/ai/v1`,\r\n});\r\n\r\n// Chat completions\r\nconst completion = await openai.chat.completions.create({\r\n  model: '@cf/meta/llama-3.1-8b-instruct',\r\n  messages: [{ role: 'user', content: 'Hello!' }],\r\n});\r\n\r\n// Embeddings\r\nconst embeddings = await openai.embeddings.create({\r\n  model: '@cf/baai/bge-base-en-v1.5',\r\n  input: 'Hello world',\r\n});\r\n```\r\n\r\n**Endpoints:**\r\n- `/v1/chat/completions` - Text generation\r\n- `/v1/embeddings` - Text embeddings\r\n\r\n---",
    "Common Patterns": "### Pattern 1: Chat Completion with History\r\n\r\n```typescript\r\napp.post('/chat', async (c) => {\r\n  const { messages } = await c.req.json<{\r\n    messages: Array<{ role: string; content: string }>;\r\n  }>();\r\n\r\n  const response = await c.env.AI.run('@cf/meta/llama-3.1-8b-instruct', {\r\n    messages,\r\n    stream: true,\r\n  });\r\n\r\n  return new Response(response, {\r\n    headers: { 'content-type': 'text/event-stream' },\r\n  });\r\n});\r\n```\r\n\r\n---\r\n\r\n### Pattern 2: RAG (Retrieval Augmented Generation)\r\n\r\n```typescript\r\n// Step 1: Generate embeddings\r\nconst embeddings = await env.AI.run('@cf/baai/bge-base-en-v1.5', {\r\n  text: [userQuery],\r\n});\r\n\r\nconst vector = embeddings.data[0];\r\n\r\n// Step 2: Search Vectorize\r\nconst matches = await env.VECTORIZE.query(vector, { topK: 3 });\r\n\r\n// Step 3: Build context from matches\r\nconst context = matches.matches.map((m) => m.metadata.text).join('\\n\\n');\r\n\r\n// Step 4: Generate response with context\r\nconst response = await env.AI.run('@cf/meta/llama-3.1-8b-instruct', {\r\n  messages: [\r\n    {\r\n      role: 'system',\r\n      content: `Answer using this context:\\n${context}`,\r\n    },\r\n    { role: 'user', content: userQuery },\r\n  ],\r\n  stream: true,\r\n});\r\n\r\nreturn new Response(response, {\r\n  headers: { 'content-type': 'text/event-stream' },\r\n});\r\n```\r\n\r\n---\r\n\r\n### Pattern 3: Structured Output with Zod\r\n\r\n```typescript\r\nimport { z } from 'zod';\r\n\r\nconst RecipeSchema = z.object({\r\n  name: z.string(),\r\n  ingredients: z.array(z.string()),\r\n  instructions: z.array(z.string()),\r\n  prepTime: z.number(),\r\n});\r\n\r\napp.post('/recipe', async (c) => {\r\n  const { dish } = await c.req.json<{ dish: string }>();\r\n\r\n  const response = await c.env.AI.run('@cf/meta/llama-3.1-8b-instruct', {\r\n    messages: [\r\n      {\r\n        role: 'user',\r\n        content: `Generate a recipe for ${dish}. Return ONLY valid JSON matching this schema: ${JSON.stringify(RecipeSchema.shape)}`,\r\n      },\r\n    ],\r\n  });\r\n\r\n  // Parse and validate\r\n  const recipe = RecipeSchema.parse(JSON.parse(response.response));\r\n\r\n  return c.json(recipe);\r\n});\r\n```\r\n\r\n---\r\n\r\n### Pattern 4: Image Generation + R2 Storage\r\n\r\n```typescript\r\napp.post('/generate-image', async (c) => {\r\n  const { prompt } = await c.req.json<{ prompt: string }>();\r\n\r\n  // Generate image\r\n  const imageStream = await c.env.AI.run('@cf/black-forest-labs/flux-1-schnell', {\r\n    prompt,\r\n  });\r\n\r\n  const imageBytes = await new Response(imageStream).bytes();\r\n\r\n  // Store in R2\r\n  const key = `images/${Date.now()}.png`;\r\n  await c.env.BUCKET.put(key, imageBytes, {\r\n    httpMetadata: { contentType: 'image/png' },\r\n  });\r\n\r\n  return c.json({\r\n    success: true,\r\n    url: `https://your-domain.com/${key}`,\r\n  });\r\n});\r\n```\r\n\r\n---",
    "References": "- [Workers AI Docs](https://developers.cloudflare.com/workers-ai/)\r\n- [Models Catalog](https://developers.cloudflare.com/workers-ai/models/)\r\n- [AI Gateway](https://developers.cloudflare.com/ai-gateway/)\r\n- [Pricing](https://developers.cloudflare.com/workers-ai/platform/pricing/)\r\n- [Limits](https://developers.cloudflare.com/workers-ai/platform/limits/)\r\n- [REST API](https://developers.cloudflare.com/workers-ai/get-started/rest-api/)",
    "Production Checklist": "### Before Deploying\r\n\r\n- [ ] **Enable AI Gateway** for cost tracking and logging\r\n- [ ] **Implement streaming** for all text generation endpoints\r\n- [ ] **Add rate limit retry** with exponential backoff\r\n- [ ] **Validate input length** to prevent token limit errors\r\n- [ ] **Set appropriate timeouts** (Workers: 30s CPU default, 5m max)\r\n- [ ] **Monitor neurons usage** in Cloudflare dashboard\r\n- [ ] **Test error handling** for model unavailable, rate limits\r\n- [ ] **Add input sanitization** to prevent prompt injection\r\n- [ ] **Configure CORS** if using from browser\r\n- [ ] **Plan for scale** - upgrade to Paid plan if needed\r\n\r\n### Error Handling\r\n\r\n```typescript\r\nasync function runAIWithRetry(\r\n  env: Env,\r\n  model: string,\r\n  inputs: any,\r\n  maxRetries = 3\r\n): Promise<any> {\r\n  let lastError: Error;\r\n\r\n  for (let i = 0; i < maxRetries; i++) {\r\n    try {\r\n      return await env.AI.run(model, inputs);\r\n    } catch (error) {\r\n      lastError = error as Error;\r\n      const message = lastError.message.toLowerCase();\r\n\r\n      // Rate limit - retry with backoff\r\n      if (message.includes('429') || message.includes('rate limit')) {\r\n        const delay = Math.pow(2, i) * 1000; // Exponential backoff\r\n        await new Promise((resolve) => setTimeout(resolve, delay));\r\n        continue;\r\n      }\r\n\r\n      // Other errors - throw immediately\r\n      throw error;\r\n    }\r\n  }\r\n\r\n  throw lastError!;\r\n}\r\n```\r\n\r\n### Monitoring\r\n\r\n```typescript\r\napp.use('*', async (c, next) => {\r\n  const start = Date.now();\r\n\r\n  await next();\r\n\r\n  // Log AI usage\r\n  console.log({\r\n    path: c.req.path,\r\n    duration: Date.now() - start,\r\n    logId: c.env.AI.aiGatewayLogId,\r\n  });\r\n});\r\n```\r\n\r\n---",
    "Rate Limits & Pricing": "### Rate Limits (per minute)\r\n\r\n| Task Type | Default Limit | Notes |\r\n|-----------|---------------|-------|\r\n| **Text Generation** | 300/min | Some fast models: 400-1500/min |\r\n| **Text Embeddings** | 3000/min | BGE-large: 1500/min |\r\n| **Image Generation** | 720/min | All image models |\r\n| **Vision Models** | 720/min | Image understanding |\r\n| **Translation** | 720/min | M2M100, Opus MT |\r\n| **Classification** | 2000/min | Text classification |\r\n| **Speech Recognition** | 720/min | Whisper models |\r\n\r\n### Pricing (Neurons-Based)\r\n\r\n**Free Tier:**\r\n- 10,000 neurons per day\r\n- Resets daily at 00:00 UTC\r\n\r\n**Paid Tier:**\r\n- $0.011 per 1,000 neurons\r\n- 10,000 neurons/day included\r\n- Unlimited usage above free allocation\r\n\r\n**Example Costs:**\r\n\r\n| Model | Input (1M tokens) | Output (1M tokens) |\r\n|-------|-------------------|-------------------|\r\n| Llama 3.2 1B | $0.027 | $0.201 |\r\n| Llama 3.1 8B | $0.088 | $0.606 |\r\n| BGE-base embeddings | $0.005 | N/A |\r\n| Flux image generation | ~$0.011/image | N/A |\r\n\r\n---",
    "Model Selection Guide": "### Text Generation (LLMs)\r\n\r\n| Model | Best For | Rate Limit | Size |\r\n|-------|----------|------------|------|\r\n| `@cf/meta/llama-3.1-8b-instruct` | General purpose, fast | 300/min | 8B |\r\n| `@cf/meta/llama-3.2-1b-instruct` | Ultra-fast, simple tasks | 300/min | 1B |\r\n| `@cf/qwen/qwen1.5-14b-chat-awq` | High quality, complex reasoning | 150/min | 14B |\r\n| `@cf/deepseek-ai/deepseek-r1-distill-qwen-32b` | Coding, technical content | 300/min | 32B |\r\n| `@hf/thebloke/mistral-7b-instruct-v0.1-awq` | Fast, efficient | 400/min | 7B |\r\n\r\n### Text Embeddings\r\n\r\n| Model | Dimensions | Best For | Rate Limit |\r\n|-------|-----------|----------|------------|\r\n| `@cf/baai/bge-base-en-v1.5` | 768 | General purpose RAG | 3000/min |\r\n| `@cf/baai/bge-large-en-v1.5` | 1024 | High accuracy search | 1500/min |\r\n| `@cf/baai/bge-small-en-v1.5` | 384 | Fast, low storage | 3000/min |\r\n\r\n### Image Generation\r\n\r\n| Model | Best For | Rate Limit | Speed |\r\n|-------|----------|------------|-------|\r\n| `@cf/black-forest-labs/flux-1-schnell` | High quality, photorealistic | 720/min | Fast |\r\n| `@cf/stabilityai/stable-diffusion-xl-base-1.0` | General purpose | 720/min | Medium |\r\n| `@cf/lykon/dreamshaper-8-lcm` | Artistic, stylized | 720/min | Fast |\r\n\r\n### Vision Models\r\n\r\n| Model | Best For | Rate Limit |\r\n|-------|----------|------------|\r\n| `@cf/meta/llama-3.2-11b-vision-instruct` | Image understanding | 720/min |\r\n| `@cf/unum/uform-gen2-qwen-500m` | Fast image captioning | 720/min |\r\n\r\n---",
    "Vercel AI SDK Integration": "```bash\r\nnpm install workers-ai-provider ai\r\n```\r\n\r\n```typescript\r\nimport { createWorkersAI } from 'workers-ai-provider';\r\nimport { generateText, streamText } from 'ai';\r\n\r\nconst workersai = createWorkersAI({ binding: env.AI });\r\n\r\n// Generate text\r\nconst result = await generateText({\r\n  model: workersai('@cf/meta/llama-3.1-8b-instruct'),\r\n  prompt: 'Write a poem',\r\n});\r\n\r\n// Stream text\r\nconst stream = streamText({\r\n  model: workersai('@cf/meta/llama-3.1-8b-instruct'),\r\n  prompt: 'Tell me a story',\r\n});\r\n```\r\n\r\n---",
    "Workers AI API Reference": "### `env.AI.run()`\r\n\r\nRun an AI model inference.\r\n\r\n**Signature:**\r\n```typescript\r\nasync env.AI.run(\r\n  model: string,\r\n  inputs: ModelInputs,\r\n  options?: { gateway?: { id: string; skipCache?: boolean } }\r\n): Promise<ModelOutput | ReadableStream>\r\n```\r\n\r\n**Parameters:**\r\n\r\n- `model` (string, required) - Model ID (e.g., `@cf/meta/llama-3.1-8b-instruct`)\r\n- `inputs` (object, required) - Model-specific inputs\r\n- `options` (object, optional) - Additional options\r\n  - `gateway` (object) - AI Gateway configuration\r\n    - `id` (string) - Gateway ID\r\n    - `skipCache` (boolean) - Skip AI Gateway cache\r\n\r\n**Returns:**\r\n\r\n- Non-streaming: `Promise<ModelOutput>` - JSON response\r\n- Streaming: `ReadableStream` - Server-sent events stream\r\n\r\n---\r\n\r\n### Text Generation Models\r\n\r\n**Input Format:**\r\n```typescript\r\n{\r\n  messages?: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>;\r\n  prompt?: string; // Deprecated, use messages\r\n  stream?: boolean; // Default: false\r\n  max_tokens?: number; // Max tokens to generate\r\n  temperature?: number; // 0.0-1.0, default varies by model\r\n  top_p?: number; // 0.0-1.0\r\n  top_k?: number;\r\n}\r\n```\r\n\r\n**Output Format (Non-Streaming):**\r\n```typescript\r\n{\r\n  response: string; // Generated text\r\n}\r\n```\r\n\r\n**Example:**\r\n```typescript\r\nconst response = await env.AI.run('@cf/meta/llama-3.1-8b-instruct', {\r\n  messages: [\r\n    { role: 'system', content: 'You are a helpful assistant.' },\r\n    { role: 'user', content: 'What is TypeScript?' },\r\n  ],\r\n  stream: false,\r\n});\r\n\r\nconsole.log(response.response);\r\n```\r\n\r\n---\r\n\r\n### Text Embeddings Models\r\n\r\n**Input Format:**\r\n```typescript\r\n{\r\n  text: string | string[]; // Single text or array of texts\r\n}\r\n```\r\n\r\n**Output Format:**\r\n```typescript\r\n{\r\n  shape: number[]; // [batch_size, embedding_dimensions]\r\n  data: number[][]; // Array of embedding vectors\r\n}\r\n```\r\n\r\n**Example:**\r\n```typescript\r\nconst embeddings = await env.AI.run('@cf/baai/bge-base-en-v1.5', {\r\n  text: ['Hello world', 'Cloudflare Workers'],\r\n});\r\n\r\nconsole.log(embeddings.shape); // [2, 768]\r\nconsole.log(embeddings.data[0]); // [0.123, -0.456, ...]\r\n```\r\n\r\n---\r\n\r\n### Image Generation Models\r\n\r\n**Input Format:**\r\n```typescript\r\n{\r\n  prompt: string; // Text description\r\n  num_steps?: number; // Default: 20\r\n  guidance?: number; // CFG scale, default: 7.5\r\n  strength?: number; // For img2img, default: 1.0\r\n  image?: number[][]; // For img2img (base64 or array)\r\n}\r\n```\r\n\r\n**Output Format:**\r\n- Binary image data (PNG/JPEG)\r\n\r\n**Example:**\r\n```typescript\r\nconst imageStream = await env.AI.run('@cf/black-forest-labs/flux-1-schnell', {\r\n  prompt: 'A beautiful sunset over mountains',\r\n});\r\n\r\nreturn new Response(imageStream, {\r\n  headers: { 'content-type': 'image/png' },\r\n});\r\n```\r\n\r\n---\r\n\r\n### Vision Models\r\n\r\n**Input Format:**\r\n```typescript\r\n{\r\n  messages: Array<{\r\n    role: 'user' | 'assistant';\r\n    content: Array<{ type: 'text' | 'image_url'; text?: string; image_url?: { url: string } }>;\r\n  }>;\r\n}\r\n```\r\n\r\n**Example:**\r\n```typescript\r\nconst response = await env.AI.run('@cf/meta/llama-3.2-11b-vision-instruct', {\r\n  messages: [\r\n    {\r\n      role: 'user',\r\n      content: [\r\n        { type: 'text', text: 'What is in this image?' },\r\n        { type: 'image_url', image_url: { url: 'data:image/png;base64,iVBOR...' } },\r\n      ],\r\n    },\r\n  ],\r\n});\r\n```\r\n\r\n---",
    "AI Gateway Integration": "AI Gateway provides caching, logging, and analytics for AI requests.\r\n\r\n**Setup:**\r\n```typescript\r\nconst response = await env.AI.run(\r\n  '@cf/meta/llama-3.1-8b-instruct',\r\n  { prompt: 'Hello' },\r\n  {\r\n    gateway: {\r\n      id: 'my-gateway', // Your gateway ID\r\n      skipCache: false, // Use cache\r\n    },\r\n  }\r\n);\r\n```\r\n\r\n**Benefits:**\r\n- ✅ **Cost Tracking** - Monitor neurons usage per request\r\n- ✅ **Caching** - Reduce duplicate inference costs\r\n- ✅ **Logging** - Debug and analyze AI requests\r\n- ✅ **Rate Limiting** - Additional layer of protection\r\n- ✅ **Analytics** - Request patterns and performance\r\n\r\n**Access Gateway Logs:**\r\n```typescript\r\nconst gateway = env.AI.gateway('my-gateway');\r\nconst logId = env.AI.aiGatewayLogId;\r\n\r\n// Send feedback\r\nawait gateway.patchLog(logId, {\r\n  feedback: { rating: 1, comment: 'Great response' },\r\n});\r\n```\r\n\r\n---",
    "Quick Start (5 minutes)": "### 1. Add AI Binding\r\n\r\n**wrangler.jsonc:**\r\n```jsonc\r\n{\r\n  \"ai\": {\r\n    \"binding\": \"AI\"\r\n  }\r\n}\r\n```\r\n\r\n### 2. Run Your First Model\r\n\r\n```typescript\r\nexport interface Env {\r\n  AI: Ai;\r\n}\r\n\r\nexport default {\r\n  async fetch(request: Request, env: Env): Promise<Response> {\r\n    const response = await env.AI.run('@cf/meta/llama-3.1-8b-instruct', {\r\n      prompt: 'What is Cloudflare?',\r\n    });\r\n\r\n    return Response.json(response);\r\n  },\r\n};\r\n```\r\n\r\n### 3. Add Streaming (Recommended)\r\n\r\n```typescript\r\nconst stream = await env.AI.run('@cf/meta/llama-3.1-8b-instruct', {\r\n  messages: [{ role: 'user', content: 'Tell me a story' }],\r\n  stream: true, // Always use streaming for text generation!\r\n});\r\n\r\nreturn new Response(stream, {\r\n  headers: { 'content-type': 'text/event-stream' },\r\n});\r\n```\r\n\r\n**Why streaming?**\r\n- Prevents buffering large responses in memory\r\n- Faster time-to-first-token\r\n- Better user experience for long-form content\r\n- Avoids Worker timeout issues\r\n\r\n---",
    "Table of Contents": "1. [Quick Start (5 minutes)](#quick-start-5-minutes)\r\n2. [Workers AI API Reference](#workers-ai-api-reference)\r\n3. [Model Selection Guide](#model-selection-guide)\r\n4. [Common Patterns](#common-patterns)\r\n5. [AI Gateway Integration](#ai-gateway-integration)\r\n6. [Rate Limits & Pricing](#rate-limits--pricing)\r\n7. [Production Checklist](#production-checklist)\r\n\r\n---",
    "Limits Summary": "| Feature | Limit |\r\n|---------|-------|\r\n| Concurrent requests | No hard limit (rate limits apply) |\r\n| Max input tokens | Varies by model (typically 2K-128K) |\r\n| Max output tokens | Varies by model (typically 512-2048) |\r\n| Streaming chunk size | ~1 KB |\r\n| Image size (output) | ~5 MB |\r\n| Request timeout | Workers timeout applies (30s default, 5m max CPU) |\r\n| Daily free neurons | 10,000 |\r\n| Rate limits | See \"Rate Limits & Pricing\" section |\r\n\r\n---"
  }
}