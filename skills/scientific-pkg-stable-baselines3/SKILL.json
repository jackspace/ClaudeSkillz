{
  "description": "Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DDPG, A2C, etc.), creating custom Gym environments, implementing callbacks for monitoring and control, using vectorized environments for parallel training, and integrating with deep RL workflows. This skill should be used when users request RL algorithm implementation, agent training, environment design, or RL experimentation.",
  "references": {
    "files": [
      "references/algorithms.md",
      "references/callbacks.md",
      "references/custom_environments.md",
      "references/vectorized_envs.md"
    ]
  },
  "content": "### 1. Training RL Agents\r\n\r\n**Basic Training Pattern:**\r\n\r\n```python\r\nimport gymnasium as gym\r\nfrom stable_baselines3 import PPO\r\n\r\nenv = gym.make(\"CartPole-v1\")\r\n\r\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\r\n\r\nmodel.learn(total_timesteps=10000)\r\n\r\nmodel.save(\"ppo_cartpole\")\r\n\r\nmodel = PPO.load(\"ppo_cartpole\", env=env)\r\n```\r\n\r\n**Important Notes:**\r\n- `total_timesteps` is a lower bound; actual training may exceed this due to batch collection\r\n- Use `model.load()` as a static method, not on an existing instance\r\n- The replay buffer is NOT saved with the model to save space\r\n\r\n**Algorithm Selection:**\r\nUse `references/algorithms.md` for detailed algorithm characteristics and selection guidance. Quick reference:\r\n- **PPO/A2C**: General-purpose, supports all action space types, good for multiprocessing\r\n- **SAC/TD3**: Continuous control, off-policy, sample-efficient\r\n- **DQN**: Discrete actions, off-policy\r\n- **HER**: Goal-conditioned tasks\r\n\r\nSee `scripts/train_rl_agent.py` for a complete training template with best practices.\r\n\r\n### 2. Custom Environments\r\n\r\n**Requirements:**\r\nCustom environments must inherit from `gymnasium.Env` and implement:\r\n- `__init__()`: Define action_space and observation_space\r\n- `reset(seed, options)`: Return initial observation and info dict\r\n- `step(action)`: Return observation, reward, terminated, truncated, info\r\n- `render()`: Visualization (optional)\r\n- `close()`: Cleanup resources\r\n\r\n**Key Constraints:**\r\n- Image observations must be `np.uint8` in range [0, 255]\r\n- Use channel-first format when possible (channels, height, width)\r\n- SB3 normalizes images automatically by dividing by 255\r\n- Set `normalize_images=False` in policy_kwargs if pre-normalized\r\n- SB3 does NOT support `Discrete` or `MultiDiscrete` spaces with `start!=0`\r\n\r\n**Validation:**\r\n```python\r\nfrom stable_baselines3.common.env_checker import check_env\r\n\r\ncheck_env(env, warn=True)\r\n```\r\n\r\nSee `scripts/custom_env_template.py` for a complete custom environment template and `references/custom_environments.md` for comprehensive guidance.\r\n\r\n### 3. Vectorized Environments\r\n\r\n**Purpose:**\r\nVectorized environments run multiple environment instances in parallel, accelerating training and enabling certain wrappers (frame-stacking, normalization).\r\n\r\n**Types:**\r\n- **DummyVecEnv**: Sequential execution on current process (for lightweight environments)\r\n- **SubprocVecEnv**: Parallel execution across processes (for compute-heavy environments)\r\n\r\n**Quick Setup:**\r\n```python\r\nfrom stable_baselines3.common.env_util import make_vec_env\r\n\r\nenv = make_vec_env(\"CartPole-v1\", n_envs=4, vec_env_cls=SubprocVecEnv)\r\n\r\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\r\nmodel.learn(total_timesteps=25000)\r\n```\r\n\r\n**Off-Policy Optimization:**\r\nWhen using multiple environments with off-policy algorithms (SAC, TD3, DQN), set `gradient_steps=-1` to perform one gradient update per environment step, balancing wall-clock time and sample efficiency.\r\n\r\n**API Differences:**\r\n- `reset()` returns only observations (info available in `vec_env.reset_infos`)\r\n- `step()` returns 4-tuple: `(obs, rewards, dones, infos)` not 5-tuple\r\n- Environments auto-reset after episodes\r\n- Terminal observations available via `infos[env_idx][\"terminal_observation\"]`\r\n\r\nSee `references/vectorized_envs.md` for detailed information on wrappers and advanced usage.\r\n\r\n### 4. Callbacks for Monitoring and Control\r\n\r\n**Purpose:**\r\nCallbacks enable monitoring metrics, saving checkpoints, implementing early stopping, and custom training logic without modifying core algorithms.\r\n\r\n**Common Callbacks:**\r\n- **EvalCallback**: Evaluate periodically and save best model\r\n- **CheckpointCallback**: Save model checkpoints at intervals\r\n- **StopTrainingOnRewardThreshold**: Stop when target reward reached\r\n- **ProgressBarCallback**: Display training progress with timing\r\n\r\n**Custom Callback Structure:**\r\n```python\r\nfrom stable_baselines3.common.callbacks import BaseCallback\r\n\r\nclass CustomCallback(BaseCallback):\r\n    def _on_training_start(self):\r\n        # Called before first rollout\r\n        pass\r\n\r\n    def _on_step(self):\r\n        # Called after each environment step\r\n        # Return False to stop training\r\n        return True\r\n\r\n    def _on_rollout_end(self):\r\n        # Called at end of rollout\r\n        pass\r\n```\r\n\r\n**Available Attributes:**\r\n- `self.model`: The RL algorithm instance\r\n- `self.num_timesteps`: Total environment steps\r\n- `self.training_env`: The training environment\r\n\r\n**Chaining Callbacks:**\r\n```python\r\nfrom stable_baselines3.common.callbacks import CallbackList\r\n\r\ncallback = CallbackList([eval_callback, checkpoint_callback, custom_callback])\r\nmodel.learn(total_timesteps=10000, callback=callback)\r\n```\r\n\r\nSee `references/callbacks.md` for comprehensive callback documentation.\r\n\r\n### 5. Model Persistence and Inspection\r\n\r\n**Saving and Loading:**\r\n```python\r\nmodel.save(\"model_name\")\r\n\r\nvec_env.save(\"vec_normalize.pkl\")\r\n\r\nmodel = PPO.load(\"model_name\", env=env)\r\n\r\nvec_env = VecNormalize.load(\"vec_normalize.pkl\", vec_env)\r\n```\r\n\r\n**Parameter Access:**\r\n```python\r\nparams = model.get_parameters()\r\n\r\nmodel.set_parameters(params)\r\n\r\nstate_dict = model.policy.state_dict()\r\n```\r\n\r\n### 6. Evaluation and Recording\r\n\r\n**Evaluation:**\r\n```python\r\nfrom stable_baselines3.common.evaluation import evaluate_policy\r\n\r\nmean_reward, std_reward = evaluate_policy(\r\n    model,\r\n    env,\r\n    n_eval_episodes=10,\r\n    deterministic=True\r\n)\r\n```\r\n\r\n**Video Recording:**\r\n```python\r\nfrom stable_baselines3.common.vec_env import VecVideoRecorder\r\n\r\n\r\n```bash\r\npip install stable-baselines3",
  "name": "stable-baselines3",
  "id": "scientific-pkg-stable-baselines3",
  "sections": {
    "Core Capabilities": "env = VecVideoRecorder(\r\n    env,\r\n    \"videos/\",\r\n    record_video_trigger=lambda x: x % 2000 == 0,\r\n    video_length=200\r\n)\r\n```\r\n\r\nSee `scripts/evaluate_agent.py` for a complete evaluation and recording template.\r\n\r\n### 7. Advanced Features\r\n\r\n**Learning Rate Schedules:**\r\n```python\r\ndef linear_schedule(initial_value):\r\n    def func(progress_remaining):\r\n        # progress_remaining goes from 1 to 0\r\n        return progress_remaining * initial_value\r\n    return func\r\n\r\nmodel = PPO(\"MlpPolicy\", env, learning_rate=linear_schedule(0.001))\r\n```\r\n\r\n**Multi-Input Policies (Dict Observations):**\r\n```python\r\nmodel = PPO(\"MultiInputPolicy\", env, verbose=1)\r\n```\r\nUse when observations are dictionaries (e.g., combining images with sensor data).\r\n\r\n**Hindsight Experience Replay:**\r\n```python\r\nfrom stable_baselines3 import SAC, HerReplayBuffer\r\n\r\nmodel = SAC(\r\n    \"MultiInputPolicy\",\r\n    env,\r\n    replay_buffer_class=HerReplayBuffer,\r\n    replay_buffer_kwargs=dict(\r\n        n_sampled_goal=4,\r\n        goal_selection_strategy=\"future\",\r\n    ),\r\n)\r\n```\r\n\r\n**TensorBoard Integration:**\r\n```python\r\nmodel = PPO(\"MlpPolicy\", env, tensorboard_log=\"./tensorboard/\")\r\nmodel.learn(total_timesteps=10000)\r\n```",
    "Workflow Guidance": "**Starting a New RL Project:**\r\n\r\n1. **Define the problem**: Identify observation space, action space, and reward structure\r\n2. **Choose algorithm**: Use `references/algorithms.md` for selection guidance\r\n3. **Create/adapt environment**: Use `scripts/custom_env_template.py` if needed\r\n4. **Validate environment**: Always run `check_env()` before training\r\n5. **Set up training**: Use `scripts/train_rl_agent.py` as starting template\r\n6. **Add monitoring**: Implement callbacks for evaluation and checkpointing\r\n7. **Optimize performance**: Consider vectorized environments for speed\r\n8. **Evaluate and iterate**: Use `scripts/evaluate_agent.py` for assessment\r\n\r\n**Common Issues:**\r\n\r\n- **Memory errors**: Reduce `buffer_size` for off-policy algorithms or use fewer parallel environments\r\n- **Slow training**: Consider SubprocVecEnv for parallel environments\r\n- **Unstable training**: Try different algorithms, tune hyperparameters, or check reward scaling\r\n- **Import errors**: Ensure `stable_baselines3` is installed: `pip install stable-baselines3[extra]`",
    "Overview": "Stable Baselines3 (SB3) is a PyTorch-based library providing reliable implementations of reinforcement learning algorithms. This skill provides comprehensive guidance for training RL agents, creating custom environments, implementing callbacks, and optimizing training workflows using SB3's unified API.",
    "Resources": "### scripts/\r\n- `train_rl_agent.py`: Complete training script template with best practices\r\n- `evaluate_agent.py`: Agent evaluation and video recording template\r\n- `custom_env_template.py`: Custom Gym environment template\r\n\r\n### references/\r\n- `algorithms.md`: Detailed algorithm comparison and selection guide\r\n- `custom_environments.md`: Comprehensive custom environment creation guide\r\n- `callbacks.md`: Complete callback system reference\r\n- `vectorized_envs.md`: Vectorized environment usage and wrappers",
    "Installation": "pip install stable-baselines3[extra]\r\n```"
  }
}