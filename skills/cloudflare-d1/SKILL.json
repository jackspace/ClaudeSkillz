{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/best-practices.md",
      "references/query-patterns.md"
    ]
  },
  "content": "**Status**: Production Ready ‚úÖ\r\n**Last Updated**: 2025-10-21\r\n**Dependencies**: cloudflare-worker-base (for Worker setup)\r\n**Latest Versions**: wrangler@4.43.0, @cloudflare/workers-types@4.20251014.0\r\n\r\n---\r\n\r\n\r\n### 1. Create D1 Database\r\n\r\n```bash\r\nnpx wrangler d1 create my-database\r\n\r\n#\r\n```\r\n\r\n### 2. Configure Bindings\r\n\r\nAdd to your `wrangler.jsonc`:\r\n\r\n```jsonc\r\n{\r\n  \"name\": \"my-worker\",\r\n  \"main\": \"src/index.ts\",\r\n  \"compatibility_date\": \"2025-10-11\",\r\n  \"d1_databases\": [\r\n    {\r\n      \"binding\": \"DB\",                    // Available as env.DB in your Worker\r\n      \"database_name\": \"my-database\",      // Name from wrangler d1 create\r\n      \"database_id\": \"<UUID>\",             // ID from wrangler d1 create\r\n      \"preview_database_id\": \"local-db\"    // For local development\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n**CRITICAL:**\r\n- `binding` is how you access the database in code (`env.DB`)\r\n- `database_id` is the production database UUID\r\n- `preview_database_id` is for local dev (can be any string)\r\n- **Never commit real `database_id` values to public repos** - use environment variables or secrets\r\n\r\n### 3. Create Your First Migration\r\n\r\n```bash\r\nnpx wrangler d1 migrations create my-database create_users_table\r\n\r\n```\r\n\r\nEdit the migration file:\r\n\r\n```sql\r\n-- migrations/0001_create_users_table.sql\r\nDROP TABLE IF EXISTS users;\r\nCREATE TABLE IF NOT EXISTS users (\r\n  user_id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n  email TEXT NOT NULL UNIQUE,\r\n  username TEXT NOT NULL,\r\n  created_at INTEGER NOT NULL,\r\n  updated_at INTEGER\r\n);\r\n\r\n-- Create index for common queries\r\nCREATE INDEX IF NOT EXISTS idx_users_email ON users(email);\r\n\r\n-- Optimize database\r\nPRAGMA optimize;\r\n```\r\n\r\n### 4. Apply Migration\r\n\r\n```bash\r\nnpx wrangler d1 migrations apply my-database --local\r\n\r\n\r\n### Migration Workflow\r\n\r\n```bash\r\nnpx wrangler d1 migrations create <DATABASE_NAME> <MIGRATION_NAME>\r\n\r\nnpx wrangler d1 migrations list <DATABASE_NAME> --local\r\nnpx wrangler d1 migrations list <DATABASE_NAME> --remote\r\n\r\n\r\n### Local vs Remote Databases\r\n\r\n```bash\r\nnpx wrangler d1 migrations apply my-database --local\r\n\r\nnpx wrangler d1 execute my-database --local --command \"SELECT * FROM users\"\r\n\r\nnpx wrangler d1 execute my-database --remote --command \"SELECT * FROM users\"\r\n```\r\n\r\n### Local Database Location\r\n\r\nLocal D1 databases are stored in:\r\n```\r\n.wrangler/state/v3/d1/miniflare-D1DatabaseObject/<database_id>.sqlite\r\n```\r\n\r\n### Seeding Local Database\r\n\r\n```bash\r\ncat > seed.sql << 'EOF'\r\nINSERT INTO users (email, username, created_at) VALUES\r\n  ('alice@example.com', 'alice', 1698000000),\r\n  ('bob@example.com', 'bob', 1698000060);\r\nEOF\r\n\r\n\r\n```bash\r\nwrangler d1 create <DATABASE_NAME>\r\nwrangler d1 list\r\nwrangler d1 delete <DATABASE_NAME>\r\nwrangler d1 info <DATABASE_NAME>\r\n\r\nwrangler d1 migrations create <DATABASE_NAME> <MIGRATION_NAME>\r\nwrangler d1 migrations list <DATABASE_NAME> --local|--remote\r\nwrangler d1 migrations apply <DATABASE_NAME> --local|--remote\r\n\r\nwrangler d1 execute <DATABASE_NAME> --local|--remote --command \"SELECT * FROM users\"\r\nwrangler d1 execute <DATABASE_NAME> --local|--remote --file=./query.sql",
  "name": "cloudflare-d1",
  "id": "cloudflare-d1",
  "sections": {
    "Official Documentation": "- **D1 Overview**: https://developers.cloudflare.com/d1/\r\n- **Get Started**: https://developers.cloudflare.com/d1/get-started/\r\n- **Migrations**: https://developers.cloudflare.com/d1/reference/migrations/\r\n- **Workers API**: https://developers.cloudflare.com/d1/worker-api/\r\n- **Best Practices**: https://developers.cloudflare.com/d1/best-practices/\r\n- **Wrangler Commands**: https://developers.cloudflare.com/workers/wrangler/commands/#d1\r\n\r\n---\r\n\r\n**Ready to build with D1!** üöÄ",
    "Query Patterns": "### Basic CRUD Operations\r\n\r\n#### Create (INSERT)\r\n\r\n```typescript\r\n// Single insert\r\nconst { meta } = await env.DB.prepare(\r\n  'INSERT INTO users (email, username, created_at) VALUES (?, ?, ?)'\r\n)\r\n.bind(email, username, Date.now())\r\n.run();\r\n\r\nconst newUserId = meta.last_row_id;\r\n\r\n// Bulk insert with batch()\r\nconst users = [\r\n  { email: 'user1@example.com', username: 'user1' },\r\n  { email: 'user2@example.com', username: 'user2' }\r\n];\r\n\r\nconst inserts = users.map(u =>\r\n  env.DB.prepare('INSERT INTO users (email, username, created_at) VALUES (?, ?, ?)')\r\n    .bind(u.email, u.username, Date.now())\r\n);\r\n\r\nawait env.DB.batch(inserts);\r\n```\r\n\r\n#### Read (SELECT)\r\n\r\n```typescript\r\n// Single row\r\nconst user = await env.DB.prepare('SELECT * FROM users WHERE user_id = ?')\r\n  .bind(userId)\r\n  .first();\r\n\r\n// Multiple rows\r\nconst { results } = await env.DB.prepare(\r\n  'SELECT * FROM users WHERE created_at > ? ORDER BY created_at DESC LIMIT ?'\r\n)\r\n.bind(timestamp, 10)\r\n.all();\r\n\r\n// Count\r\nconst count = await env.DB.prepare('SELECT COUNT(*) as total FROM users')\r\n  .first('total');\r\n\r\n// Exists check\r\nconst exists = await env.DB.prepare('SELECT 1 FROM users WHERE email = ? LIMIT 1')\r\n  .bind(email)\r\n  .first();\r\n\r\nif (exists) {\r\n  // Email already registered\r\n}\r\n```\r\n\r\n#### Update (UPDATE)\r\n\r\n```typescript\r\nconst { meta } = await env.DB.prepare(\r\n  'UPDATE users SET username = ?, updated_at = ? WHERE user_id = ?'\r\n)\r\n.bind(newUsername, Date.now(), userId)\r\n.run();\r\n\r\nconst rowsAffected = meta.rows_written;\r\n\r\nif (rowsAffected === 0) {\r\n  // User not found\r\n}\r\n```\r\n\r\n#### Delete (DELETE)\r\n\r\n```typescript\r\nconst { meta } = await env.DB.prepare('DELETE FROM users WHERE user_id = ?')\r\n  .bind(userId)\r\n  .run();\r\n\r\nconst rowsDeleted = meta.rows_written;\r\n```\r\n\r\n### Advanced Queries\r\n\r\n#### Pagination\r\n\r\n```typescript\r\napp.get('/api/users', async (c) => {\r\n  const page = parseInt(c.req.query('page') || '1');\r\n  const limit = parseInt(c.req.query('limit') || '20');\r\n  const offset = (page - 1) * limit;\r\n\r\n  const [countResult, usersResult] = await c.env.DB.batch([\r\n    c.env.DB.prepare('SELECT COUNT(*) as total FROM users'),\r\n    c.env.DB.prepare('SELECT * FROM users ORDER BY created_at DESC LIMIT ? OFFSET ?')\r\n      .bind(limit, offset)\r\n  ]);\r\n\r\n  const total = countResult.results[0].total as number;\r\n  const users = usersResult.results;\r\n\r\n  return c.json({\r\n    users,\r\n    pagination: {\r\n      page,\r\n      limit,\r\n      total,\r\n      pages: Math.ceil(total / limit)\r\n    }\r\n  });\r\n});\r\n```\r\n\r\n#### Joins\r\n\r\n```typescript\r\nconst { results } = await env.DB.prepare(`\r\n  SELECT\r\n    posts.*,\r\n    users.username as author_name,\r\n    users.email as author_email\r\n  FROM posts\r\n  INNER JOIN users ON posts.user_id = users.user_id\r\n  WHERE posts.published = ?\r\n  ORDER BY posts.created_at DESC\r\n  LIMIT ?\r\n`)\r\n.bind(1, 10)\r\n.all();\r\n```\r\n\r\n#### Transactions (Batch Pattern)\r\n\r\nD1 doesn't support multi-statement transactions, but batch() provides sequential execution:\r\n\r\n```typescript\r\n// Transfer credits between users (pseudo-transaction)\r\nawait env.DB.batch([\r\n  env.DB.prepare('UPDATE users SET credits = credits - ? WHERE user_id = ?')\r\n    .bind(amount, fromUserId),\r\n  env.DB.prepare('UPDATE users SET credits = credits + ? WHERE user_id = ?')\r\n    .bind(amount, toUserId),\r\n  env.DB.prepare('INSERT INTO transactions (from_user, to_user, amount) VALUES (?, ?, ?)')\r\n    .bind(fromUserId, toUserId, amount)\r\n]);\r\n```\r\n\r\n**Note**: If any statement fails, the batch stops. This provides some transaction-like behavior.\r\n\r\n---",
    "D1 Workers API": "### Type Definitions\r\n\r\n```typescript\r\n// Add to env.d.ts or worker-configuration.d.ts\r\ninterface Env {\r\n  DB: D1Database;\r\n  // ... other bindings\r\n}\r\n\r\n// For Hono\r\ntype Bindings = {\r\n  DB: D1Database;\r\n};\r\n\r\nconst app = new Hono<{ Bindings: Bindings }>();\r\n```\r\n\r\n### prepare() - Prepared Statements (PRIMARY METHOD)\r\n\r\n**Always use prepared statements for queries with user input.**\r\n\r\n```typescript\r\n// Basic prepared statement\r\nconst stmt = env.DB.prepare('SELECT * FROM users WHERE user_id = ?');\r\nconst bound = stmt.bind(userId);\r\nconst result = await bound.first();\r\n\r\n// Chained (most common pattern)\r\nconst user = await env.DB.prepare('SELECT * FROM users WHERE email = ?')\r\n  .bind(email)\r\n  .first();\r\n```\r\n\r\n**Why use prepare():**\r\n- ‚úÖ Prevents SQL injection\r\n- ‚úÖ Can be reused with different parameters\r\n- ‚úÖ Better performance (query plan caching)\r\n- ‚úÖ Type-safe with TypeScript\r\n\r\n### Query Result Methods\r\n\r\n#### .all() - Get All Rows\r\n\r\n```typescript\r\nconst { results, meta } = await env.DB.prepare(\r\n  'SELECT * FROM users WHERE created_at > ?'\r\n)\r\n.bind(timestamp)\r\n.all();\r\n\r\nconsole.log(results);  // Array of rows\r\nconsole.log(meta);     // { duration, rows_read, rows_written }\r\n```\r\n\r\n#### .first() - Get First Row\r\n\r\n```typescript\r\n// Returns first row or null\r\nconst user = await env.DB.prepare('SELECT * FROM users WHERE email = ?')\r\n  .bind('user@example.com')\r\n  .first();\r\n\r\nif (!user) {\r\n  return c.json({ error: 'Not found' }, 404);\r\n}\r\n```\r\n\r\n#### .first(column) - Get Single Column Value\r\n\r\n```typescript\r\n// Returns the value of a specific column from first row\r\nconst count = await env.DB.prepare('SELECT COUNT(*) as total FROM users')\r\n  .first('total');\r\n\r\nconsole.log(count);  // 42 (just the number, not an object)\r\n```\r\n\r\n#### .run() - Execute Without Results\r\n\r\n```typescript\r\n// For INSERT, UPDATE, DELETE\r\nconst { success, meta } = await env.DB.prepare(\r\n  'INSERT INTO users (email, username, created_at) VALUES (?, ?, ?)'\r\n)\r\n.bind(email, username, Date.now())\r\n.run();\r\n\r\nconsole.log(meta);  // { duration, rows_read, rows_written, last_row_id }\r\n```\r\n\r\n### batch() - Execute Multiple Queries\r\n\r\n**CRITICAL FOR PERFORMANCE**: Use batch() to reduce latency.\r\n\r\n```typescript\r\n// Prepare multiple statements\r\nconst stmt1 = env.DB.prepare('SELECT * FROM users WHERE user_id = ?').bind(1);\r\nconst stmt2 = env.DB.prepare('SELECT * FROM users WHERE user_id = ?').bind(2);\r\nconst stmt3 = env.DB.prepare('SELECT * FROM posts WHERE user_id = ?').bind(1);\r\n\r\n// Execute all in one round trip\r\nconst results = await env.DB.batch([stmt1, stmt2, stmt3]);\r\n\r\nconsole.log(results[0].results);  // Users query 1\r\nconsole.log(results[1].results);  // Users query 2\r\nconsole.log(results[2].results);  // Posts query\r\n```\r\n\r\n**Batch Behavior:**\r\n- Executes sequentially (in order)\r\n- Each statement commits individually (auto-commit mode)\r\n- If one fails, remaining statements don't execute\r\n- Much faster than individual queries (single network round trip)\r\n\r\n**Batch Use Cases:**\r\n```typescript\r\n// ‚úÖ Insert multiple rows efficiently\r\nconst inserts = users.map(user =>\r\n  env.DB.prepare('INSERT INTO users (email, username) VALUES (?, ?)')\r\n    .bind(user.email, user.username)\r\n);\r\nawait env.DB.batch(inserts);\r\n\r\n// ‚úÖ Fetch related data in parallel\r\nconst [user, posts, comments] = await env.DB.batch([\r\n  env.DB.prepare('SELECT * FROM users WHERE user_id = ?').bind(userId),\r\n  env.DB.prepare('SELECT * FROM posts WHERE user_id = ?').bind(userId),\r\n  env.DB.prepare('SELECT * FROM comments WHERE user_id = ?').bind(userId)\r\n]);\r\n```\r\n\r\n### exec() - Execute Raw SQL (AVOID IN PRODUCTION)\r\n\r\n```typescript\r\n// Only for migrations, maintenance, and one-off tasks\r\nconst result = await env.DB.exec(`\r\n  SELECT * FROM users;\r\n  SELECT * FROM posts;\r\n`);\r\n\r\nconsole.log(result);  // { count: 2, duration: 5 }\r\n```\r\n\r\n**NEVER use exec() for:**\r\n- ‚ùå Queries with user input (SQL injection risk)\r\n- ‚ùå Production queries (poor performance)\r\n- ‚ùå Queries that need results (exec doesn't return data)\r\n\r\n**ONLY use exec() for:**\r\n- ‚úÖ Running migration SQL files locally\r\n- ‚úÖ One-off maintenance tasks\r\n- ‚úÖ Database initialization scripts\r\n\r\n---",
    "Local Development": "npx wrangler d1 execute my-database --local --file=seed.sql\r\n```\r\n\r\n---",
    "Wrangler Commands Reference": "wrangler d1 time-travel info <DATABASE_NAME> --timestamp \"2025-10-20\"\r\nwrangler d1 time-travel restore <DATABASE_NAME> --timestamp \"2025-10-20\"\r\n```\r\n\r\n---",
    "Performance Optimization": "### Indexes\r\n\r\nIndexes dramatically improve query performance for filtered columns.\r\n\r\n#### When to Create Indexes\r\n\r\n```typescript\r\n// ‚úÖ Index columns used in WHERE clauses\r\nCREATE INDEX idx_users_email ON users(email);\r\n\r\n// ‚úÖ Index foreign keys\r\nCREATE INDEX idx_posts_user_id ON posts(user_id);\r\n\r\n// ‚úÖ Index columns used for sorting\r\nCREATE INDEX idx_posts_created_at ON posts(created_at DESC);\r\n\r\n// ‚úÖ Multi-column indexes for complex queries\r\nCREATE INDEX idx_posts_user_published ON posts(user_id, published);\r\n```\r\n\r\n#### Test Index Usage\r\n\r\n```sql\r\n-- Check if index is being used\r\nEXPLAIN QUERY PLAN SELECT * FROM users WHERE email = 'user@example.com';\r\n\r\n-- Should see: SEARCH users USING INDEX idx_users_email\r\n```\r\n\r\n#### Partial Indexes\r\n\r\n```sql\r\n-- Index only non-deleted records\r\nCREATE INDEX idx_users_active ON users(email) WHERE deleted = 0;\r\n\r\n-- Index only published posts\r\nCREATE INDEX idx_posts_published ON posts(created_at DESC) WHERE published = 1;\r\n```\r\n\r\n### PRAGMA optimize\r\n\r\nRun after creating indexes or making schema changes:\r\n\r\n```sql\r\n-- In your migration file\r\nCREATE INDEX idx_users_email ON users(email);\r\nPRAGMA optimize;\r\n```\r\n\r\nOr from Worker:\r\n\r\n```typescript\r\nawait env.DB.exec('PRAGMA optimize');\r\n```\r\n\r\n### Query Optimization Tips\r\n\r\n```typescript\r\n// ‚úÖ Use specific columns instead of SELECT *\r\nconst users = await env.DB.prepare(\r\n  'SELECT user_id, email, username FROM users'\r\n).all();\r\n\r\n// ‚úÖ Use LIMIT to prevent scanning entire table\r\nconst latest = await env.DB.prepare(\r\n  'SELECT * FROM posts ORDER BY created_at DESC LIMIT 10'\r\n).all();\r\n\r\n// ‚úÖ Use indexes for WHERE conditions\r\n// Create index first: CREATE INDEX idx_users_email ON users(email)\r\nconst user = await env.DB.prepare('SELECT * FROM users WHERE email = ?')\r\n  .bind(email)\r\n  .first();\r\n\r\n// ‚ùå Avoid functions in WHERE (can't use indexes)\r\n// Bad: WHERE LOWER(email) = 'user@example.com'\r\n// Good: WHERE email = 'user@example.com' (store email lowercase)\r\n```\r\n\r\n---",
    "Known Issues Prevented": "| Issue | Description | How to Avoid |\r\n|-------|-------------|--------------|\r\n| **Statement too long** | Large INSERT statements exceed D1 limits | Break into batches of 100-250 rows |\r\n| **Transaction conflicts** | `BEGIN TRANSACTION` in migration files | Remove BEGIN/COMMIT (D1 handles this) |\r\n| **Foreign key violations** | Schema changes break foreign key constraints | Use `PRAGMA defer_foreign_keys = true` |\r\n| **Rate limiting / queue overload** | Too many individual queries | Use `batch()` instead of loops |\r\n| **Memory limit exceeded** | Query loads too much data into memory | Add LIMIT, paginate results, shard queries |\r\n| **Type mismatch errors** | Using `undefined` instead of `null` | Always use `null` for optional values |\r\n\r\n---",
    "D1 Migrations System": "npx wrangler d1 migrations apply <DATABASE_NAME> --local   # Test locally\r\nnpx wrangler d1 migrations apply <DATABASE_NAME> --remote  # Deploy to production\r\n```\r\n\r\n### Migration File Naming\r\n\r\nMigrations are automatically versioned:\r\n\r\n```\r\nmigrations/\r\n‚îú‚îÄ‚îÄ 0000_initial_schema.sql\r\n‚îú‚îÄ‚îÄ 0001_add_users_table.sql\r\n‚îú‚îÄ‚îÄ 0002_add_posts_table.sql\r\n‚îî‚îÄ‚îÄ 0003_add_indexes.sql\r\n```\r\n\r\n**Rules:**\r\n- Files are executed in sequential order\r\n- Each migration runs once (tracked in `d1_migrations` table)\r\n- Failed migrations roll back (transactional)\r\n- Can't modify or delete applied migrations\r\n\r\n### Custom Migration Configuration\r\n\r\n```jsonc\r\n{\r\n  \"d1_databases\": [\r\n    {\r\n      \"binding\": \"DB\",\r\n      \"database_name\": \"my-database\",\r\n      \"database_id\": \"<UUID>\",\r\n      \"migrations_dir\": \"db/migrations\",        // Custom directory (default: migrations/)\r\n      \"migrations_table\": \"schema_migrations\"   // Custom tracking table (default: d1_migrations)\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Migration Best Practices\r\n\r\n#### ‚úÖ Always Do:\r\n\r\n```sql\r\n-- Use IF NOT EXISTS to make migrations idempotent\r\nCREATE TABLE IF NOT EXISTS users (...);\r\nCREATE INDEX IF NOT EXISTS idx_users_email ON users(email);\r\n\r\n-- Run PRAGMA optimize after schema changes\r\nPRAGMA optimize;\r\n\r\n-- Use transactions for data migrations\r\nBEGIN TRANSACTION;\r\nUPDATE users SET updated_at = unixepoch() WHERE updated_at IS NULL;\r\nCOMMIT;\r\n```\r\n\r\n#### ‚ùå Never Do:\r\n\r\n```sql\r\n-- DON'T include BEGIN TRANSACTION at start (D1 handles this)\r\nBEGIN TRANSACTION;  -- ‚ùå Remove this\r\n\r\n-- DON'T use MySQL/PostgreSQL syntax\r\nALTER TABLE users MODIFY COLUMN email VARCHAR(255);  -- ‚ùå Not SQLite\r\n\r\n-- DON'T create tables without IF NOT EXISTS\r\nCREATE TABLE users (...);  -- ‚ùå Fails if table exists\r\n```\r\n\r\n### Handling Foreign Keys in Migrations\r\n\r\n```sql\r\n-- Temporarily disable foreign key checks during schema changes\r\nPRAGMA defer_foreign_keys = true;\r\n\r\n-- Make schema changes that would violate foreign keys\r\nALTER TABLE posts DROP COLUMN author_id;\r\nALTER TABLE posts ADD COLUMN user_id INTEGER REFERENCES users(user_id);\r\n\r\n-- Foreign keys re-enabled automatically at end of migration\r\n```\r\n\r\n---",
    "Quick Start (5 Minutes)": "npx wrangler d1 migrations apply my-database --remote\r\n```\r\n\r\n### 5. Query from Your Worker\r\n\r\n```typescript\r\n// src/index.ts\r\nimport { Hono } from 'hono';\r\n\r\ntype Bindings = {\r\n  DB: D1Database;\r\n};\r\n\r\nconst app = new Hono<{ Bindings: Bindings }>();\r\n\r\napp.get('/api/users/:email', async (c) => {\r\n  const email = c.req.param('email');\r\n\r\n  try {\r\n    // ALWAYS use prepared statements with bind()\r\n    const result = await c.env.DB.prepare(\r\n      'SELECT * FROM users WHERE email = ?'\r\n    )\r\n    .bind(email)\r\n    .first();\r\n\r\n    if (!result) {\r\n      return c.json({ error: 'User not found' }, 404);\r\n    }\r\n\r\n    return c.json(result);\r\n  } catch (error: any) {\r\n    console.error('D1 Error:', error.message);\r\n    return c.json({ error: 'Database error' }, 500);\r\n  }\r\n});\r\n\r\nexport default app;\r\n```\r\n\r\n---",
    "Error Handling": "### Error Types\r\n\r\n```typescript\r\ntry {\r\n  const result = await env.DB.prepare('SELECT * FROM users WHERE user_id = ?')\r\n    .bind(userId)\r\n    .first();\r\n} catch (error: any) {\r\n  // D1 errors have a message property\r\n  const errorMessage = error.message;\r\n\r\n  if (errorMessage.includes('D1_ERROR')) {\r\n    // D1-specific error\r\n  } else if (errorMessage.includes('D1_EXEC_ERROR')) {\r\n    // SQL syntax error\r\n  } else if (errorMessage.includes('D1_TYPE_ERROR')) {\r\n    // Type mismatch (e.g., undefined instead of null)\r\n  } else if (errorMessage.includes('D1_COLUMN_NOTFOUND')) {\r\n    // Column doesn't exist\r\n  }\r\n\r\n  console.error('Database error:', errorMessage);\r\n  return c.json({ error: 'Database operation failed' }, 500);\r\n}\r\n```\r\n\r\n### Common Errors and Fixes\r\n\r\n#### \"Statement too long\"\r\n\r\n```typescript\r\n// ‚ùå DON'T: Single massive INSERT\r\nawait env.DB.exec(`\r\n  INSERT INTO users (email) VALUES\r\n    ('user1@example.com'),\r\n    ('user2@example.com'),\r\n    ... // 1000 more rows\r\n`);\r\n\r\n// ‚úÖ DO: Break into batches\r\nconst batchSize = 100;\r\nfor (let i = 0; i < users.length; i += batchSize) {\r\n  const batch = users.slice(i, i + batchSize);\r\n  const inserts = batch.map(u =>\r\n    env.DB.prepare('INSERT INTO users (email) VALUES (?)').bind(u.email)\r\n  );\r\n  await env.DB.batch(inserts);\r\n}\r\n```\r\n\r\n#### \"Too many requests queued\"\r\n\r\n```typescript\r\n// ‚ùå DON'T: Fire off many individual queries\r\nfor (const user of users) {\r\n  await env.DB.prepare('INSERT INTO users (email) VALUES (?)').bind(user.email).run();\r\n}\r\n\r\n// ‚úÖ DO: Use batch()\r\nconst inserts = users.map(u =>\r\n  env.DB.prepare('INSERT INTO users (email) VALUES (?)').bind(u.email)\r\n);\r\nawait env.DB.batch(inserts);\r\n```\r\n\r\n#### \"D1_TYPE_ERROR\" (undefined vs null)\r\n\r\n```typescript\r\n// ‚ùå DON'T: Use undefined\r\nawait env.DB.prepare('INSERT INTO users (email, bio) VALUES (?, ?)')\r\n  .bind(email, undefined);  // ‚ùå D1 doesn't support undefined\r\n\r\n// ‚úÖ DO: Use null for optional values\r\nawait env.DB.prepare('INSERT INTO users (email, bio) VALUES (?, ?)')\r\n  .bind(email, bio || null);\r\n```\r\n\r\n### Retry Logic\r\n\r\n```typescript\r\nasync function queryWithRetry<T>(\r\n  queryFn: () => Promise<T>,\r\n  maxRetries = 3\r\n): Promise<T> {\r\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\r\n    try {\r\n      return await queryFn();\r\n    } catch (error: any) {\r\n      const message = error.message;\r\n\r\n      // Retry on transient errors\r\n      const isRetryable =\r\n        message.includes('Network connection lost') ||\r\n        message.includes('storage caused object to be reset') ||\r\n        message.includes('reset because its code was updated');\r\n\r\n      if (!isRetryable || attempt === maxRetries - 1) {\r\n        throw error;\r\n      }\r\n\r\n      // Exponential backoff\r\n      const delay = Math.min(1000 * Math.pow(2, attempt), 5000);\r\n      await new Promise(resolve => setTimeout(resolve, delay));\r\n    }\r\n  }\r\n\r\n  throw new Error('Retry logic failed');\r\n}\r\n\r\n// Usage\r\nconst user = await queryWithRetry(() =>\r\n  env.DB.prepare('SELECT * FROM users WHERE user_id = ?')\r\n    .bind(userId)\r\n    .first()\r\n);\r\n```\r\n\r\n---",
    "Drizzle ORM (Optional)": "While D1 works great with raw SQL, some developers prefer ORMs. Drizzle ORM supports D1:\r\n\r\n```bash\r\nnpm install drizzle-orm\r\nnpm install -D drizzle-kit\r\n```\r\n\r\n**Note**: Drizzle adds complexity and another layer to learn. For most D1 use cases, **raw SQL with wrangler is simpler and more direct**. Only consider Drizzle if you:\r\n- Prefer TypeScript schema definitions over SQL\r\n- Want auto-complete for queries\r\n- Are building a very large application with complex schemas\r\n\r\n**Official Drizzle D1 docs**: https://orm.drizzle.team/docs/get-started-sqlite#cloudflare-d1\r\n\r\n---",
    "Best Practices Summary": "### ‚úÖ Always Do:\r\n\r\n1. **Use prepared statements** with `.bind()` for user input\r\n2. **Use `.batch()`** for multiple queries (reduces latency)\r\n3. **Create indexes** on frequently queried columns\r\n4. **Run `PRAGMA optimize`** after schema changes\r\n5. **Use `IF NOT EXISTS`** in migrations for idempotency\r\n6. **Test migrations locally** before applying to production\r\n7. **Handle errors gracefully** with try/catch\r\n8. **Use `null`** instead of `undefined` for optional values\r\n9. **Validate input** before binding to queries\r\n10. **Check `meta.rows_written`** after UPDATE/DELETE\r\n\r\n### ‚ùå Never Do:\r\n\r\n1. **Never use `.exec()`** with user input (SQL injection risk)\r\n2. **Never hardcode `database_id`** in public repos\r\n3. **Never use `undefined`** in bind parameters (causes D1_TYPE_ERROR)\r\n4. **Never fire individual queries in loops** (use batch instead)\r\n5. **Never forget `LIMIT`** on potentially large result sets\r\n6. **Never use `SELECT *`** in production (specify columns)\r\n7. **Never include `BEGIN TRANSACTION`** in migration files\r\n8. **Never modify applied migrations** (create new ones)\r\n9. **Never skip error handling** on database operations\r\n10. **Never assume queries succeed** (always check results)\r\n\r\n---"
  }
}