{
  "sections": {
    "Example Evaluation Workflow": "**User Request:** \"Evaluate this research paper on machine learning for drug discovery\"\r\n\r\n**Response Process:**\r\n1. Identify work type (empirical research paper) and scope (comprehensive evaluation)\r\n2. Load `references/evaluation_framework.md` for detailed criteria\r\n3. Systematically assess each dimension:\r\n   - Problem formulation: Clear research question about ML model performance\r\n   - Literature review: Comprehensive coverage of recent ML and drug discovery work\r\n   - Methodology: Appropriate deep learning architecture with validation procedures\r\n   - [Continue through all dimensions...]\r\n4. Calculate dimension scores and overall assessment\r\n5. Synthesize findings into structured report highlighting:\r\n   - Strong methodology and reproducible code\r\n   - Needs more diverse dataset evaluation\r\n   - Writing could improve clarity in results section\r\n6. Provide prioritized recommendations with specific suggestions",
    "Best Practices": "1. **Maintain Objectivity** - Base evaluations on established criteria, not personal preferences\r\n2. **Be Comprehensive** - Evaluate all applicable dimensions systematically\r\n3. **Provide Evidence** - Support assessments with specific examples from the work\r\n4. **Stay Constructive** - Frame weaknesses as opportunities for improvement\r\n5. **Consider Context** - Adjust expectations based on work stage and purpose\r\n6. **Document Rationale** - Explain the reasoning behind assessments and scores\r\n7. **Encourage Strengths** - Explicitly acknowledge what the work does well\r\n8. **Prioritize Feedback** - Focus on high-impact improvements first",
    "Notes": "- Evaluation rigor should match the work's purpose and stage\r\n- Some dimensions may not apply to all work types (e.g., data collection for purely theoretical papers)\r\n- Cultural and disciplinary differences in scholarly norms should be considered\r\n- This framework complements, not replaces, domain-specific expertise",
    "Overview": "Apply the ScholarEval framework to systematically evaluate scholarly and research work. This skill provides structured evaluation methodology based on peer-reviewed research assessment criteria, enabling comprehensive analysis of academic papers, research proposals, literature reviews, and scholarly writing across multiple quality dimensions.",
    "When to Use This Skill": "Use this skill when:\r\n- Evaluating research papers for quality and rigor\r\n- Assessing literature review comprehensiveness and quality\r\n- Reviewing research methodology design\r\n- Scoring data analysis approaches\r\n- Evaluating scholarly writing and presentation\r\n- Providing structured feedback on academic work\r\n- Benchmarking research quality against established criteria",
    "Citation": "This skill is based on the ScholarEval framework introduced in:\r\n\r\n**Moussa, H. N., Da Silva, P. Q., Adu-Ampratwum, D., East, A., Lu, Z., Puccetti, N., Xue, M., Sun, H., Majumder, B. P., & Kumar, S. (2025).** _ScholarEval: Research Idea Evaluation Grounded in Literature_. arXiv preprint arXiv:2510.16234. [https://arxiv.org/abs/2510.16234](https://arxiv.org/abs/2510.16234)\r\n\r\n**Abstract:** ScholarEval is a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness (the empirical validity of proposed methods based on existing literature) and contribution (the degree of advancement made by the idea across different dimensions relative to prior research). The framework achieves significantly higher coverage of expert-annotated evaluation points and is consistently preferred over baseline systems in terms of evaluation actionability, depth, and evidence support.",
    "Resources": "### references/evaluation_framework.md\r\n\r\nDetailed evaluation criteria, rubrics, and quality indicators for each ScholarEval dimension. Load this reference when conducting evaluations to access specific assessment guidelines and scoring rubrics.\r\n\r\nSearch patterns for quick access:\r\n- \"Problem Formulation criteria\"\r\n- \"Literature Review rubric\"\r\n- \"Methodology assessment\"\r\n- \"Data quality indicators\"\r\n- \"Analysis rigor standards\"\r\n- \"Writing quality checklist\"\r\n\r\n### scripts/calculate_scores.py\r\n\r\nPython script for calculating aggregate evaluation scores from dimension-level ratings. Supports weighted averaging, threshold analysis, and score visualization.\r\n\r\nUsage:\r\n```python\r\npython scripts/calculate_scores.py --scores <dimension_scores.json> --output <report.txt>\r\n```",
    "Evaluation Workflow": "### Step 1: Initial Assessment and Scope Definition\r\n\r\nBegin by identifying the type of scholarly work being evaluated and the evaluation scope:\r\n\r\n**Work Types:**\r\n- Full research paper (empirical, theoretical, or review)\r\n- Research proposal or protocol\r\n- Literature review (systematic, narrative, or scoping)\r\n- Thesis or dissertation chapter\r\n- Conference abstract or short paper\r\n\r\n**Evaluation Scope:**\r\n- Comprehensive (all dimensions)\r\n- Targeted (specific aspects like methodology or writing)\r\n- Comparative (benchmarking against other work)\r\n\r\nAsk the user to clarify if the scope is ambiguous.\r\n\r\n### Step 2: Dimension-Based Evaluation\r\n\r\nSystematically evaluate the work across the ScholarEval dimensions. For each applicable dimension, assess quality, identify strengths and weaknesses, and provide scores where appropriate.\r\n\r\nRefer to `references/evaluation_framework.md` for detailed criteria and rubrics for each dimension.\r\n\r\n**Core Evaluation Dimensions:**\r\n\r\n1. **Problem Formulation & Research Questions**\r\n   - Clarity and specificity of research questions\r\n   - Theoretical or practical significance\r\n   - Feasibility and scope appropriateness\r\n   - Novelty and contribution potential\r\n\r\n2. **Literature Review**\r\n   - Comprehensiveness of coverage\r\n   - Critical synthesis vs. mere summarization\r\n   - Identification of research gaps\r\n   - Currency and relevance of sources\r\n   - Proper contextualization\r\n\r\n3. **Methodology & Research Design**\r\n   - Appropriateness for research questions\r\n   - Rigor and validity\r\n   - Reproducibility and transparency\r\n   - Ethical considerations\r\n   - Limitations acknowledgment\r\n\r\n4. **Data Collection & Sources**\r\n   - Quality and appropriateness of data\r\n   - Sample size and representativeness\r\n   - Data collection procedures\r\n   - Source credibility and reliability\r\n\r\n5. **Analysis & Interpretation**\r\n   - Appropriateness of analytical methods\r\n   - Rigor of analysis\r\n   - Logical coherence\r\n   - Alternative explanations considered\r\n   - Results-claims alignment\r\n\r\n6. **Results & Findings**\r\n   - Clarity of presentation\r\n   - Statistical or qualitative rigor\r\n   - Visualization quality\r\n   - Interpretation accuracy\r\n   - Implications discussion\r\n\r\n7. **Scholarly Writing & Presentation**\r\n   - Clarity and organization\r\n   - Academic tone and style\r\n   - Grammar and mechanics\r\n   - Logical flow\r\n   - Accessibility to target audience\r\n\r\n8. **Citations & References**\r\n   - Citation completeness\r\n   - Source quality and appropriateness\r\n   - Citation accuracy\r\n   - Balance of perspectives\r\n   - Adherence to citation standards\r\n\r\n### Step 3: Scoring and Rating\r\n\r\nFor each evaluated dimension, provide:\r\n\r\n**Qualitative Assessment:**\r\n- Key strengths (2-3 specific points)\r\n- Areas for improvement (2-3 specific points)\r\n- Critical issues (if any)\r\n\r\n**Quantitative Scoring (Optional):**\r\nUse a 5-point scale where applicable:\r\n- 5: Excellent - Exemplary quality, publishable in top venues\r\n- 4: Good - Strong quality with minor improvements needed\r\n- 3: Adequate - Acceptable quality with notable areas for improvement\r\n- 2: Needs Improvement - Significant revisions required\r\n- 1: Poor - Fundamental issues requiring major revision\r\n\r\nTo calculate aggregate scores programmatically, use `scripts/calculate_scores.py`.\r\n\r\n### Step 4: Synthesize Overall Assessment\r\n\r\nProvide an integrated evaluation summary:\r\n\r\n1. **Overall Quality Assessment** - Holistic judgment of the work's scholarly merit\r\n2. **Major Strengths** - 3-5 key strengths across dimensions\r\n3. **Critical Weaknesses** - 3-5 primary areas requiring attention\r\n4. **Priority Recommendations** - Ranked list of improvements by impact\r\n5. **Publication Readiness** (if applicable) - Assessment of suitability for target venues\r\n\r\n### Step 5: Provide Actionable Feedback\r\n\r\nTransform evaluation findings into constructive, actionable feedback:\r\n\r\n**Feedback Structure:**\r\n- **Specific** - Reference exact sections, paragraphs, or page numbers\r\n- **Actionable** - Provide concrete suggestions for improvement\r\n- **Prioritized** - Rank recommendations by importance and feasibility\r\n- **Balanced** - Acknowledge strengths while addressing weaknesses\r\n- **Evidence-based** - Ground feedback in evaluation criteria\r\n\r\n**Feedback Format Options:**\r\n- Structured report with dimension-by-dimension analysis\r\n- Annotated comments mapped to specific document sections\r\n- Executive summary with key findings and recommendations\r\n- Comparative analysis against benchmark standards\r\n\r\n### Step 6: Contextual Considerations\r\n\r\nAdjust evaluation approach based on:\r\n\r\n**Stage of Development:**\r\n- Early draft: Focus on conceptual and structural issues\r\n- Advanced draft: Focus on refinement and polish\r\n- Final submission: Comprehensive quality check\r\n\r\n**Purpose and Venue:**\r\n- Journal article: High standards for rigor and contribution\r\n- Conference paper: Balance novelty with presentation clarity\r\n- Student work: Educational feedback with developmental focus\r\n- Grant proposal: Emphasis on feasibility and impact\r\n\r\n**Discipline-Specific Norms:**\r\n- STEM fields: Emphasis on reproducibility and statistical rigor\r\n- Social sciences: Balance quantitative and qualitative standards\r\n- Humanities: Focus on argumentation and scholarly interpretation"
  },
  "references": {
    "files": [
      "references/evaluation_framework.md"
    ]
  },
  "id": "scientific-thinking-scholar-evaluation",
  "name": "scholar-evaluation",
  "description": "Systematic framework for evaluating scholarly and research work based on the ScholarEval methodology. This skill should be used when assessing research papers, evaluating literature reviews, scoring research methodologies, analyzing scientific writing quality, or applying structured evaluation criteria to academic work. Provides comprehensive assessment across multiple dimensions including problem formulation, literature review, methodology, data collection, analysis, results interpretation, and scholarly writing quality."
}