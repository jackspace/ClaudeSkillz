{
  "description": "This skill should be used when working with pre-trained transformer models for natural language processing, computer vision, audio, or multimodal tasks. Use for text generation, classification, question answering, translation, summarization, image classification, object detection, speech recognition, and fine-tuning models on custom datasets.",
  "references": {
    "files": [
      "references/generation.md",
      "references/models.md",
      "references/pipelines.md",
      "references/tokenizers.md",
      "references/training.md"
    ]
  },
  "content": "Use the Pipeline API for fast inference without manual configuration:\r\n\r\n```python\r\nfrom transformers import pipeline\r\n\r\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\r\nresult = generator(\"The future of AI is\", max_length=50)\r\n\r\nclassifier = pipeline(\"text-classification\")\r\nresult = classifier(\"This movie was excellent!\")",
  "name": "transformers",
  "id": "scientific-pkg-transformers",
  "sections": {
    "Quick Start": "qa = pipeline(\"question-answering\")\r\nresult = qa(question=\"What is AI?\", context=\"AI is artificial intelligence...\")\r\n```",
    "Common Patterns": "### Pattern 1: Simple Inference\r\nFor straightforward tasks, use pipelines:\r\n```python\r\npipe = pipeline(\"task-name\", model=\"model-id\")\r\noutput = pipe(input_data)\r\n```\r\n\r\n### Pattern 2: Custom Model Usage\r\nFor advanced control, load model and tokenizer separately:\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"model-id\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"model-id\", device_map=\"auto\")\r\n\r\ninputs = tokenizer(\"text\", return_tensors=\"pt\")\r\noutputs = model.generate(**inputs, max_new_tokens=100)\r\nresult = tokenizer.decode(outputs[0])\r\n```\r\n\r\n### Pattern 3: Fine-Tuning\r\nFor task adaptation, use Trainer:\r\n```python\r\nfrom transformers import Trainer, TrainingArguments\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./results\",\r\n    num_train_epochs=3,\r\n    per_device_train_batch_size=8,\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n)\r\n\r\ntrainer.train()\r\n```",
    "Installation": "Install transformers and core dependencies:\r\n\r\n```bash\r\nuv pip install torch transformers datasets evaluate accelerate\r\n```\r\n\r\nFor vision tasks, add:\r\n```bash\r\nuv pip install timm pillow\r\n```\r\n\r\nFor audio tasks, add:\r\n```bash\r\nuv pip install librosa soundfile\r\n```",
    "Authentication": "Many models on the Hugging Face Hub require authentication. Set up access:\r\n\r\n```python\r\nfrom huggingface_hub import login\r\nlogin()  # Follow prompts to enter token\r\n```\r\n\r\nOr set environment variable:\r\n```bash\r\nexport HUGGINGFACE_TOKEN=\"your_token_here\"\r\n```\r\n\r\nGet tokens at: https://huggingface.co/settings/tokens",
    "Overview": "The Hugging Face Transformers library provides access to thousands of pre-trained models for tasks across NLP, computer vision, audio, and multimodal domains. Use this skill to load models, perform inference, and fine-tune on custom data.",
    "Reference Documentation": "For detailed information on specific components:\r\n- **Pipelines**: `references/pipelines.md` - All supported tasks and optimization\r\n- **Models**: `references/models.md` - Loading, saving, and configuration\r\n- **Generation**: `references/generation.md` - Text generation strategies and parameters\r\n- **Training**: `references/training.md` - Fine-tuning with Trainer API\r\n- **Tokenizers**: `references/tokenizers.md` - Tokenization and preprocessing",
    "Core Capabilities": "### 1. Pipelines for Quick Inference\r\n\r\nUse for simple, optimized inference across many tasks. Supports text generation, classification, NER, question answering, summarization, translation, image classification, object detection, audio classification, and more.\r\n\r\n**When to use**: Quick prototyping, simple inference tasks, no custom preprocessing needed.\r\n\r\nSee `references/pipelines.md` for comprehensive task coverage and optimization.\r\n\r\n### 2. Model Loading and Management\r\n\r\nLoad pre-trained models with fine-grained control over configuration, device placement, and precision.\r\n\r\n**When to use**: Custom model initialization, advanced device management, model inspection.\r\n\r\nSee `references/models.md` for loading patterns and best practices.\r\n\r\n### 3. Text Generation\r\n\r\nGenerate text with LLMs using various decoding strategies (greedy, beam search, sampling) and control parameters (temperature, top-k, top-p).\r\n\r\n**When to use**: Creative text generation, code generation, conversational AI, text completion.\r\n\r\nSee `references/generation.md` for generation strategies and parameters.\r\n\r\n### 4. Training and Fine-Tuning\r\n\r\nFine-tune pre-trained models on custom datasets using the Trainer API with automatic mixed precision, distributed training, and logging.\r\n\r\n**When to use**: Task-specific model adaptation, domain adaptation, improving model performance.\r\n\r\nSee `references/training.md` for training workflows and best practices.\r\n\r\n### 5. Tokenization\r\n\r\nConvert text to tokens and token IDs for model input, with padding, truncation, and special token handling.\r\n\r\n**When to use**: Custom preprocessing pipelines, understanding model inputs, batch processing.\r\n\r\nSee `references/tokenizers.md` for tokenization details."
  }
}