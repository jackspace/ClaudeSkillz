{
  "description": "Prevent request timeouts in Claude Code sessions by chunking long operations, implementing progress checkpoints, using background processes, and optimizing tool usage patterns. Use when performing bulk operations, processing large datasets, running long-running commands, downloading multiple files, or executing tasks that exceed 2-minute timeout limits. Implements strategies like task decomposition, incremental processing, parallel execution, checkpoint-resume patterns, and efficient resource management to ensure reliable completion of complex workflows.",
  "metadata": {
    "license": "MIT"
  },
  "content": "Expert strategies for preventing request timeouts in Claude Code by breaking down long operations into manageable chunks with progress tracking and recovery mechanisms.\r\n\r\n\r\n### 1. Task Chunking\r\n\r\nBreak large operations into smaller batches:\r\n\r\n```bash\r\nfor file in $(find . -name \"*.txt\"); do\r\n    process_file \"$file\"\r\ndone\r\n\r\nfind . -name \"*.txt\" | head -10 | while read file; do\r\n    process_file \"$file\"\r\ndone\r\n```\r\n\r\n### 2. Progress Checkpoints\r\n\r\nSave progress to resume if interrupted:\r\n\r\n```bash\r\nCHECKPOINT_FILE=\"progress.txt\"\r\n\r\necho \"completed_item_1\" >> \"$CHECKPOINT_FILE\"\r\n\r\nif grep -q \"completed_item_1\" \"$CHECKPOINT_FILE\"; then\r\n    echo \"Skipping already processed item\"\r\nfi\r\n```\r\n\r\n### 3. Background Processes\r\n\r\nRun long operations in background:\r\n\r\n```bash\r\nlong_command > output.log 2>&1 &\r\necho $! > pid.txt\r\n\r\nif ps -p $(cat pid.txt) > /dev/null; then\r\n    echo \"Still running\"\r\nelse\r\n    echo \"Completed\"\r\n    cat output.log\r\nfi\r\n```\r\n\r\n### 4. Incremental Processing\r\n\r\nProcess and report incrementally:\r\n\r\n```bash\r\nTOTAL=100\r\nfor i in $(seq 1 10); do  # First 10 of 100\r\n    process_item $i\r\n    echo \"Progress: $i/$TOTAL\"\r\ndone\r\n```\r\n\r\n### 5. Timeout Configuration\r\n\r\nSet explicit timeouts for commands:\r\n\r\n```bash\r\ntimeout 60s long_command || echo \"Command timed out\"\r\n\r\n\r\n### Pattern 1: Batch File Processing\r\n\r\n```bash\r\nBATCH_SIZE=10\r\nOFFSET=${1:-0}  # Accept offset as parameter\r\n\r\nfind . -name \"*.jpg\" | tail -n +$((OFFSET + 1)) | head -$BATCH_SIZE | while read file; do\r\n    convert \"$file\" -resize 800x600 \"thumbnails/$(basename $file)\"\r\n    echo \"Processed: $file\"\r\ndone\r\n\r\nNEXT_OFFSET=$((OFFSET + BATCH_SIZE))\r\necho \"Next batch: Run with offset $NEXT_OFFSET\"\r\n```\r\n\r\n**Usage**:\r\n```bash\r\n./process.sh 0\r\n\r\n./process.sh 10\r\n\r\n./process.sh 20\r\n```\r\n\r\n### Pattern 2: Repository Cloning with Checkpoints\r\n\r\n```bash\r\n#!/bin/bash\r\nREPOS_FILE=\"repos.txt\"\r\nCHECKPOINT=\"downloaded.txt\"\r\n\r\nwhile read repo; do\r\n    # Check if already downloaded\r\n    if grep -q \"$repo\" \"$CHECKPOINT\" 2>/dev/null; then\r\n        echo \"✓ Skipping $repo (already downloaded)\"\r\n        continue\r\n    fi\r\n\r\n    # Clone with timeout\r\n    echo \"Downloading $repo...\"\r\n    if timeout 60s git clone --depth 1 \"$repo\" 2>/dev/null; then\r\n        echo \"$repo\" >> \"$CHECKPOINT\"\r\n        echo \"✓ Downloaded $repo\"\r\n    else\r\n        echo \"✗ Failed: $repo\"\r\n    fi\r\ndone < <(head -5 \"$REPOS_FILE\")  # Only 5 at a time\r\n\r\necho \"Processed 5 repos. Check $CHECKPOINT for status.\"\r\n```\r\n\r\n### Pattern 3: Progress-Tracked Downloads\r\n\r\n```bash\r\n#!/bin/bash\r\nURLS_FILE=\"urls.txt\"\r\nPROGRESS_FILE=\"download_progress.json\"\r\n\r\n[ ! -f \"$PROGRESS_FILE\" ] && echo '{\"completed\": 0, \"total\": 0}' > \"$PROGRESS_FILE\"\r\n\r\nCOMPLETED=$(jq -r '.completed' \"$PROGRESS_FILE\")\r\nTOTAL=$(wc -l < \"$URLS_FILE\")\r\n\r\nBATCH_SIZE=5\r\ntail -n +$((COMPLETED + 1)) \"$URLS_FILE\" | head -$BATCH_SIZE | while read url; do\r\n    echo \"Downloading: $url\"\r\n    if yt-dlp --socket-timeout 30 \"$url\"; then\r\n        COMPLETED=$((COMPLETED + 1))\r\n        echo \"{\\\"completed\\\": $COMPLETED, \\\"total\\\": $TOTAL}\" > \"$PROGRESS_FILE\"\r\n    fi\r\ndone\r\n\r\nCOMPLETED=$(jq -r '.completed' \"$PROGRESS_FILE\")\r\necho \"Progress: $COMPLETED/$TOTAL\"\r\n[ $COMPLETED -lt $TOTAL ] && echo \"Run again to continue.\"\r\n```\r\n\r\n### Pattern 4: Parallel Execution with Rate Limiting\r\n\r\n```bash\r\n#!/bin/bash\r\nMAX_PARALLEL=3\r\nCOUNT=0\r\n\r\nfor item in $(seq 1 15); do\r\n    # Start background job\r\n    (\r\n        process_item $item\r\n        echo \"Completed: $item\"\r\n    ) &\r\n\r\n    COUNT=$((COUNT + 1))\r\n\r\n    # Wait when reaching max parallel\r\n    if [ $COUNT -ge $MAX_PARALLEL ]; then\r\n        wait -n  # Wait for any job to finish\r\n        COUNT=$((COUNT - 1))\r\n    fi\r\n\r\n    # Rate limiting\r\n    sleep 0.5\r\ndone\r\n\r\nwait\r\necho \"All items processed\"\r\n```\r\n\r\n### Pattern 5: Resumable State Machine\r\n\r\n```bash\r\n#!/bin/bash\r\nSTATE_FILE=\"workflow_state.txt\"\r\n\r\n\r\n### Optimize Bash Commands\r\n\r\n```bash\r\ngit clone repo1\r\ngit clone repo2\r\ngit clone repo3\r\n\r\ngit clone repo1 &\r\ngit clone repo2 &\r\ngit clone repo3 &\r\nwait\r\n```\r\n\r\n### Optimize File Operations\r\n\r\n```bash\r\nfor file in *.txt; do\r\n    cat \"$file\" | process > \"output/$file\"\r\ndone\r\n\r\nfor file in *.txt; do\r\n    cat \"$file\" | process > \"output/$file\" &\r\ndone\r\nwait\r\n```\r\n\r\n### Optimize Network Calls\r\n\r\n```bash\r\nfor id in {1..100}; do\r\n    curl \"https://api.example.com/item/$id\"\r\ndone\r\n\r\n\r\n### Strategy 1: Multi-Turn Workflow\r\n\r\nInstead of one long operation, break into multiple turns:\r\n\r\n**Turn 1**:\r\n```\r\nProcess first 10 repositories and report progress\r\n```\r\n\r\n**Turn 2**:\r\n```\r\nProcess next 10 repositories (11-20)\r\n```\r\n\r\n**Turn 3**:\r\n```\r\nProcess final batch and generate report\r\n```\r\n\r\n### Strategy 2: Status Check Pattern\r\n\r\n```bash\r\n./long_operation.sh > output.log 2>&1 &\r\necho $! > pid.txt\r\necho \"Started background process\"\r\n\r\n\r\n### Graceful Degradation\r\n\r\n```bash\r\n\r\n### Scenario 1: Downloading 100 YouTube Videos\r\n\r\n```bash\r\nyt-dlp -a urls.txt  # 100 URLs\r\n\r\nhead -5 urls.txt | yt-dlp -a -\r\n```\r\n\r\n### Scenario 2: Cloning 50 Repositories\r\n\r\n```bash\r\nfor repo in $(cat repos.txt); do\r\n    git clone $repo\r\ndone\r\n\r\nhead -5 repos.txt | while read repo; do\r\n    timeout 60s git clone --depth 1 $repo\r\ndone\r\n```\r\n\r\n### Scenario 3: Processing 1000 Images\r\n\r\n```bash\r\nfor img in *.jpg; do\r\n    convert $img processed/$img\r\ndone\r\n\r\nfind . -name \"*.jpg\" | head -20 | xargs -P 4 -I {} convert {} processed/{}\r\n\r\n### Track Execution Time\r\n\r\n```bash\r\nstart_time=$(date +%s)\r\n\r\n### Checkpoint-Based Recovery\r\n\r\n```bash\r\nLAST_CHECKPOINT=$(cat .checkpoint 2>/dev/null || echo 0)\r\necho \"Resuming from item $LAST_CHECKPOINT\"\r\n\r\nfor i in $(seq $((LAST_CHECKPOINT + 1)) $((LAST_CHECKPOINT + 10))); do\r\n    process_item $i && echo $i > .checkpoint\r\ndone\r\n```\r\n\r\n### Transaction-Like Operations\r\n\r\n```bash\r\nTEMP_DIR=$(mktemp -d)\r\ntrap \"rm -rf $TEMP_DIR\" EXIT",
  "name": "timeout-prevention",
  "id": "timeout-prevention",
  "sections": {
    "Command Optimization": "curl \"https://api.example.com/items?ids=1,2,3,4,5\"\r\n```",
    "Common Scenarios": "```",
    "Monitoring and Debugging": "end_time=$(date +%s)\r\nduration=$((end_time - start_time))\r\necho \"Duration: ${duration}s\"\r\n```\r\n\r\n### Log Everything\r\n\r\n```bash\r\nLOG_FILE=\"operation.log\"\r\n{\r\n    echo \"Started at $(date)\"\r\n    perform_operations\r\n    echo \"Completed at $(date)\"\r\n} | tee -a \"$LOG_FILE\"\r\n```\r\n\r\n### Progress Indicators\r\n\r\n```bash\r\nTOTAL=100\r\nfor i in $(seq 1 10); do\r\n    process_item $i\r\n    echo -ne \"Progress: $i/$TOTAL\\r\"\r\ndone\r\necho \"\"\r\n```",
    "Best Practices": "### ✅ DO\r\n\r\n1. **Chunk Large Operations**: Process 5-10 items at a time\r\n2. **Save Progress**: Use checkpoint files\r\n3. **Report Incrementally**: Show results as you go\r\n4. **Use Timeouts**: Set explicit timeout values\r\n5. **Parallelize Wisely**: 3-5 concurrent operations max\r\n6. **Enable Resume**: Make operations resumable\r\n7. **Monitor Progress**: Show completion percentage\r\n8. **Background Long Tasks**: Use `&` for async operations\r\n\r\n### ❌ DON'T\r\n\r\n1. **Don't Process Everything**: Batch large datasets\r\n2. **Don't Block Forever**: Always set timeouts\r\n3. **Don't Ignore State**: Save progress between runs\r\n4. **Don't Go Silent**: Report progress regularly\r\n5. **Don't Retry Infinitely**: Limit retry attempts\r\n6. **Don't Over-Parallelize**: Limit concurrent operations\r\n7. **Don't Assume Success**: Always check return codes\r\n8. **Don't Skip Cleanup**: Clean up temp files",
    "Error Handling": "if timeout 30s expensive_operation; then\r\n    echo \"Operation completed\"\r\nelse\r\n    echo \"Operation timed out, using cached result\"\r\n    cat cached_result.txt\r\nfi\r\n```\r\n\r\n### Retry Logic\r\n\r\n```bash\r\nMAX_RETRIES=3\r\nRETRY_COUNT=0\r\n\r\nwhile [ $RETRY_COUNT -lt $MAX_RETRIES ]; do\r\n    if timeout 60s risky_operation; then\r\n        echo \"Success!\"\r\n        break\r\n    else\r\n        RETRY_COUNT=$((RETRY_COUNT + 1))\r\n        echo \"Attempt $RETRY_COUNT failed, retrying...\"\r\n        sleep 5\r\n    fi\r\ndone\r\n```",
    "When to Use This Skill": "Use when:\r\n- Performing bulk operations (100+ files, repos, etc.)\r\n- Running long commands (>60 seconds)\r\n- Processing large datasets\r\n- Downloading multiple files sequentially\r\n- Executing complex multi-step workflows\r\n- Working with slow external APIs\r\n- Any operation that risks timing out",
    "Practical Patterns": "STATE=$(cat \"$STATE_FILE\" 2>/dev/null || echo \"START\")\r\n\r\ncase $STATE in\r\n    START)\r\n        echo \"Phase 1: Initialization\"\r\n        initialize_project\r\n        echo \"PHASE1\" > \"$STATE_FILE\"\r\n        echo \"Run again to continue to Phase 2\"\r\n        ;;\r\n    PHASE1)\r\n        echo \"Phase 2: Processing (Batch 1 of 3)\"\r\n        process_batch_1\r\n        echo \"PHASE2\" > \"$STATE_FILE\"\r\n        echo \"Run again for Phase 3\"\r\n        ;;\r\n    PHASE2)\r\n        echo \"Phase 3: Processing (Batch 2 of 3)\"\r\n        process_batch_2\r\n        echo \"PHASE3\" > \"$STATE_FILE\"\r\n        echo \"Run again for Phase 4\"\r\n        ;;\r\n    PHASE3)\r\n        echo \"Phase 4: Processing (Batch 3 of 3)\"\r\n        process_batch_3\r\n        echo \"PHASE4\" > \"$STATE_FILE\"\r\n        echo \"Run again for finalization\"\r\n        ;;\r\n    PHASE4)\r\n        echo \"Phase 5: Finalization\"\r\n        finalize_project\r\n        echo \"COMPLETE\" > \"$STATE_FILE\"\r\n        echo \"Workflow complete!\"\r\n        ;;\r\n    COMPLETE)\r\n        echo \"Workflow already completed\"\r\n        ;;\r\nesac\r\n```",
    "Core Prevention Strategies": "yt-dlp --socket-timeout 30 \"URL\"\r\ngit clone --depth 1 --timeout 60 \"URL\"\r\n```",
    "Recovery Strategies": "cd $TEMP_DIR\r\nperform_operations\r\nif [ $? -eq 0 ]; then\r\n    mv $TEMP_DIR/results ~/final/\r\nfi\r\n```\r\n\r\n---\r\n\r\n**Version**: 1.0.0\r\n**Last Updated**: 2025-11-07\r\n**Key Principle**: Break long operations into manageable chunks with progress tracking and resumability",
    "Understanding Timeout Limits": "### Claude Code Timeout Behavior\r\n- **Default Tool Timeout**: 120 seconds (2 minutes)\r\n- **Request Timeout**: Entire response cycle\r\n- **What Causes Timeouts**:\r\n  - Long-running bash commands\r\n  - Large file operations\r\n  - Sequential processing of many items\r\n  - Network operations without proper timeout settings\r\n  - Blocking operations without progress",
    "Claude Code Integration": "if ps -p $(cat pid.txt) > /dev/null; then\r\n    echo \"Still running... Check again\"\r\n    tail -20 output.log\r\nelse\r\n    echo \"Completed!\"\r\n    cat output.log\r\nfi\r\n```\r\n\r\n### Strategy 3: Incremental Results\r\n\r\nReport partial results as you go:\r\n\r\n```bash\r\necho \"=== Batch 1 Results ===\"\r\nprocess_batch_1\r\necho \"\"\r\necho \"=== Batch 2 Results ===\"\r\nprocess_batch_2\r\n```"
  }
}