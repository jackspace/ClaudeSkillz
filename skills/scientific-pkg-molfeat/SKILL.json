{
  "description": "\"Molecular featurization for ML (100+ featurizers). ECFP, MACCS, descriptors, pretrained models (ChemBERTa), convert SMILES to features, for QSAR and molecular ML.\"",
  "references": {
    "files": [
      "references/api_reference.md",
      "references/available_featurizers.md",
      "references/examples.md"
    ]
  },
  "content": "```bash\r\nmamba install -c conda-forge molfeat\r\n\r\npip install molfeat\r\n\r\n\r\n### Basic Featurization\r\n\r\n```python\r\nimport datamol as dm\r\nfrom molfeat.calc import FPCalculator\r\nfrom molfeat.trans import MoleculeTransformer\r\n\r\nsmiles = [\"CCO\", \"CC(=O)O\", \"c1ccccc1\", \"CC(C)O\"]\r\n\r\ncalc = FPCalculator(\"ecfp\", radius=3)\r\ntransformer = MoleculeTransformer(calc, n_jobs=-1)\r\n\r\nfeatures = transformer(smiles)\r\nprint(f\"Shape: {features.shape}\")  # (4, 2048)\r\n```\r\n\r\n### Save and Load Configuration\r\n\r\n```python\r\ntransformer.to_state_yaml_file(\"featurizer_config.yml\")\r\n\r\nloaded = MoleculeTransformer.from_state_yaml_file(\"featurizer_config.yml\")\r\n```\r\n\r\n### Handle Errors Gracefully\r\n\r\n```python\r\ntransformer = MoleculeTransformer(\r\n    calc,\r\n    n_jobs=-1,\r\n    ignore_errors=True,  # Continue on failures\r\n    verbose=True          # Log error details\r\n)\r\n\r\nfeatures = transformer(smiles_with_errors)\r\n\r\n### For Traditional Machine Learning (RF, SVM, XGBoost)\r\n\r\n**Start with fingerprints:**\r\n```python\r\nFPCalculator(\"ecfp\", radius=3, fpSize=2048)\r\n\r\nFPCalculator(\"maccs\")\r\n\r\nFPCalculator(\"map4\")\r\n```\r\n\r\n**For interpretable models:**\r\n```python\r\nfrom molfeat.calc import RDKitDescriptors2D\r\nRDKitDescriptors2D()\r\n\r\nfrom molfeat.calc import MordredDescriptors\r\nMordredDescriptors()\r\n```\r\n\r\n**Combine multiple featurizers:**\r\n```python\r\nfrom molfeat.trans import FeatConcat\r\n\r\nconcat = FeatConcat([\r\n    FPCalculator(\"maccs\"),      # 167 dimensions\r\n    FPCalculator(\"ecfp\")         # 2048 dimensions\r\n])  # Result: 2215-dimensional combined features\r\n```\r\n\r\n### For Deep Learning\r\n\r\n**Transformer-based embeddings:**\r\n```python\r\nPretrainedMolTransformer(\"ChemBERTa-77M-MLM\")\r\n\r\nPretrainedMolTransformer(\"ChemGPT-1.2B\")\r\n```\r\n\r\n**Graph neural networks:**\r\n```python\r\nPretrainedMolTransformer(\"gin-supervised-masking\")\r\nPretrainedMolTransformer(\"gin-supervised-infomax\")\r\n\r\nPretrainedMolTransformer(\"Graphormer-pcqm4mv2\")\r\n```\r\n\r\n### For Similarity Searching\r\n\r\n```python\r\nFPCalculator(\"ecfp\")\r\n\r\nFPCalculator(\"maccs\")\r\n\r\nFPCalculator(\"map4\")\r\n\r\nfrom molfeat.calc import USRDescriptors\r\nUSRDescriptors()\r\n```\r\n\r\n### For Pharmacophore-Based Approaches\r\n\r\n```python\r\nFPCalculator(\"fcfp\")\r\n\r\nfrom molfeat.calc import CATSCalculator\r\nCATSCalculator(mode=\"2D\")\r\n\r\n\r\n### Building a QSAR Model\r\n\r\n```python\r\nfrom molfeat.trans import MoleculeTransformer\r\nfrom molfeat.calc import FPCalculator\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.model_selection import cross_val_score\r\n\r\ntransformer = MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\r\nX = transformer(smiles_train)\r\n\r\nmodel = RandomForestRegressor(n_estimators=100)\r\nscores = cross_val_score(model, X, y_train, cv=5)\r\nprint(f\"RÂ² = {scores.mean():.3f}\")\r\n\r\ntransformer.to_state_yaml_file(\"production_featurizer.yml\")\r\n```\r\n\r\n### Virtual Screening Pipeline\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\ntransformer = MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\r\nX_train = transformer(train_smiles)\r\nclf = RandomForestClassifier(n_estimators=500)\r\nclf.fit(X_train, train_labels)\r\n\r\nX_screen = transformer(screening_library)  # e.g., 1M compounds\r\npredictions = clf.predict_proba(X_screen)[:, 1]\r\n\r\ntop_indices = predictions.argsort()[::-1][:1000]\r\ntop_hits = [screening_library[i] for i in top_indices]\r\n```\r\n\r\n### Similarity Search\r\n\r\n```python\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\ncalc = FPCalculator(\"ecfp\")\r\nquery_fp = calc(query_smiles).reshape(1, -1)\r\n\r\ntransformer = MoleculeTransformer(calc, n_jobs=-1)\r\ndatabase_fps = transformer(database_smiles)\r\n\r\nsimilarities = cosine_similarity(query_fp, database_fps)[0]\r\ntop_similar = similarities.argsort()[-10:][::-1]\r\n```\r\n\r\n### Scikit-learn Pipeline Integration\r\n\r\n```python\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\npipeline = Pipeline([\r\n    ('featurizer', MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)),\r\n    ('classifier', RandomForestClassifier(n_estimators=100))\r\n])\r\n\r\n\r\nUse the ModelStore to explore all available featurizers:\r\n\r\n```python\r\nfrom molfeat.store.modelstore import ModelStore\r\n\r\nstore = ModelStore()\r\n\r\nall_models = store.available_models\r\nprint(f\"Total featurizers: {len(all_models)}\")\r\n\r\nchemberta_models = store.search(name=\"ChemBERTa\")\r\nfor model in chemberta_models:\r\n    print(f\"- {model.name}: {model.description}\")\r\n\r\nmodel_card = store.search(name=\"ChemBERTa-77M-MLM\")[0]\r\nmodel_card.usage()  # Display usage examples",
  "name": "molfeat",
  "id": "scientific-pkg-molfeat",
  "sections": {
    "Additional Resources": "- **Official Documentation**: https://molfeat-docs.datamol.io/\r\n- **GitHub Repository**: https://github.com/datamol-io/molfeat\r\n- **PyPI Package**: https://pypi.org/project/molfeat/\r\n- **Tutorial**: https://portal.valencelabs.com/datamol/post/types-of-featurizers-b1e8HHrbFMkbun6",
    "Overview": "Molfeat is a comprehensive Python library for molecular featurization that unifies 100+ pre-trained embeddings and hand-crafted featurizers. Convert chemical structures (SMILES strings or RDKit molecules) into numerical representations for machine learning tasks including QSAR modeling, virtual screening, similarity searching, and deep learning applications. Features fast parallel processing, scikit-learn compatible transformers, and built-in caching.",
    "Advanced Features": "### Custom Preprocessing\r\n\r\n```python\r\nclass CustomTransformer(MoleculeTransformer):\r\n    def preprocess(self, mol):\r\n        \"\"\"Custom preprocessing pipeline\"\"\"\r\n        if isinstance(mol, str):\r\n            mol = dm.to_mol(mol)\r\n        mol = dm.standardize_mol(mol)\r\n        mol = dm.remove_salts(mol)\r\n        return mol\r\n\r\ntransformer = CustomTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\r\n```\r\n\r\n### Batch Processing Large Datasets\r\n\r\n```python\r\ndef featurize_in_chunks(smiles_list, transformer, chunk_size=10000):\r\n    \"\"\"Process large datasets in chunks to manage memory\"\"\"\r\n    all_features = []\r\n    for i in range(0, len(smiles_list), chunk_size):\r\n        chunk = smiles_list[i:i+chunk_size]\r\n        features = transformer(chunk)\r\n        all_features.append(features)\r\n    return np.vstack(all_features)\r\n```\r\n\r\n### Caching Expensive Embeddings\r\n\r\n```python\r\nimport pickle\r\n\r\ncache_file = \"embeddings_cache.pkl\"\r\ntransformer = PretrainedMolTransformer(\"ChemBERTa-77M-MLM\", n_jobs=-1)\r\n\r\ntry:\r\n    with open(cache_file, \"rb\") as f:\r\n        embeddings = pickle.load(f)\r\nexcept FileNotFoundError:\r\n    embeddings = transformer(smiles_list)\r\n    with open(cache_file, \"wb\") as f:\r\n        pickle.dump(embeddings, f)\r\n```",
    "Core Concepts": "Molfeat organizes featurization into three hierarchical classes:\r\n\r\n### 1. Calculators (`molfeat.calc`)\r\n\r\nCallable objects that convert individual molecules into feature vectors. Accept RDKit `Chem.Mol` objects or SMILES strings.\r\n\r\n**Use calculators for:**\r\n- Single molecule featurization\r\n- Custom processing loops\r\n- Direct feature computation\r\n\r\n**Example:**\r\n```python\r\nfrom molfeat.calc import FPCalculator\r\n\r\ncalc = FPCalculator(\"ecfp\", radius=3, fpSize=2048)\r\nfeatures = calc(\"CCO\")  # Returns numpy array (2048,)\r\n```\r\n\r\n### 2. Transformers (`molfeat.trans`)\r\n\r\nScikit-learn compatible transformers that wrap calculators for batch processing with parallelization.\r\n\r\n**Use transformers for:**\r\n- Batch featurization of molecular datasets\r\n- Integration with scikit-learn pipelines\r\n- Parallel processing (automatic CPU utilization)\r\n\r\n**Example:**\r\n```python\r\nfrom molfeat.trans import MoleculeTransformer\r\nfrom molfeat.calc import FPCalculator\r\n\r\ntransformer = MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\r\nfeatures = transformer(smiles_list)  # Parallel processing\r\n```\r\n\r\n### 3. Pretrained Transformers (`molfeat.trans.pretrained`)\r\n\r\nSpecialized transformers for deep learning models with batched inference and caching.\r\n\r\n**Use pretrained transformers for:**\r\n- State-of-the-art molecular embeddings\r\n- Transfer learning from large chemical datasets\r\n- Deep learning feature extraction\r\n\r\n**Example:**\r\n```python\r\nfrom molfeat.trans.pretrained import PretrainedMolTransformer\r\n\r\ntransformer = PretrainedMolTransformer(\"ChemBERTa-77M-MLM\", n_jobs=-1)\r\nembeddings = transformer(smiles_list)  # Deep learning embeddings\r\n```",
    "Troubleshooting": "### Invalid Molecules\r\nEnable error handling to skip invalid SMILES:\r\n```python\r\ntransformer = MoleculeTransformer(\r\n    calc,\r\n    ignore_errors=True,\r\n    verbose=True\r\n)\r\n```\r\n\r\n### Memory Issues with Large Datasets\r\nProcess in chunks or use streaming approaches for datasets > 100K molecules.\r\n\r\n### Pretrained Model Dependencies\r\nSome models require additional packages. Install specific extras:\r\n```bash\r\npip install \"molfeat[transformer]\"  # For ChemBERTa/ChemGPT\r\npip install \"molfeat[dgl]\"          # For GIN models\r\n```\r\n\r\n### Reproducibility\r\nSave exact configurations and document versions:\r\n```python\r\ntransformer.to_state_yaml_file(\"config.yml\")\r\nimport molfeat\r\nprint(f\"molfeat version: {molfeat.__version__}\")\r\n```",
    "Installation": "pip install \"molfeat[all]\"\r\n```\r\n\r\n**Optional dependencies for specific featurizers:**\r\n- `molfeat[dgl]` - GNN models (GIN variants)\r\n- `molfeat[graphormer]` - Graphormer models\r\n- `molfeat[transformer]` - ChemBERTa, ChemGPT, MolT5\r\n- `molfeat[fcd]` - FCD descriptors\r\n- `molfeat[map4]` - MAP4 fingerprints",
    "Common Featurizers Reference": "**Quick reference for frequently used featurizers:**\r\n\r\n| Featurizer | Type | Dimensions | Speed | Use Case |\r\n|------------|------|------------|-------|----------|\r\n| `ecfp` | Fingerprint | 2048 | Fast | General purpose |\r\n| `maccs` | Fingerprint | 167 | Very fast | Scaffold similarity |\r\n| `desc2D` | Descriptors | 200+ | Fast | Interpretable models |\r\n| `mordred` | Descriptors | 1800+ | Medium | Comprehensive features |\r\n| `map4` | Fingerprint | 1024 | Fast | Large-scale screening |\r\n| `ChemBERTa-77M-MLM` | Deep learning | 768 | Slow* | Transfer learning |\r\n| `gin-supervised-masking` | GNN | Variable | Slow* | Graph-based models |\r\n\r\n*First run is slow; subsequent runs benefit from caching",
    "Common Workflows": "pipeline.fit(smiles_train, y_train)\r\npredictions = pipeline.predict(smiles_test)\r\n```\r\n\r\n### Comparing Multiple Featurizers\r\n\r\n```python\r\nfeaturizers = {\r\n    'ECFP': FPCalculator(\"ecfp\"),\r\n    'MACCS': FPCalculator(\"maccs\"),\r\n    'Descriptors': RDKitDescriptors2D(),\r\n    'ChemBERTa': PretrainedMolTransformer(\"ChemBERTa-77M-MLM\")\r\n}\r\n\r\nresults = {}\r\nfor name, feat in featurizers.items():\r\n    transformer = MoleculeTransformer(feat, n_jobs=-1)\r\n    X = transformer(smiles)\r\n    # Evaluate with your ML model\r\n    score = evaluate_model(X, y)\r\n    results[name] = score\r\n```",
    "Resources": "This skill includes comprehensive reference documentation:\r\n\r\n### references/api_reference.md\r\nComplete API documentation covering:\r\n- `molfeat.calc` - All calculator classes and parameters\r\n- `molfeat.trans` - Transformer classes and methods\r\n- `molfeat.store` - ModelStore usage\r\n- Common patterns and integration examples\r\n- Performance optimization tips\r\n\r\n**When to load:** Reference when implementing specific calculators, understanding transformer parameters, or integrating with scikit-learn/PyTorch.\r\n\r\n### references/available_featurizers.md\r\nComprehensive catalog of all 100+ featurizers organized by category:\r\n- Transformer-based language models (ChemBERTa, ChemGPT)\r\n- Graph neural networks (GIN, Graphormer)\r\n- Molecular descriptors (RDKit, Mordred)\r\n- Fingerprints (ECFP, MACCS, MAP4, and 15+ others)\r\n- Pharmacophore descriptors (CATS, Gobbi)\r\n- Shape descriptors (USR, ElectroShape)\r\n- Scaffold-based descriptors\r\n\r\n**When to load:** Reference when selecting the optimal featurizer for a specific task, exploring available options, or understanding featurizer characteristics.\r\n\r\n**Search tip:** Use grep to find specific featurizer types:\r\n```bash\r\ngrep -i \"chembert\" references/available_featurizers.md\r\ngrep -i \"pharmacophore\" references/available_featurizers.md\r\n```\r\n\r\n### references/examples.md\r\nPractical code examples for common scenarios:\r\n- Installation and quick start\r\n- Calculator and transformer examples\r\n- Pretrained model usage\r\n- Scikit-learn and PyTorch integration\r\n- Virtual screening workflows\r\n- QSAR model building\r\n- Similarity searching\r\n- Troubleshooting and best practices\r\n\r\n**When to load:** Reference when implementing specific workflows, troubleshooting issues, or learning molfeat patterns.",
    "Performance Tips": "1. **Use parallelization**: Set `n_jobs=-1` to utilize all CPU cores\r\n2. **Batch processing**: Process multiple molecules at once instead of loops\r\n3. **Choose appropriate featurizers**: Fingerprints are faster than deep learning models\r\n4. **Cache pretrained models**: Leverage built-in caching for repeated use\r\n5. **Use float32**: Set `dtype=np.float32` when precision allows\r\n6. **Handle errors efficiently**: Use `ignore_errors=True` for large datasets",
    "Quick Start Workflow": "```",
    "Choosing the Right Featurizer": "FPCalculator(\"gobbi2D\")\r\n```",
    "Discovering Available Featurizers": "transformer = store.load(\"ChemBERTa-77M-MLM\")\r\n```",
    "When to Use This Skill": "This skill should be used when working with:\r\n- **Molecular machine learning**: Building QSAR/QSPR models, property prediction\r\n- **Virtual screening**: Ranking compound libraries for biological activity\r\n- **Similarity searching**: Finding structurally similar molecules\r\n- **Chemical space analysis**: Clustering, visualization, dimensionality reduction\r\n- **Deep learning**: Training neural networks on molecular data\r\n- **Featurization pipelines**: Converting SMILES to ML-ready representations\r\n- **Cheminformatics**: Any task requiring molecular feature extraction"
  }
}