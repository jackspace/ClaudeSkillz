{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/built-in-tools-guide.md",
      "references/mcp-integration-guide.md",
      "references/migration-guide.md",
      "references/reasoning-preservation.md",
      "references/responses-vs-chat-completions.md",
      "references/stateful-conversations.md",
      "references/top-errors.md"
    ]
  },
  "content": "**Status**: Production Ready\r\n**Last Updated**: 2025-10-25\r\n**API Launch**: March 2025\r\n**Dependencies**: openai@5.19.1+ (Node.js) or fetch API (Cloudflare Workers)\r\n\r\n---\r\n\r\n\r\n### 1. Get API Key\r\n\r\n```bash",
  "name": "openai-responses",
  "id": "openai-responses",
  "sections": {
    "Built-in Tools (Server-Side)": "The Responses API includes **server-side hosted tools** that eliminate costly backend round trips.\r\n\r\n### Available Tools\r\n\r\n| Tool | Purpose | Use Case |\r\n|------|---------|----------|\r\n| **Code Interpreter** | Execute Python code | Data analysis, calculations, charts |\r\n| **File Search** | RAG without vector stores | Search uploaded files for answers |\r\n| **Web Search** | Real-time web information | Current events, fact-checking |\r\n| **Image Generation** | DALL-E integration | Create images from descriptions |\r\n| **MCP** | Connect external tools | Stripe, databases, custom APIs |\r\n\r\n### Code Interpreter\r\n\r\nExecute Python code server-side for data analysis, calculations, and visualizations.\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Calculate the mean, median, and mode of: 10, 20, 30, 40, 50',\r\n  tools: [{ type: 'code_interpreter' }],\r\n});\r\n\r\nconsole.log(response.output_text);\r\n// Model writes and executes Python code, returns results\r\n```\r\n\r\n**Advanced Example: Data Analysis**\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Analyze this sales data and create a bar chart showing monthly revenue: [data here]',\r\n  tools: [{ type: 'code_interpreter' }],\r\n});\r\n\r\n// Check output for code execution results\r\nresponse.output.forEach(item => {\r\n  if (item.type === 'code_interpreter_call') {\r\n    console.log('Code executed:', item.input);\r\n    console.log('Result:', item.output);\r\n  }\r\n});\r\n```\r\n\r\n**Why this matters:**\r\n- No need to run Python locally\r\n- Sandboxed execution environment\r\n- Automatic chart generation\r\n- Can process uploaded files\r\n\r\n### File Search (RAG Without Vector Stores)\r\n\r\nSearch through uploaded files without building your own RAG pipeline.\r\n\r\n```typescript\r\n// 1. Upload files first (one-time setup)\r\nconst file = await openai.files.create({\r\n  file: fs.createReadStream('knowledge-base.pdf'),\r\n  purpose: 'assistants',\r\n});\r\n\r\n// 2. Use file search\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'What does the document say about pricing?',\r\n  tools: [\r\n    {\r\n      type: 'file_search',\r\n      file_ids: [file.id],\r\n    },\r\n  ],\r\n});\r\n\r\nconsole.log(response.output_text);\r\n// Model searches file and provides answer with citations\r\n```\r\n\r\n**Supported File Types:**\r\n- PDFs, Word docs, text files\r\n- Markdown, HTML\r\n- Code files (Python, JavaScript, etc.)\r\n- Max: 512MB per file\r\n\r\n### Web Search\r\n\r\nGet real-time information from the web.\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'What are the latest updates on GPT-5?',\r\n  tools: [{ type: 'web_search' }],\r\n});\r\n\r\nconsole.log(response.output_text);\r\n// Model searches web and provides current information with sources\r\n```\r\n\r\n**Why this matters:**\r\n- No cutoff date limitations\r\n- Automatic source citations\r\n- Real-time data access\r\n- No need for external search APIs\r\n\r\n### Image Generation (DALL-E)\r\n\r\nGenerate images directly in the Responses API.\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Create an image of a futuristic cityscape at sunset',\r\n  tools: [{ type: 'image_generation' }],\r\n});\r\n\r\n// Find image in output\r\nresponse.output.forEach(item => {\r\n  if (item.type === 'image_generation_call') {\r\n    console.log('Image URL:', item.output.url);\r\n  }\r\n});\r\n```\r\n\r\n**Models Available:**\r\n- DALL-E 3 (default)\r\n- Various sizes and quality options\r\n\r\n---",
    "Quick Start (5 Minutes)": "export OPENAI_API_KEY=\"sk-proj-...\"\r\n```\r\n\r\n**Why this matters:**\r\n- API key required for all requests\r\n- Keep secure (never commit to git)\r\n- Use environment variables\r\n\r\n### 2. Install SDK (Node.js)\r\n\r\n```bash\r\nnpm install openai\r\n```\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI({\r\n  apiKey: process.env.OPENAI_API_KEY,\r\n});\r\n\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'What are the 5 Ds of dodgeball?',\r\n});\r\n\r\nconsole.log(response.output_text);\r\n```\r\n\r\n**CRITICAL:**\r\n- Always use server-side (never expose API key in client code)\r\n- Model defaults to `gpt-5` (can use `gpt-5-mini`, `gpt-4o`, etc.)\r\n- `input` can be string or array of messages\r\n\r\n### 3. Or Use Direct API (Cloudflare Workers)\r\n\r\n```typescript\r\n// No SDK needed - use fetch()\r\nconst response = await fetch('https://api.openai.com/v1/responses', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'gpt-5',\r\n    input: 'Hello, world!',\r\n  }),\r\n});\r\n\r\nconst data = await response.json();\r\nconsole.log(data.output_text);\r\n```\r\n\r\n**Why fetch?**\r\n- No dependencies in edge environments\r\n- Full control over request/response\r\n- Works in Cloudflare Workers, Deno, Bun\r\n\r\n---",
    "Node.js vs Cloudflare Workers": "### Node.js Implementation\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI({\r\n  apiKey: process.env.OPENAI_API_KEY,\r\n});\r\n\r\nexport async function handleRequest(input: string) {\r\n  const response = await openai.responses.create({\r\n    model: 'gpt-5',\r\n    input,\r\n    tools: [{ type: 'web_search' }],\r\n  });\r\n\r\n  return response.output_text;\r\n}\r\n```\r\n\r\n**Pros:**\r\n- Full SDK support\r\n- Type safety\r\n- Streaming helpers\r\n\r\n**Cons:**\r\n- Requires Node.js runtime\r\n- Larger bundle size\r\n\r\n### Cloudflare Workers Implementation\r\n\r\n```typescript\r\nexport default {\r\n  async fetch(request: Request, env: Env): Promise<Response> {\r\n    const { input } = await request.json();\r\n\r\n    const response = await fetch('https://api.openai.com/v1/responses', {\r\n      method: 'POST',\r\n      headers: {\r\n        'Authorization': `Bearer ${env.OPENAI_API_KEY}`,\r\n        'Content-Type': 'application/json',\r\n      },\r\n      body: JSON.stringify({\r\n        model: 'gpt-5',\r\n        input,\r\n        tools: [{ type: 'web_search' }],\r\n      }),\r\n    });\r\n\r\n    const data = await response.json();\r\n\r\n    return new Response(data.output_text, {\r\n      headers: { 'Content-Type': 'text/plain' },\r\n    });\r\n  },\r\n};\r\n```\r\n\r\n**Pros:**\r\n- No dependencies\r\n- Edge deployment\r\n- Faster cold starts\r\n\r\n**Cons:**\r\n- Manual request building\r\n- No type safety without custom types\r\n\r\n---",
    "Stateful Conversations": "### Automatic State Management\r\n\r\nThe Responses API can automatically manage conversation state using **conversation IDs**.\r\n\r\n#### Creating a Conversation\r\n\r\n```typescript\r\n// Create conversation with initial message\r\nconst conversation = await openai.conversations.create({\r\n  metadata: { user_id: 'user_123' },\r\n  items: [\r\n    {\r\n      type: 'message',\r\n      role: 'user',\r\n      content: 'Hello!',\r\n    },\r\n  ],\r\n});\r\n\r\nconsole.log(conversation.id); // \"conv_abc123...\"\r\n```\r\n\r\n#### Using Conversation ID\r\n\r\n```typescript\r\n// First turn\r\nconst response1 = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  conversation: 'conv_abc123',\r\n  input: 'What are the 5 Ds of dodgeball?',\r\n});\r\n\r\nconsole.log(response1.output_text);\r\n\r\n// Second turn - model remembers previous context\r\nconst response2 = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  conversation: 'conv_abc123',\r\n  input: 'Tell me more about the first one',\r\n});\r\n\r\nconsole.log(response2.output_text);\r\n// Model automatically knows \"first one\" refers to first D from previous turn\r\n```\r\n\r\n**Why this matters:**\r\n- No manual history tracking required\r\n- Reasoning state preserved between turns\r\n- Automatic context management\r\n- Lower risk of context errors\r\n\r\n### Manual State Management (Alternative)\r\n\r\nIf you need full control, you can manually manage history:\r\n\r\n```typescript\r\nlet history = [\r\n  { role: 'user', content: 'Tell me a joke' },\r\n];\r\n\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: history,\r\n  store: true, // Optional: store for retrieval later\r\n});\r\n\r\n// Add response to history\r\nhistory = [\r\n  ...history,\r\n  ...response.output.map(el => ({\r\n    role: el.role,\r\n    content: el.content,\r\n  })),\r\n];\r\n\r\n// Next turn\r\nhistory.push({ role: 'user', content: 'Tell me another' });\r\n\r\nconst secondResponse = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: history,\r\n});\r\n```\r\n\r\n**When to use manual management:**\r\n- Need custom history pruning logic\r\n- Want to modify conversation history programmatically\r\n- Implementing custom caching strategies\r\n\r\n---",
    "Polymorphic Outputs": "The Responses API returns **multiple output types** instead of a single message.\r\n\r\n### Output Types\r\n\r\n| Type | Description | Example |\r\n|------|-------------|---------|\r\n| `message` | Text response to user | Final answer, explanation |\r\n| `reasoning` | Model's internal thought process | Step-by-step reasoning summary |\r\n| `code_interpreter_call` | Code execution | Python code + results |\r\n| `mcp_call` | Tool invocation | Tool name, args, output |\r\n| `mcp_list_tools` | Available tools | Tool definitions from MCP server |\r\n| `file_search_call` | File search results | Matched chunks, citations |\r\n| `web_search_call` | Web search results | URLs, snippets |\r\n| `image_generation_call` | Image generation | Image URL |\r\n\r\n### Processing Polymorphic Outputs\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Search the web for the latest AI news and summarize',\r\n  tools: [{ type: 'web_search' }],\r\n});\r\n\r\n// Process different output types\r\nresponse.output.forEach(item => {\r\n  switch (item.type) {\r\n    case 'reasoning':\r\n      console.log('Reasoning:', item.summary[0].text);\r\n      break;\r\n    case 'web_search_call':\r\n      console.log('Searched:', item.query);\r\n      console.log('Sources:', item.results);\r\n      break;\r\n    case 'message':\r\n      console.log('Response:', item.content[0].text);\r\n      break;\r\n  }\r\n});\r\n\r\n// Or use helper for text-only\r\nconsole.log(response.output_text);\r\n```\r\n\r\n**Why This Matters:**\r\n- Better debugging (see all steps)\r\n- Audit trails (track all tool calls)\r\n- Richer UX (show progress to users)\r\n- Compliance (log all actions)\r\n\r\n---",
    "References": "### Official Documentation\r\n- **Responses API Guide**: https://platform.openai.com/docs/guides/responses\r\n- **API Reference**: https://platform.openai.com/docs/api-reference/responses\r\n- **MCP Integration**: https://platform.openai.com/docs/guides/tools-connectors-mcp\r\n- **Blog Post (Why Responses API)**: https://developers.openai.com/blog/responses-api/\r\n- **Starter App**: https://github.com/openai/openai-responses-starter-app\r\n\r\n### Skill Resources\r\n- `templates/` - Working code examples\r\n- `references/responses-vs-chat-completions.md` - Feature comparison\r\n- `references/mcp-integration-guide.md` - MCP server setup\r\n- `references/built-in-tools-guide.md` - Tool usage patterns\r\n- `references/stateful-conversations.md` - Conversation management\r\n- `references/migration-guide.md` - Chat Completions â†’ Responses\r\n- `references/top-errors.md` - Common errors and solutions\r\n\r\n---",
    "Error Handling": "### Common Errors and Solutions\r\n\r\n#### 1. Session State Not Persisting\r\n\r\n**Error:**\r\n```\r\nConversation state not maintained between turns\r\n```\r\n\r\n**Cause:**\r\n- Not using conversation IDs\r\n- Using different conversation IDs per turn\r\n\r\n**Solution:**\r\n```typescript\r\n// Create conversation once\r\nconst conv = await openai.conversations.create();\r\n\r\n// Reuse conversation ID for all turns\r\nconst response1 = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  conversation: conv.id, // âœ… Same ID\r\n  input: 'First message',\r\n});\r\n\r\nconst response2 = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  conversation: conv.id, // âœ… Same ID\r\n  input: 'Follow-up message',\r\n});\r\n```\r\n\r\n#### 2. MCP Server Connection Failed\r\n\r\n**Error:**\r\n```json\r\n{\r\n  \"error\": {\r\n    \"type\": \"mcp_connection_error\",\r\n    \"message\": \"Failed to connect to MCP server\"\r\n  }\r\n}\r\n```\r\n\r\n**Causes:**\r\n- Invalid server URL\r\n- Missing or expired authorization token\r\n- Server not responding\r\n\r\n**Solutions:**\r\n```typescript\r\n// 1. Verify URL is correct\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Test MCP',\r\n  tools: [\r\n    {\r\n      type: 'mcp',\r\n      server_label: 'test',\r\n      server_url: 'https://api.example.com/mcp', // âœ… Full URL\r\n      authorization: process.env.AUTH_TOKEN, // âœ… Valid token\r\n    },\r\n  ],\r\n});\r\n\r\n// 2. Test server URL manually\r\nconst testResponse = await fetch('https://api.example.com/mcp');\r\nconsole.log(testResponse.status); // Should be 200\r\n\r\n// 3. Check token expiration\r\nconsole.log('Token expires:', parseJWT(token).exp);\r\n```\r\n\r\n#### 3. Code Interpreter Timeout\r\n\r\n**Error:**\r\n```json\r\n{\r\n  \"error\": {\r\n    \"type\": \"code_interpreter_timeout\",\r\n    \"message\": \"Code execution exceeded time limit\"\r\n  }\r\n}\r\n```\r\n\r\n**Cause:**\r\n- Code runs longer than 30 seconds\r\n\r\n**Solution:**\r\n```typescript\r\n// Use background mode for long-running code\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Process this large dataset',\r\n  background: true, // âœ… Extended timeout\r\n  tools: [{ type: 'code_interpreter' }],\r\n});\r\n\r\n// Poll for results\r\nconst result = await openai.responses.retrieve(response.id);\r\n```\r\n\r\n#### 4. Image Generation Rate Limit\r\n\r\n**Error:**\r\n```json\r\n{\r\n  \"error\": {\r\n    \"type\": \"rate_limit_error\",\r\n    \"message\": \"DALL-E rate limit exceeded\"\r\n  }\r\n}\r\n```\r\n\r\n**Cause:**\r\n- Too many image generation requests\r\n\r\n**Solution:**\r\n```typescript\r\n// Implement retry with exponential backoff\r\nconst generateImage = async (prompt, retries = 3) => {\r\n  try {\r\n    return await openai.responses.create({\r\n      model: 'gpt-5',\r\n      input: prompt,\r\n      tools: [{ type: 'image_generation' }],\r\n    });\r\n  } catch (error) {\r\n    if (error.type === 'rate_limit_error' && retries > 0) {\r\n      const delay = (4 - retries) * 1000; // 1s, 2s, 3s\r\n      await new Promise(resolve => setTimeout(resolve, delay));\r\n      return generateImage(prompt, retries - 1);\r\n    }\r\n    throw error;\r\n  }\r\n};\r\n```\r\n\r\n#### 5. File Search Relevance Issues\r\n\r\n**Problem:**\r\n- File search returns irrelevant results\r\n\r\n**Solution:**\r\n```typescript\r\n// Use more specific queries\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Find sections about pricing in Q4 2024 specifically', // âœ… Specific\r\n  // NOT: 'Find pricing' (too vague)\r\n  tools: [{ type: 'file_search', file_ids: [fileId] }],\r\n});\r\n\r\n// Or filter results manually\r\nresponse.output.forEach(item => {\r\n  if (item.type === 'file_search_call') {\r\n    const relevantChunks = item.results.filter(\r\n      chunk => chunk.score > 0.7 // âœ… Only high-confidence matches\r\n    );\r\n  }\r\n});\r\n```\r\n\r\n#### 6. Cost Tracking Confusion\r\n\r\n**Problem:**\r\n- Billing different than expected\r\n\r\n**Explanation:**\r\n- Responses API bills for: input tokens + output tokens + tool usage + stored conversations\r\n- Chat Completions bills only: input tokens + output tokens\r\n\r\n**Solution:**\r\n```typescript\r\n// Monitor usage\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Hello',\r\n  store: false, // âœ… Don't store if not needed\r\n});\r\n\r\nconsole.log('Usage:', response.usage);\r\n// {\r\n//   prompt_tokens: 10,\r\n//   completion_tokens: 20,\r\n//   tool_tokens: 5,\r\n//   total_tokens: 35\r\n// }\r\n```\r\n\r\n#### 7. Conversation Not Found\r\n\r\n**Error:**\r\n```json\r\n{\r\n  \"error\": {\r\n    \"type\": \"invalid_request_error\",\r\n    \"message\": \"Conversation conv_xyz not found\"\r\n  }\r\n}\r\n```\r\n\r\n**Causes:**\r\n- Conversation ID typo\r\n- Conversation deleted\r\n- Conversation expired (90 days)\r\n\r\n**Solution:**\r\n```typescript\r\n// Verify conversation exists before using\r\nconst conversations = await openai.conversations.list();\r\nconst exists = conversations.data.some(c => c.id === 'conv_xyz');\r\n\r\nif (!exists) {\r\n  // Create new conversation\r\n  const newConv = await openai.conversations.create();\r\n  // Use newConv.id\r\n}\r\n```\r\n\r\n#### 8. Tool Output Parsing Failed\r\n\r\n**Problem:**\r\n- Can't access tool outputs correctly\r\n\r\n**Solution:**\r\n```typescript\r\n// Use helper methods\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Search for AI news',\r\n  tools: [{ type: 'web_search' }],\r\n});\r\n\r\n// Helper: Get text-only output\r\nconsole.log(response.output_text);\r\n\r\n// Manual: Inspect all outputs\r\nresponse.output.forEach(item => {\r\n  console.log('Type:', item.type);\r\n  console.log('Content:', item);\r\n});\r\n```\r\n\r\n---",
    "Reasoning Preservation": "### How It Works\r\n\r\nThe Responses API preserves the model's **internal reasoning state** across turns, unlike Chat Completions which discards it.\r\n\r\n**Visual Analogy:**\r\n- **Chat Completions**: Model has a scratchpad, writes reasoning, then **tears out the page** before responding\r\n- **Responses API**: Model keeps the scratchpad open, **previous reasoning visible** for next turn\r\n\r\n### Performance Impact\r\n\r\n**TAUBench Results (GPT-5):**\r\n- Chat Completions: Baseline score\r\n- Responses API: **+5% better** (purely from preserved reasoning)\r\n\r\n**Why This Matters:**\r\n- Better multi-turn problem solving\r\n- More coherent long conversations\r\n- Improved step-by-step reasoning\r\n- Fewer context errors\r\n\r\n### Reasoning Summaries (Free!)\r\n\r\nThe Responses API provides **reasoning summaries** at no additional cost.\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Solve this complex math problem: [problem]',\r\n});\r\n\r\n// Inspect reasoning\r\nresponse.output.forEach(item => {\r\n  if (item.type === 'reasoning') {\r\n    console.log('Model reasoning:', item.summary[0].text);\r\n  }\r\n  if (item.type === 'message') {\r\n    console.log('Final answer:', item.content[0].text);\r\n  }\r\n});\r\n```\r\n\r\n**Use Cases:**\r\n- Debugging model decisions\r\n- Audit trails for compliance\r\n- Understanding model thought process\r\n- Building transparent AI systems\r\n\r\n---",
    "Responses vs Chat Completions: Complete Comparison": "### When to Use Each\r\n\r\n**Use Responses API when:**\r\n- âœ… Building agentic applications (reasoning + actions)\r\n- âœ… Need preserved reasoning state across turns\r\n- âœ… Want built-in tools (Code Interpreter, File Search, Web Search)\r\n- âœ… Using MCP servers for external integrations\r\n- âœ… Implementing conversational AI with automatic state management\r\n- âœ… Background processing for long-running tasks\r\n- âœ… Need polymorphic outputs (messages, reasoning, tool calls)\r\n\r\n**Use Chat Completions when:**\r\n- âœ… Simple one-off text generation\r\n- âœ… Fully stateless interactions (no conversation continuity needed)\r\n- âœ… Legacy integrations (existing Chat Completions code)\r\n- âœ… Very simple use cases without tools\r\n\r\n### Architecture Differences\r\n\r\n**Chat Completions Flow:**\r\n```\r\nUser Input â†’ Model â†’ Single Message â†’ Done\r\n(Reasoning discarded, state lost)\r\n```\r\n\r\n**Responses API Flow:**\r\n```\r\nUser Input â†’ Model (preserved reasoning) â†’ Polymorphic Outputs\r\n            â†“ (server-side tools)\r\n    Tool Call â†’ Tool Result â†’ Model â†’ Final Response\r\n(Reasoning preserved, state maintained)\r\n```\r\n\r\n### Performance Benefits\r\n\r\n**Cache Utilization:**\r\n- Chat Completions: Baseline performance\r\n- Responses API: **40-80% better cache utilization**\r\n- Result: Lower latency + reduced costs\r\n\r\n**Reasoning Performance:**\r\n- Chat Completions: Reasoning dropped between turns\r\n- Responses API: Reasoning preserved across turns\r\n- Result: **5% better on TAUBench** (GPT-5 with Responses vs Chat Completions)\r\n\r\n---",
    "Background Mode (Long-Running Tasks)": "For tasks that take longer than standard timeout limits, use **background mode**.\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Analyze this 500-page document and summarize key findings',\r\n  background: true,\r\n  tools: [{ type: 'file_search', file_ids: [fileId] }],\r\n});\r\n\r\n// Returns immediately with status\r\nconsole.log(response.status); // \"in_progress\"\r\nconsole.log(response.id); // Use to check status later\r\n\r\n// Poll for completion\r\nconst checkStatus = async (responseId) => {\r\n  const result = await openai.responses.retrieve(responseId);\r\n  if (result.status === 'completed') {\r\n    console.log(result.output_text);\r\n  } else if (result.status === 'failed') {\r\n    console.error('Task failed:', result.error);\r\n  } else {\r\n    // Still running, check again later\r\n    setTimeout(() => checkStatus(responseId), 5000);\r\n  }\r\n};\r\n\r\ncheckStatus(response.id);\r\n```\r\n\r\n**When to Use:**\r\n- Large file processing\r\n- Complex calculations\r\n- Multi-step research tasks\r\n- Data analysis on large datasets\r\n\r\n**Timeout Limits:**\r\n- Standard mode: 60 seconds\r\n- Background mode: Up to 10 minutes\r\n\r\n---",
    "MCP Server Integration": "The Responses API has built-in support for **Model Context Protocol (MCP)** servers, allowing you to connect external tools.\r\n\r\n### What Is MCP?\r\n\r\nMCP is an open protocol that standardizes how applications provide context to LLMs. It allows you to:\r\n- Connect to external APIs (Stripe, databases, CRMs)\r\n- Use hosted MCP servers\r\n- Build custom tool integrations\r\n\r\n### Basic MCP Integration\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Roll 2d6 dice',\r\n  tools: [\r\n    {\r\n      type: 'mcp',\r\n      server_label: 'dice',\r\n      server_url: 'https://example.com/mcp',\r\n    },\r\n  ],\r\n});\r\n\r\n// Model discovers available tools on MCP server and uses them\r\nconsole.log(response.output_text);\r\n```\r\n\r\n### MCP with Authentication (OAuth)\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Create a $20 payment link',\r\n  tools: [\r\n    {\r\n      type: 'mcp',\r\n      server_label: 'stripe',\r\n      server_url: 'https://mcp.stripe.com',\r\n      authorization: process.env.STRIPE_OAUTH_TOKEN,\r\n    },\r\n  ],\r\n});\r\n\r\nconsole.log(response.output_text);\r\n// Model uses Stripe MCP server to create payment link\r\n```\r\n\r\n**CRITICAL:**\r\n- API does NOT store authorization tokens\r\n- Must provide token with each request\r\n- Use environment variables for security\r\n\r\n### Polymorphic Output: MCP Tool Calls\r\n\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Roll 2d4+1',\r\n  tools: [\r\n    {\r\n      type: 'mcp',\r\n      server_label: 'dice',\r\n      server_url: 'https://dmcp.example.com',\r\n    },\r\n  ],\r\n});\r\n\r\n// Inspect tool calls\r\nresponse.output.forEach(item => {\r\n  if (item.type === 'mcp_call') {\r\n    console.log('Tool:', item.name);\r\n    console.log('Arguments:', item.arguments);\r\n    console.log('Output:', item.output);\r\n  }\r\n  if (item.type === 'mcp_list_tools') {\r\n    console.log('Available tools:', item.tools);\r\n  }\r\n});\r\n```\r\n\r\n**Output Types:**\r\n- `mcp_list_tools` - Tools discovered on server\r\n- `mcp_call` - Tool invocation and result\r\n- `message` - Final response to user\r\n\r\n---",
    "Always Do / Never Do": "### âœ… Always Do\r\n\r\n1. **Use conversation IDs for multi-turn interactions**\r\n   ```typescript\r\n   const conv = await openai.conversations.create();\r\n   // Reuse conv.id for all related turns\r\n   ```\r\n\r\n2. **Handle all output types in polymorphic responses**\r\n   ```typescript\r\n   response.output.forEach(item => {\r\n     if (item.type === 'reasoning') { /* log */ }\r\n     if (item.type === 'message') { /* display */ }\r\n   });\r\n   ```\r\n\r\n3. **Use background mode for long-running tasks**\r\n   ```typescript\r\n   const response = await openai.responses.create({\r\n     background: true, // âœ… For tasks >30s\r\n     ...\r\n   });\r\n   ```\r\n\r\n4. **Provide authorization tokens for MCP servers**\r\n   ```typescript\r\n   tools: [{\r\n     type: 'mcp',\r\n     authorization: process.env.TOKEN, // âœ… Required\r\n   }]\r\n   ```\r\n\r\n5. **Monitor token usage for cost control**\r\n   ```typescript\r\n   console.log(response.usage.total_tokens);\r\n   ```\r\n\r\n### âŒ Never Do\r\n\r\n1. **Never expose API keys in client-side code**\r\n   ```typescript\r\n   // âŒ DANGER: API key in browser\r\n   const response = await fetch('https://api.openai.com/v1/responses', {\r\n     headers: { 'Authorization': 'Bearer sk-proj-...' }\r\n   });\r\n   ```\r\n\r\n2. **Never assume single message output**\r\n   ```typescript\r\n   // âŒ BAD: Ignores reasoning, tool calls\r\n   console.log(response.output[0].content);\r\n\r\n   // âœ… GOOD: Use helper or check all types\r\n   console.log(response.output_text);\r\n   ```\r\n\r\n3. **Never reuse conversation IDs across users**\r\n   ```typescript\r\n   // âŒ DANGER: User A sees User B's conversation\r\n   const sharedConv = 'conv_123';\r\n   ```\r\n\r\n4. **Never ignore error types**\r\n   ```typescript\r\n   // âŒ BAD: Generic error handling\r\n   try { ... } catch (e) { console.log('error'); }\r\n\r\n   // âœ… GOOD: Type-specific handling\r\n   catch (e) {\r\n     if (e.type === 'rate_limit_error') { /* retry */ }\r\n     if (e.type === 'mcp_connection_error') { /* alert */ }\r\n   }\r\n   ```\r\n\r\n5. **Never poll faster than 1 second for background tasks**\r\n   ```typescript\r\n   // âŒ BAD: Too frequent\r\n   setInterval(() => checkStatus(), 100);\r\n\r\n   // âœ… GOOD: Reasonable interval\r\n   setInterval(() => checkStatus(), 5000);\r\n   ```\r\n\r\n---",
    "What Is the Responses API?": "The Responses API (`/v1/responses`) is OpenAI's unified interface for building agentic applications, launched in March 2025. It fundamentally changes how you interact with OpenAI models by providing **stateful conversations** and a **structured loop for reasoning and acting**.\r\n\r\n### Key Innovation: Preserved Reasoning State\r\n\r\nUnlike Chat Completions where reasoning is discarded between turns, Responses **keeps the notebook open**. The model's step-by-step thought processes survive into the next turn, improving performance by approximately **5% on TAUBench** and enabling better multi-turn interactions.\r\n\r\n### Why Use Responses Over Chat Completions?\r\n\r\n| Feature | Chat Completions | Responses API | Benefit |\r\n|---------|-----------------|---------------|---------|\r\n| **State Management** | Manual (you track history) | Automatic (conversation IDs) | Simpler code, less error-prone |\r\n| **Reasoning** | Dropped between turns | Preserved across turns | Better multi-turn performance |\r\n| **Tools** | Client-side round trips | Server-side hosted | Lower latency, simpler code |\r\n| **Output Format** | Single message | Polymorphic (messages, reasoning, tool calls) | Richer debugging, better UX |\r\n| **Cache Utilization** | Baseline | 40-80% better | Lower costs, faster responses |\r\n| **MCP Support** | Manual integration | Built-in | Easy external tool connections |\r\n\r\n---",
    "Next Steps": "1. âœ… Read `templates/basic-response.ts` - Simple example\r\n2. âœ… Try `templates/stateful-conversation.ts` - Multi-turn chat\r\n3. âœ… Explore `templates/mcp-integration.ts` - External tools\r\n4. âœ… Review `references/top-errors.md` - Avoid common pitfalls\r\n5. âœ… Check `references/migration-guide.md` - If migrating from Chat Completions\r\n\r\n**Happy building with the Responses API!** ðŸš€",
    "Migration from Chat Completions": "### Breaking Changes\r\n\r\n| Feature | Chat Completions | Responses API | Migration |\r\n|---------|-----------------|---------------|-----------|\r\n| **Endpoint** | `/v1/chat/completions` | `/v1/responses` | Update URL |\r\n| **Parameter** | `messages` | `input` | Rename parameter |\r\n| **State** | Manual (`messages` array) | Automatic (`conversation` ID) | Use conversation IDs |\r\n| **Tools** | `tools` array with functions | Built-in types + MCP | Update tool definitions |\r\n| **Output** | `choices[0].message.content` | `output_text` or `output` array | Update response parsing |\r\n| **Streaming** | `data: {\"choices\":[...]}` | SSE with multiple item types | Update stream parser |\r\n\r\n### Migration Example\r\n\r\n**Before (Chat Completions):**\r\n```typescript\r\nconst response = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [\r\n    { role: 'system', content: 'You are a helpful assistant.' },\r\n    { role: 'user', content: 'Hello!' },\r\n  ],\r\n});\r\n\r\nconsole.log(response.choices[0].message.content);\r\n```\r\n\r\n**After (Responses):**\r\n```typescript\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: [\r\n    { role: 'developer', content: 'You are a helpful assistant.' },\r\n    { role: 'user', content: 'Hello!' },\r\n  ],\r\n});\r\n\r\nconsole.log(response.output_text);\r\n```\r\n\r\n**Key Differences:**\r\n1. `chat.completions.create` â†’ `responses.create`\r\n2. `messages` â†’ `input`\r\n3. `system` role â†’ `developer` role\r\n4. `choices[0].message.content` â†’ `output_text`\r\n\r\n### When to Migrate\r\n\r\n**Migrate now if:**\r\n- âœ… Building new applications\r\n- âœ… Need stateful conversations\r\n- âœ… Using agentic patterns (reasoning + tools)\r\n- âœ… Want better performance (preserved reasoning)\r\n\r\n**Stay on Chat Completions if:**\r\n- âœ… Simple one-off generations\r\n- âœ… Legacy integrations\r\n- âœ… No need for state management\r\n\r\n---",
    "Production Patterns": "### Cost Optimization\r\n\r\n**1. Use Conversation IDs (Cache Benefits)**\r\n```typescript\r\n// âœ… GOOD: Reuse conversation ID\r\nconst conv = await openai.conversations.create();\r\nconst response1 = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  conversation: conv.id,\r\n  input: 'Question 1',\r\n});\r\n// 40-80% better cache utilization\r\n\r\n// âŒ BAD: New manual history each time\r\nconst response2 = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: [...previousHistory, newMessage],\r\n});\r\n// No cache benefits\r\n```\r\n\r\n**2. Disable Storage When Not Needed**\r\n```typescript\r\n// For one-off requests\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5',\r\n  input: 'Quick question',\r\n  store: false, // âœ… Don't store conversation\r\n});\r\n```\r\n\r\n**3. Use Smaller Models When Possible**\r\n```typescript\r\n// For simple tasks\r\nconst response = await openai.responses.create({\r\n  model: 'gpt-5-mini', // âœ… 50% cheaper\r\n  input: 'Summarize this paragraph',\r\n});\r\n```\r\n\r\n### Rate Limit Handling\r\n\r\n```typescript\r\nconst createResponseWithRetry = async (params, maxRetries = 3) => {\r\n  for (let i = 0; i < maxRetries; i++) {\r\n    try {\r\n      return await openai.responses.create(params);\r\n    } catch (error) {\r\n      if (error.type === 'rate_limit_error' && i < maxRetries - 1) {\r\n        const delay = Math.pow(2, i) * 1000; // Exponential backoff\r\n        console.log(`Rate limited, retrying in ${delay}ms`);\r\n        await new Promise(resolve => setTimeout(resolve, delay));\r\n      } else {\r\n        throw error;\r\n      }\r\n    }\r\n  }\r\n};\r\n```\r\n\r\n### Monitoring and Logging\r\n\r\n```typescript\r\nconst monitoredResponse = async (input) => {\r\n  const startTime = Date.now();\r\n\r\n  try {\r\n    const response = await openai.responses.create({\r\n      model: 'gpt-5',\r\n      input,\r\n    });\r\n\r\n    // Log success metrics\r\n    console.log({\r\n      status: 'success',\r\n      latency: Date.now() - startTime,\r\n      tokens: response.usage.total_tokens,\r\n      model: response.model,\r\n      conversation: response.conversation_id,\r\n    });\r\n\r\n    return response;\r\n  } catch (error) {\r\n    // Log error metrics\r\n    console.error({\r\n      status: 'error',\r\n      latency: Date.now() - startTime,\r\n      error: error.message,\r\n      type: error.type,\r\n    });\r\n    throw error;\r\n  }\r\n};\r\n```\r\n\r\n---"
  }
}