{
  "description": "\"UMAP dimensionality reduction. Fast nonlinear manifold learning for 2D/3D visualization, clustering preprocessing (HDBSCAN), supervised/parametric UMAP, for high-dimensional data.\"",
  "references": {
    "files": [
      "references/api_reference.md"
    ]
  },
  "content": "### Installation\r\n\r\n```bash\r\nconda install -c conda-forge umap-learn\r\n\r\npip install umap-learn\r\n```\r\n\r\n### Basic Usage\r\n\r\nUMAP follows scikit-learn conventions and can be used as a drop-in replacement for t-SNE or PCA.\r\n\r\n```python\r\nimport umap\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nscaled_data = StandardScaler().fit_transform(data)\r\n\r\nembedding = umap.UMAP().fit_transform(scaled_data)\r\n\r\nreducer = umap.UMAP(random_state=42)\r\nreducer.fit(scaled_data)\r\nembedding = reducer.embedding_  # Access the trained embedding\r\n```\r\n\r\n**Critical preprocessing requirement:** Always standardize features to comparable scales before applying UMAP to ensure equal weighting across dimensions.\r\n\r\n### Typical Workflow\r\n\r\n```python\r\nimport umap\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nscaler = StandardScaler()\r\nscaled_data = scaler.fit_transform(raw_data)\r\n\r\nreducer = umap.UMAP(\r\n    n_neighbors=15,\r\n    min_dist=0.1,\r\n    n_components=2,\r\n    metric='euclidean',\r\n    random_state=42\r\n)\r\nembedding = reducer.fit_transform(scaled_data)\r\n\r\n\r\nUMAP has four primary parameters that control the embedding behavior. Understanding these is crucial for effective usage.\r\n\r\n### n_neighbors (default: 15)\r\n\r\n**Purpose:** Balances local versus global structure in the embedding.\r\n\r\n**How it works:** Controls the size of the local neighborhood UMAP examines when learning manifold structure.\r\n\r\n**Effects by value:**\r\n- **Low values (2-5):** Emphasizes fine local detail but may fragment data into disconnected components\r\n- **Medium values (15-20):** Balanced view of both local structure and global relationships (recommended starting point)\r\n- **High values (50-200):** Prioritizes broad topological structure at the expense of fine-grained details\r\n\r\n**Recommendation:** Start with 15 and adjust based on results. Increase for more global structure, decrease for more local detail.\r\n\r\n### min_dist (default: 0.1)\r\n\r\n**Purpose:** Controls how tightly points cluster in the low-dimensional space.\r\n\r\n**How it works:** Sets the minimum distance apart that points are allowed to be in the output representation.\r\n\r\n**Effects by value:**\r\n- **Low values (0.0-0.1):** Creates clumped embeddings useful for clustering; reveals fine topological details\r\n- **High values (0.5-0.99):** Prevents tight packing; emphasizes broad topological preservation over local structure\r\n\r\n**Recommendation:** Use 0.0 for clustering applications, 0.1-0.3 for visualization, 0.5+ for loose structure.\r\n\r\n### n_components (default: 2)\r\n\r\n**Purpose:** Determines the dimensionality of the embedded output space.\r\n\r\n**Key feature:** Unlike t-SNE, UMAP scales well in the embedding dimension, enabling use beyond visualization.\r\n\r\n**Common uses:**\r\n- **2-3 dimensions:** Visualization\r\n- **5-10 dimensions:** Clustering preprocessing (better preserves density than 2D)\r\n- **10-50 dimensions:** Feature engineering for downstream ML models\r\n\r\n**Recommendation:** Use 2 for visualization, 5-10 for clustering, higher for ML pipelines.\r\n\r\n### metric (default: 'euclidean')\r\n\r\n**Purpose:** Specifies how distance is calculated between input data points.\r\n\r\n**Supported metrics:**\r\n- **Minkowski variants:** euclidean, manhattan, chebyshev\r\n- **Spatial metrics:** canberra, braycurtis, haversine\r\n- **Correlation metrics:** cosine, correlation (good for text/document embeddings)\r\n- **Binary data metrics:** hamming, jaccard, dice, russellrao, kulsinski, rogerstanimoto, sokalmichener, sokalsneath, yule\r\n- **Custom metrics:** User-defined distance functions via Numba\r\n\r\n**Recommendation:** Use euclidean for numeric data, cosine for text/document vectors, hamming for binary data.\r\n\r\n### Parameter Tuning Example\r\n\r\n```python\r\numap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean')\r\n\r\numap.UMAP(n_neighbors=30, min_dist=0.0, n_components=10, metric='euclidean')\r\n\r\numap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine')\r\n\r\n\r\nUMAP supports incorporating label information to guide the embedding process, enabling class separation while preserving internal structure.\r\n\r\n### Supervised UMAP\r\n\r\nPass target labels via the `y` parameter when fitting:\r\n\r\n```python\r\nembedding = umap.UMAP().fit_transform(data, y=labels)\r\n```\r\n\r\n**Key benefits:**\r\n- Achieves cleanly separated classes\r\n- Preserves internal structure within each class\r\n- Maintains global relationships between classes\r\n\r\n**When to use:** When you have labeled data and want to separate known classes while keeping meaningful point embeddings.\r\n\r\n### Semi-Supervised UMAP\r\n\r\nFor partial labels, mark unlabeled points with `-1` following scikit-learn convention:\r\n\r\n```python\r\nsemi_labels = labels.copy()\r\nsemi_labels[unlabeled_indices] = -1\r\n\r\nembedding = umap.UMAP().fit_transform(data, y=semi_labels)\r\n```\r\n\r\n**When to use:** When labeling is expensive or you have more data than labels available.\r\n\r\n### Metric Learning with UMAP\r\n\r\nTrain a supervised embedding on labeled data, then apply to new unlabeled data:\r\n\r\n```python\r\nmapper = umap.UMAP().fit(train_data, train_labels)\r\n\r\ntest_embedding = mapper.transform(test_data)\r\n\r\n\r\nUMAP serves as effective preprocessing for density-based clustering algorithms like HDBSCAN, overcoming the curse of dimensionality.\r\n\r\n### Best Practices for Clustering\r\n\r\n**Key principle:** Configure UMAP differently for clustering than for visualization.\r\n\r\n**Recommended parameters:**\r\n- **n_neighbors:** Increase to ~30 (default 15 is too local and can create artificial fine-grained clusters)\r\n- **min_dist:** Set to 0.0 (pack points densely within clusters for clearer boundaries)\r\n- **n_components:** Use 5-10 dimensions (maintains performance while improving density preservation vs. 2D)\r\n\r\n### Clustering Workflow\r\n\r\n```python\r\nimport umap\r\nimport hdbscan\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nscaled_data = StandardScaler().fit_transform(data)\r\n\r\nreducer = umap.UMAP(\r\n    n_neighbors=30,\r\n    min_dist=0.0,\r\n    n_components=10,  # Higher than 2 for better density preservation\r\n    metric='euclidean',\r\n    random_state=42\r\n)\r\nembedding = reducer.fit_transform(scaled_data)\r\n\r\nclusterer = hdbscan.HDBSCAN(\r\n    min_cluster_size=15,\r\n    min_samples=5,\r\n    metric='euclidean'\r\n)\r\nlabels = clusterer.fit_predict(embedding)\r\n\r\nfrom sklearn.metrics import adjusted_rand_score\r\nscore = adjusted_rand_score(true_labels, labels)\r\nprint(f\"Adjusted Rand Score: {score:.3f}\")\r\nprint(f\"Number of clusters: {len(set(labels)) - (1 if -1 in labels else 0)}\")\r\nprint(f\"Noise points: {sum(labels == -1)}\")\r\n```\r\n\r\n### Visualization After Clustering\r\n\r\n```python\r\nvis_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\r\nvis_embedding = vis_reducer.fit_transform(scaled_data)\r\n\r\n\r\nUMAP enables preprocessing of new data through its `transform()` method, allowing trained models to project unseen data into the learned embedding space.\r\n\r\n### Basic Transform Usage\r\n\r\n```python\r\ntrans = umap.UMAP(n_neighbors=15, random_state=42).fit(X_train)\r\n\r\ntest_embedding = trans.transform(X_test)\r\n```\r\n\r\n### Integration with Machine Learning Pipelines\r\n\r\n```python\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\nimport umap\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\r\n\r\nscaler = StandardScaler()\r\nX_train_scaled = scaler.fit_transform(X_train)\r\nX_test_scaled = scaler.transform(X_test)\r\n\r\nreducer = umap.UMAP(n_components=10, random_state=42)\r\nX_train_embedded = reducer.fit_transform(X_train_scaled)\r\nX_test_embedded = reducer.transform(X_test_scaled)\r\n\r\n\r\n### Parametric UMAP\r\n\r\nParametric UMAP replaces direct embedding optimization with a learned neural network mapping function.\r\n\r\n**Key differences from standard UMAP:**\r\n- Uses TensorFlow/Keras to train encoder networks\r\n- Enables efficient transformation of new data\r\n- Supports reconstruction via decoder networks (inverse transform)\r\n- Allows custom architectures (CNNs for images, RNNs for sequences)\r\n\r\n**Installation:**\r\n```bash\r\npip install umap-learn[parametric_umap]\r\n```\r\n\r\n**Basic usage:**\r\n```python\r\nfrom umap.parametric_umap import ParametricUMAP\r\n\r\nembedder = ParametricUMAP()\r\nembedding = embedder.fit_transform(data)\r\n\r\nnew_embedding = embedder.transform(new_data)\r\n```\r\n\r\n**Custom architecture:**\r\n```python\r\nimport tensorflow as tf\r\n\r\nencoder = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(input_dim,)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(2)  # Output dimension\r\n])\r\n\r\nembedder = ParametricUMAP(encoder=encoder, dims=(input_dim,))\r\nembedding = embedder.fit_transform(data)\r\n```\r\n\r\n**When to use Parametric UMAP:**\r\n- Need efficient transformation of new data after training\r\n- Require reconstruction capabilities (inverse transforms)\r\n- Want to combine UMAP with autoencoders\r\n- Working with complex data types (images, sequences) benefiting from specialized architectures\r\n\r\n**When to use standard UMAP:**\r\n- Need simplicity and quick prototyping\r\n- Dataset is small and computational efficiency isn't critical\r\n- Don't require learned transformations for future data\r\n\r\n### Inverse Transforms\r\n\r\nInverse transforms enable reconstruction of high-dimensional data from low-dimensional embeddings.\r\n\r\n**Basic usage:**\r\n```python\r\nreducer = umap.UMAP()\r\nembedding = reducer.fit_transform(data)\r\n\r\nreconstructed = reducer.inverse_transform(embedding)\r\n```\r\n\r\n**Important limitations:**\r\n- Computationally expensive operation\r\n- Works poorly outside the convex hull of the embedding\r\n- Accuracy decreases in regions with gaps between clusters\r\n\r\n**Use cases:**\r\n- Understanding structure of embedded data\r\n- Visualizing smooth transitions between clusters\r\n- Exploring interpolations between data points\r\n- Generating synthetic samples in embedding space\r\n\r\n**Example: Exploring embedding space:**\r\n```python\r\nimport numpy as np\r\n\r\nx = np.linspace(embedding[:, 0].min(), embedding[:, 0].max(), 10)\r\ny = np.linspace(embedding[:, 1].min(), embedding[:, 1].max(), 10)\r\nxx, yy = np.meshgrid(x, y)\r\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\r\n\r\nreconstructed_samples = reducer.inverse_transform(grid_points)\r\n```\r\n\r\n### AlignedUMAP\r\n\r\nFor analyzing temporal or related datasets (e.g., time-series experiments, batch data):\r\n\r\n```python\r\nfrom umap import AlignedUMAP\r\n\r\ndatasets = [day1_data, day2_data, day3_data]",
  "name": "umap-learn",
  "id": "scientific-pkg-umap-learn",
  "sections": {
    "Quick Start": "plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='Spectral', s=5)\r\nplt.colorbar()\r\nplt.title('UMAP Embedding')\r\nplt.show()\r\n```",
    "UMAP for Clustering": "import matplotlib.pyplot as plt\r\nplt.scatter(vis_embedding[:, 0], vis_embedding[:, 1], c=labels, cmap='Spectral', s=5)\r\nplt.colorbar()\r\nplt.title('UMAP Visualization with HDBSCAN Clusters')\r\nplt.show()\r\n```\r\n\r\n**Important caveat:** UMAP does not completely preserve density and can create artificial cluster divisions. Always validate and explore resulting clusters.",
    "Common Issues and Solutions": "**Issue:** Disconnected components or fragmented clusters\r\n- **Solution:** Increase `n_neighbors` to emphasize more global structure\r\n\r\n**Issue:** Clusters too spread out or not well separated\r\n- **Solution:** Decrease `min_dist` to allow tighter packing\r\n\r\n**Issue:** Poor clustering results\r\n- **Solution:** Use clustering-specific parameters (n_neighbors=30, min_dist=0.0, n_components=5-10)\r\n\r\n**Issue:** Transform results differ significantly from training\r\n- **Solution:** Ensure test data distribution matches training, or use Parametric UMAP\r\n\r\n**Issue:** Slow performance on large datasets\r\n- **Solution:** Set `low_memory=True` (default), or consider dimensionality reduction with PCA first\r\n\r\n**Issue:** All points collapsed to single cluster\r\n- **Solution:** Check data preprocessing (ensure proper scaling), increase `min_dist`",
    "Advanced Features": "mapper = AlignedUMAP().fit(datasets)\r\naligned_embeddings = mapper.embeddings_  # List of embeddings\r\n```\r\n\r\n**When to use:** Comparing embeddings across related datasets while maintaining consistent coordinate systems.",
    "Parameter Tuning Guide": "umap.UMAP(n_neighbors=100, min_dist=0.5, n_components=2, metric='euclidean')\r\n```",
    "Overview": "UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique for visualization and general non-linear dimensionality reduction. Apply this skill for fast, scalable embeddings that preserve local and global structure, supervised learning, and clustering preprocessing.",
    "Resources": "### references/\r\n\r\nContains detailed API documentation:\r\n- `api_reference.md`: Complete UMAP class parameters and methods\r\n\r\nLoad these references when detailed parameter information or advanced method usage is needed.",
    "Reproducibility": "To ensure reproducible results, always set the `random_state` parameter:\r\n\r\n```python\r\nreducer = umap.UMAP(random_state=42)\r\n```\r\n\r\nUMAP uses stochastic optimization, so results will vary slightly between runs without a fixed random state.",
    "Supervised and Semi-Supervised Dimension Reduction": "from sklearn.svm import SVC\r\nclf = SVC().fit(mapper.embedding_, train_labels)\r\npredictions = clf.predict(test_embedding)\r\n```\r\n\r\n**When to use:** For supervised feature engineering in machine learning pipelines.",
    "Transforming New Data": "clf = SVC()\r\nclf.fit(X_train_embedded, y_train)\r\naccuracy = clf.score(X_test_embedded, y_test)\r\nprint(f\"Test accuracy: {accuracy:.3f}\")\r\n```\r\n\r\n### Important Considerations\r\n\r\n**Data consistency:** The transform method assumes the overall distribution in the higher-dimensional space is consistent between training and test data. When this assumption fails, consider using Parametric UMAP instead.\r\n\r\n**Performance:** Transform operations are efficient (typically <1 second), though initial calls may be slower due to Numba JIT compilation.\r\n\r\n**Scikit-learn compatibility:** UMAP follows standard sklearn conventions and works seamlessly in pipelines:\r\n\r\n```python\r\nfrom sklearn.pipeline import Pipeline\r\n\r\npipeline = Pipeline([\r\n    ('scaler', StandardScaler()),\r\n    ('umap', umap.UMAP(n_components=10)),\r\n    ('classifier', SVC())\r\n])\r\n\r\npipeline.fit(X_train, y_train)\r\npredictions = pipeline.predict(X_test)\r\n```"
  }
}