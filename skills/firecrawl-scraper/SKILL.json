{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/common-patterns.md",
      "references/endpoints.md"
    ]
  },
  "content": "**Status**: Production Ready ✅\r\n**Last Updated**: 2025-10-24\r\n**Official Docs**: https://docs.firecrawl.dev\r\n**API Version**: v2\r\n\r\n---\r\n\r\n\r\nFirecrawl requires an API key for all requests.\r\n\r\n### Get API Key\r\n1. Sign up at https://www.firecrawl.dev\r\n2. Go to dashboard → API Keys\r\n3. Copy your API key (starts with `fc-`)\r\n\r\n### Store Securely\r\n**NEVER hardcode API keys in code!**\r\n\r\n```bash\r\nFIRECRAWL_API_KEY=fc-your-api-key-here\r\n```\r\n\r\n```bash\r\n\r\n### Installation\r\n\r\n```bash\r\npip install firecrawl-py\r\n```\r\n\r\n**Latest Version**: `firecrawl-py v4.5.0+`\r\n\r\n### Basic Scrape\r\n\r\n```python\r\nimport os\r\nfrom firecrawl import FirecrawlApp\r\n\r\napp = FirecrawlApp(api_key=os.environ.get(\"FIRECRAWL_API_KEY\"))\r\n\r\nresult = app.scrape_url(\r\n    url=\"https://example.com/article\",\r\n    params={\r\n        \"formats\": [\"markdown\", \"html\"],\r\n        \"onlyMainContent\": True\r\n    }\r\n)\r\n\r\nmarkdown = result.get(\"markdown\")\r\nprint(markdown)\r\n```\r\n\r\n### Crawl Multiple Pages\r\n\r\n```python\r\nimport os\r\nfrom firecrawl import FirecrawlApp\r\n\r\napp = FirecrawlApp(api_key=os.environ.get(\"FIRECRAWL_API_KEY\"))\r\n\r\ncrawl_result = app.crawl_url(\r\n    url=\"https://docs.example.com\",\r\n    params={\r\n        \"limit\": 100,\r\n        \"scrapeOptions\": {\r\n            \"formats\": [\"markdown\"]\r\n        }\r\n    },\r\n    poll_interval=5  # Check status every 5 seconds\r\n)\r\n\r\nfor page in crawl_result.get(\"data\", []):\r\n    url = page.get(\"url\")\r\n    markdown = page.get(\"markdown\")\r\n    print(f\"Scraped: {url}\")\r\n```\r\n\r\n### Extract Structured Data\r\n\r\n```python\r\nimport os\r\nfrom firecrawl import FirecrawlApp\r\n\r\napp = FirecrawlApp(api_key=os.environ.get(\"FIRECRAWL_API_KEY\"))\r\n\r\nschema = {\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \"company_name\": {\"type\": \"string\"},\r\n        \"product_price\": {\"type\": \"number\"},\r\n        \"availability\": {\"type\": \"string\"}\r\n    },\r\n    \"required\": [\"company_name\", \"product_price\"]\r\n}\r\n\r\n\r\n### Installation\r\n\r\n```bash\r\nnpm install @mendable/firecrawl-js\r\npnpm add @mendable/firecrawl-js\r\n\r\n### 1. Documentation Scraping\r\n\r\n**Scenario**: Convert entire documentation site to markdown for RAG/chatbot\r\n\r\n```python\r\napp = FirecrawlApp(api_key=os.environ.get(\"FIRECRAWL_API_KEY\"))\r\n\r\ndocs = app.crawl_url(\r\n    url=\"https://docs.myapi.com\",\r\n    params={\r\n        \"limit\": 500,\r\n        \"scrapeOptions\": {\r\n            \"formats\": [\"markdown\"],\r\n            \"onlyMainContent\": True\r\n        },\r\n        \"allowedDomains\": [\"docs.myapi.com\"]\r\n    }\r\n)\r\n\r\nfor page in docs.get(\"data\", []):\r\n    filename = page[\"url\"].replace(\"https://\", \"\").replace(\"/\", \"_\") + \".md\"\r\n    with open(f\"docs/{filename}\", \"w\") as f:\r\n        f.write(page[\"markdown\"])\r\n```\r\n\r\n### 2. Product Data Extraction\r\n\r\n**Scenario**: Extract structured product data for e-commerce\r\n\r\n```typescript\r\nconst schema = z.object({\r\n  title: z.string(),\r\n  price: z.number(),\r\n  description: z.string(),\r\n  images: z.array(z.string()),\r\n  in_stock: z.boolean()\r\n});\r\n\r\nconst products = await app.extract({\r\n  urls: productUrls,\r\n  schema: schema,\r\n  systemPrompt: 'Extract all product details including price and availability'\r\n});\r\n```\r\n\r\n### 3. News Article Scraping\r\n\r\n**Scenario**: Extract clean article content without ads/navigation\r\n\r\n```python\r\narticle = app.scrape_url(\r\n    url=\"https://news.com/article\",\r\n    params={\r\n        \"formats\": [\"markdown\"],\r\n        \"onlyMainContent\": True,\r\n        \"removeBase64Images\": True\r\n    }\r\n)\r\n\r\n\r\n### Issue: \"Invalid API Key\"\r\n**Cause**: API key not set or incorrect\r\n**Fix**:\r\n```bash\r\necho $FIRECRAWL_API_KEY",
  "name": "firecrawl-scraper",
  "id": "firecrawl-scraper",
  "sections": {
    "Authentication": "FIRECRAWL_API_KEY=fc-your-api-key-here\r\n```\r\n\r\n---",
    "API Endpoints": "### 1. `/v2/scrape` - Single Page Scraping\r\nScrapes a single webpage and returns clean, structured content.\r\n\r\n**Use Cases**:\r\n- Extract article content\r\n- Get product details\r\n- Scrape specific pages\r\n- Convert HTML to markdown\r\n\r\n**Key Options**:\r\n- `formats`: [\"markdown\", \"html\", \"screenshot\"]\r\n- `onlyMainContent`: true/false (removes nav, footer, ads)\r\n- `waitFor`: milliseconds to wait before scraping\r\n- `actions`: browser automation actions (click, scroll, etc.)\r\n\r\n### 2. `/v2/crawl` - Full Site Crawling\r\nCrawls all accessible pages from a starting URL.\r\n\r\n**Use Cases**:\r\n- Index entire documentation sites\r\n- Archive website content\r\n- Build knowledge bases\r\n- Scrape multi-page content\r\n\r\n**Key Options**:\r\n- `limit`: max pages to crawl\r\n- `maxDepth`: how many links deep to follow\r\n- `allowedDomains`: restrict to specific domains\r\n- `excludePaths`: skip certain URL patterns\r\n\r\n### 3. `/v2/map` - URL Discovery\r\nMaps all URLs on a website without scraping content.\r\n\r\n**Use Cases**:\r\n- Find sitemap\r\n- Discover all pages\r\n- Plan crawling strategy\r\n- Audit website structure\r\n\r\n### 4. `/v2/extract` - Structured Data Extraction\r\nUses AI to extract specific data fields from pages.\r\n\r\n**Use Cases**:\r\n- Extract product prices and names\r\n- Parse contact information\r\n- Build structured datasets\r\n- Custom data schemas\r\n\r\n**Key Options**:\r\n- `schema`: Zod or JSON schema defining desired structure\r\n- `systemPrompt`: guide AI extraction behavior\r\n\r\n---",
    "Cloudflare Workers Integration": "### ⚠️ Important: SDK Compatibility\r\n\r\n**The Firecrawl SDK cannot run in Cloudflare Workers** due to Node.js dependencies (specifically `axios` which uses Node.js `http` module). Workers require Web Standard APIs.\r\n\r\n**✅ Use the direct REST API with `fetch` instead** (see example below).\r\n\r\n**Alternative**: Self-host with [workers-firecrawl](https://github.com/G4brym/workers-firecrawl) - a Workers-native implementation (requires Workers Paid Plan, only implements `/search` endpoint).\r\n\r\n---\r\n\r\n### Workers Example: Direct REST API\r\n\r\nThis example uses the `fetch` API to call Firecrawl directly - works perfectly in Cloudflare Workers:\r\n\r\n```typescript\r\ninterface Env {\r\n  FIRECRAWL_API_KEY: string;\r\n  SCRAPED_CACHE?: KVNamespace; // Optional: for caching results\r\n}\r\n\r\ninterface FirecrawlScrapeResponse {\r\n  success: boolean;\r\n  data: {\r\n    markdown?: string;\r\n    html?: string;\r\n    metadata: {\r\n      title?: string;\r\n      description?: string;\r\n      language?: string;\r\n      sourceURL: string;\r\n    };\r\n  };\r\n}\r\n\r\nexport default {\r\n  async fetch(request: Request, env: Env): Promise<Response> {\r\n    if (request.method !== 'POST') {\r\n      return Response.json({ error: 'Method not allowed' }, { status: 405 });\r\n    }\r\n\r\n    try {\r\n      const { url } = await request.json<{ url: string }>();\r\n\r\n      if (!url) {\r\n        return Response.json({ error: 'URL is required' }, { status: 400 });\r\n      }\r\n\r\n      // Check cache (optional)\r\n      if (env.SCRAPED_CACHE) {\r\n        const cached = await env.SCRAPED_CACHE.get(url, 'json');\r\n        if (cached) {\r\n          return Response.json({ cached: true, data: cached });\r\n        }\r\n      }\r\n\r\n      // Call Firecrawl API directly using fetch\r\n      const response = await fetch('https://api.firecrawl.dev/v2/scrape', {\r\n        method: 'POST',\r\n        headers: {\r\n          'Authorization': `Bearer ${env.FIRECRAWL_API_KEY}`,\r\n          'Content-Type': 'application/json',\r\n        },\r\n        body: JSON.stringify({\r\n          url: url,\r\n          formats: ['markdown'],\r\n          onlyMainContent: true,\r\n          removeBase64Images: true\r\n        })\r\n      });\r\n\r\n      if (!response.ok) {\r\n        const errorText = await response.text();\r\n        throw new Error(`Firecrawl API error (${response.status}): ${errorText}`);\r\n      }\r\n\r\n      const result = await response.json<FirecrawlScrapeResponse>();\r\n\r\n      // Cache for 1 hour (optional)\r\n      if (env.SCRAPED_CACHE && result.success) {\r\n        await env.SCRAPED_CACHE.put(\r\n          url,\r\n          JSON.stringify(result.data),\r\n          { expirationTtl: 3600 }\r\n        );\r\n      }\r\n\r\n      return Response.json({\r\n        cached: false,\r\n        data: result.data\r\n      });\r\n\r\n    } catch (error) {\r\n      console.error('Scraping error:', error);\r\n      return Response.json(\r\n        { error: error instanceof Error ? error.message : 'Unknown error' },\r\n        { status: 500 }\r\n      );\r\n    }\r\n  }\r\n};\r\n```\r\n\r\n**Environment Setup**: Add `FIRECRAWL_API_KEY` in Wrangler secrets:\r\n\r\n```bash\r\nnpx wrangler secret put FIRECRAWL_API_KEY\r\n```\r\n\r\n**Optional KV Binding** (for caching - add to `wrangler.jsonc`):\r\n\r\n```jsonc\r\n{\r\n  \"kv_namespaces\": [\r\n    {\r\n      \"binding\": \"SCRAPED_CACHE\",\r\n      \"id\": \"your-kv-namespace-id\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nSee `templates/firecrawl-worker-fetch.ts` for a complete production-ready example.\r\n\r\n---",
    "Advanced Features": "### Browser Actions\r\n\r\nPerform interactions before scraping:\r\n\r\n```python\r\nresult = app.scrape_url(\r\n    url=\"https://example.com\",\r\n    params={\r\n        \"actions\": [\r\n            {\"type\": \"click\", \"selector\": \"button.load-more\"},\r\n            {\"type\": \"wait\", \"milliseconds\": 2000},\r\n            {\"type\": \"scroll\", \"direction\": \"down\"}\r\n        ]\r\n    }\r\n)\r\n```\r\n\r\n### Custom Headers\r\n\r\n```python\r\nresult = app.scrape_url(\r\n    url=\"https://example.com\",\r\n    params={\r\n        \"headers\": {\r\n            \"User-Agent\": \"Custom Bot 1.0\",\r\n            \"Accept-Language\": \"en-US\"\r\n        }\r\n    }\r\n)\r\n```\r\n\r\n### Webhooks for Long Crawls\r\n\r\nInstead of polling, receive results via webhook:\r\n\r\n```python\r\ncrawl = app.crawl_url(\r\n    url=\"https://docs.example.com\",\r\n    params={\r\n        \"limit\": 1000,\r\n        \"webhook\": \"https://your-domain.com/webhook\"\r\n    }\r\n)\r\n```\r\n\r\n---",
    "TypeScript/Node.js SDK Usage": "npm install firecrawl\r\n```\r\n\r\n**Latest Version**: `@mendable/firecrawl-js v4.4.1+` (or `firecrawl v4.4.1+`)\r\n\r\n### Basic Scrape\r\n\r\n```typescript\r\nimport FirecrawlApp from '@mendable/firecrawl-js';\r\n\r\n// Initialize client\r\nconst app = new FirecrawlApp({\r\n  apiKey: process.env.FIRECRAWL_API_KEY\r\n});\r\n\r\n// Scrape a single page\r\nconst result = await app.scrapeUrl('https://example.com/article', {\r\n  formats: ['markdown', 'html'],\r\n  onlyMainContent: true\r\n});\r\n\r\n// Access markdown content\r\nconst markdown = result.markdown;\r\nconsole.log(markdown);\r\n```\r\n\r\n### Crawl Multiple Pages\r\n\r\n```typescript\r\nimport FirecrawlApp from '@mendable/firecrawl-js';\r\n\r\nconst app = new FirecrawlApp({\r\n  apiKey: process.env.FIRECRAWL_API_KEY\r\n});\r\n\r\n// Start crawl\r\nconst crawlResult = await app.crawlUrl('https://docs.example.com', {\r\n  limit: 100,\r\n  scrapeOptions: {\r\n    formats: ['markdown']\r\n  }\r\n});\r\n\r\n// Process results\r\nfor (const page of crawlResult.data) {\r\n  console.log(`Scraped: ${page.url}`);\r\n  console.log(page.markdown);\r\n}\r\n```\r\n\r\n### Extract Structured Data with Zod\r\n\r\n```typescript\r\nimport FirecrawlApp from '@mendable/firecrawl-js';\r\nimport { z } from 'zod';\r\n\r\nconst app = new FirecrawlApp({\r\n  apiKey: process.env.FIRECRAWL_API_KEY\r\n});\r\n\r\n// Define schema with Zod\r\nconst schema = z.object({\r\n  company_name: z.string(),\r\n  product_price: z.number(),\r\n  availability: z.string()\r\n});\r\n\r\n// Extract data\r\nconst result = await app.extract({\r\n  urls: ['https://example.com/product'],\r\n  schema: schema,\r\n  systemPrompt: 'Extract product information from the page'\r\n});\r\n\r\nconsole.log(result);\r\n```\r\n\r\n---",
    "Package Versions": "| Package | Version | Last Checked |\r\n|---------|---------|--------------|\r\n| firecrawl-py | 4.5.0+ | 2025-10-20 |\r\n| @mendable/firecrawl-js (or firecrawl) | 4.4.1+ | 2025-10-24 |\r\n| API Version | v2 | Current |\r\n\r\n**Note**: The Node.js SDK requires Node.js >=22.0.0 and cannot run in Cloudflare Workers. Use direct REST API calls in Workers (see Cloudflare Workers Integration section).\r\n\r\n---",
    "Next Steps After Using This Skill": "1. **Store scraped data**: Use Cloudflare D1, R2, or KV to persist results\r\n2. **Build RAG system**: Combine with Vectorize for semantic search\r\n3. **Add scheduling**: Use Cloudflare Queues for recurring scrapes\r\n4. **Process content**: Use Workers AI to analyze scraped data\r\n\r\n---\r\n\r\n**Token Savings**: ~60% vs manual integration\r\n**Error Prevention**: API authentication, rate limiting, format handling\r\n**Production Ready**: ✅",
    "Error Handling": "### Python\r\n\r\n```python\r\nfrom firecrawl import FirecrawlApp\r\nfrom firecrawl.exceptions import FirecrawlException\r\n\r\napp = FirecrawlApp(api_key=os.environ.get(\"FIRECRAWL_API_KEY\"))\r\n\r\ntry:\r\n    result = app.scrape_url(\"https://example.com\")\r\nexcept FirecrawlException as e:\r\n    print(f\"Firecrawl error: {e}\")\r\nexcept Exception as e:\r\n    print(f\"Unexpected error: {e}\")\r\n```\r\n\r\n### TypeScript\r\n\r\n```typescript\r\nimport FirecrawlApp from '@mendable/firecrawl-js';\r\n\r\nconst app = new FirecrawlApp({\r\n  apiKey: process.env.FIRECRAWL_API_KEY\r\n});\r\n\r\ntry {\r\n  const result = await app.scrapeUrl('https://example.com');\r\n} catch (error) {\r\n  if (error.response) {\r\n    // API error\r\n    console.error('API Error:', error.response.data);\r\n  } else {\r\n    // Network or other error\r\n    console.error('Error:', error.message);\r\n  }\r\n}\r\n```\r\n\r\n---",
    "When to Use This Skill": "✅ **Use Firecrawl when:**\r\n- Scraping modern websites with JavaScript\r\n- Need clean markdown output for LLMs\r\n- Building RAG systems from web content\r\n- Extracting structured data at scale\r\n- Dealing with bot protection\r\n- Need reliable, production-ready scraping\r\n\r\n❌ **Don't use Firecrawl when:**\r\n- Scraping simple static HTML (use cheerio/beautifulsoup)\r\n- Have existing Puppeteer/Playwright setup working well\r\n- Working with APIs (use direct API calls instead)\r\n- Budget constraints (free tier has limits)\r\n\r\n---",
    "Common Use Cases": "content = article.get(\"markdown\")\r\n```\r\n\r\n---",
    "Common Issues & Solutions": "```\r\n\r\n### Issue: \"Rate limit exceeded\"\r\n**Cause**: Exceeded monthly credits\r\n**Fix**:\r\n- Check usage in dashboard\r\n- Upgrade plan or wait for reset\r\n- Use `onlyMainContent: true` to reduce credits\r\n\r\n### Issue: \"Timeout error\"\r\n**Cause**: Page takes too long to load\r\n**Fix**:\r\n```python\r\nresult = app.scrape_url(url, params={\"waitFor\": 10000})  # Wait 10s\r\n```\r\n\r\n### Issue: \"Content is empty\"\r\n**Cause**: Content loaded via JavaScript after initial render\r\n**Fix**:\r\n```python\r\nresult = app.scrape_url(url, params={\r\n    \"waitFor\": 5000,\r\n    \"actions\": [{\"type\": \"wait\", \"milliseconds\": 3000}]\r\n})\r\n```\r\n\r\n---",
    "Official Documentation": "- **Docs**: https://docs.firecrawl.dev\r\n- **Python SDK**: https://docs.firecrawl.dev/sdks/python\r\n- **Node.js SDK**: https://docs.firecrawl.dev/sdks/node\r\n- **API Reference**: https://docs.firecrawl.dev/api-reference\r\n- **GitHub**: https://github.com/mendableai/firecrawl\r\n- **Dashboard**: https://www.firecrawl.dev/app\r\n\r\n---",
    "Rate Limits & Best Practices": "### Rate Limits\r\n- **Free tier**: 500 credits/month\r\n- **Paid tiers**: Higher limits based on plan\r\n- Credits consumed vary by endpoint and options\r\n\r\n### Best Practices\r\n\r\n1. **Use `onlyMainContent: true`** to reduce credits and get cleaner data\r\n2. **Set reasonable limits** on crawls to avoid excessive costs\r\n3. **Handle retries** with exponential backoff for transient errors\r\n4. **Cache results** locally to avoid re-scraping same content\r\n5. **Use `map` endpoint first** to plan crawling strategy\r\n6. **Batch extract calls** when processing multiple URLs\r\n7. **Monitor credit usage** in dashboard\r\n\r\n---",
    "What is Firecrawl?": "Firecrawl is a **Web Data API for AI** that turns entire websites into LLM-ready markdown or structured data. It handles:\r\n\r\n- **JavaScript rendering** - Executes client-side JavaScript to capture dynamic content\r\n- **Anti-bot bypass** - Gets past CAPTCHA and bot detection systems\r\n- **Format conversion** - Outputs as markdown, JSON, or structured data\r\n- **Screenshot capture** - Saves visual representations of pages\r\n- **Browser automation** - Full headless browser capabilities\r\n\r\n---",
    "Python SDK Usage": "result = app.extract(\r\n    urls=[\"https://example.com/product\"],\r\n    params={\r\n        \"schema\": schema,\r\n        \"systemPrompt\": \"Extract product information from the page\"\r\n    }\r\n)\r\n\r\nprint(result)\r\n```\r\n\r\n---"
  }
}