{
  "description": "\"Graph Neural Networks (PyG). Node/graph classification, link prediction, GCN, GAT, GraphSAGE, heterogeneous graphs, molecular property prediction, for geometric deep learning.\"",
  "references": {
    "files": [
      "references/datasets_reference.md",
      "references/layers_reference.md",
      "references/transforms_reference.md"
    ]
  },
  "content": "### Installation\r\n\r\n```bash\r\npip install torch_geometric\r\n```\r\n\r\nFor additional dependencies (sparse operations, clustering):\r\n```bash\r\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\r\n```\r\n\r\n### Basic Graph Creation\r\n\r\n```python\r\nimport torch\r\nfrom torch_geometric.data import Data\r\n\r\nedge_index = torch.tensor([[0, 1, 1, 2],  # source nodes\r\n                           [1, 0, 2, 1]], dtype=torch.long)  # target nodes\r\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)  # node features\r\n\r\ndata = Data(x=x, edge_index=edge_index)\r\nprint(f\"Nodes: {data.num_nodes}, Edges: {data.num_edges}\")\r\n```\r\n\r\n### Loading a Benchmark Dataset\r\n\r\n```python\r\nfrom torch_geometric.datasets import Planetoid\r\n\r\n\r\n### Data Structure\r\n\r\nPyG represents graphs using the `torch_geometric.data.Data` class with these key attributes:\r\n\r\n- **`data.x`**: Node feature matrix `[num_nodes, num_node_features]`\r\n- **`data.edge_index`**: Graph connectivity in COO format `[2, num_edges]`\r\n- **`data.edge_attr`**: Edge feature matrix `[num_edges, num_edge_features]` (optional)\r\n- **`data.y`**: Target labels for nodes or graphs\r\n- **`data.pos`**: Node spatial positions `[num_nodes, num_dimensions]` (optional)\r\n- **Custom attributes**: Can add any attribute (e.g., `data.train_mask`, `data.batch`)\r\n\r\n**Important**: These attributes are not mandatory—extend Data objects with custom attributes as needed.\r\n\r\n### Edge Index Format\r\n\r\nEdges are stored in COO (coordinate) format as a `[2, num_edges]` tensor:\r\n- First row: source node indices\r\n- Second row: target node indices\r\n\r\n```python\r\n\r\n### Loading Built-in Datasets\r\n\r\nPyG provides extensive benchmark datasets:\r\n\r\n```python\r\nfrom torch_geometric.datasets import Planetoid\r\ndataset = Planetoid(root='/tmp/Cora', name='Cora')  # or 'CiteSeer', 'PubMed'\r\n\r\nfrom torch_geometric.datasets import TUDataset\r\ndataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\r\n\r\nfrom torch_geometric.datasets import QM9\r\ndataset = QM9(root='/tmp/QM9')\r\n\r\nfrom torch_geometric.datasets import Reddit\r\ndataset = Reddit(root='/tmp/Reddit')\r\n```\r\n\r\nCheck `references/datasets_reference.md` for a comprehensive list.\r\n\r\n### Creating Custom Datasets\r\n\r\nFor datasets that fit in memory, inherit from `InMemoryDataset`:\r\n\r\n```python\r\nfrom torch_geometric.data import InMemoryDataset, Data\r\nimport torch\r\n\r\nclass MyOwnDataset(InMemoryDataset):\r\n    def __init__(self, root, transform=None, pre_transform=None):\r\n        super().__init__(root, transform, pre_transform)\r\n        self.load(self.processed_paths[0])\r\n\r\n    @property\r\n    def raw_file_names(self):\r\n        return ['my_data.csv']  # Files needed in raw_dir\r\n\r\n    @property\r\n    def processed_file_names(self):\r\n        return ['data.pt']  # Files in processed_dir\r\n\r\n    def download(self):\r\n        # Download raw data to self.raw_dir\r\n        pass\r\n\r\n    def process(self):\r\n        # Read data, create Data objects\r\n        data_list = []\r\n\r\n        # Example: Create a simple graph\r\n        edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\r\n        x = torch.randn(2, 16)\r\n        y = torch.tensor([0], dtype=torch.long)\r\n\r\n        data = Data(x=x, edge_index=edge_index, y=y)\r\n        data_list.append(data)\r\n\r\n        # Apply pre_filter and pre_transform\r\n        if self.pre_filter is not None:\r\n            data_list = [d for d in data_list if self.pre_filter(d)]\r\n\r\n        if self.pre_transform is not None:\r\n            data_list = [self.pre_transform(d) for d in data_list]\r\n\r\n        # Save processed data\r\n        self.save(data_list, self.processed_paths[0])\r\n```\r\n\r\nFor large datasets that don't fit in memory, inherit from `Dataset` and implement `len()` and `get(idx)`.\r\n\r\n### Loading Graphs from CSV\r\n\r\n```python\r\nimport pandas as pd\r\nimport torch\r\nfrom torch_geometric.data import HeteroData\r\n\r\nnodes_df = pd.read_csv('nodes.csv')\r\nx = torch.tensor(nodes_df[['feat1', 'feat2']].values, dtype=torch.float)\r\n\r\n\r\n### Node Classification (Single Graph)\r\n\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.datasets import Planetoid\r\n\r\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\r\ndata = dataset[0]\r\n\r\nmodel = GCN(dataset.num_features, dataset.num_classes)\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\r\n\r\nmodel.train()\r\nfor epoch in range(200):\r\n    optimizer.zero_grad()\r\n    out = model(data)\r\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n    if epoch % 10 == 0:\r\n        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\r\n\r\nmodel.eval()\r\npred = model(data).argmax(dim=1)\r\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\r\nacc = int(correct) / int(data.test_mask.sum())\r\nprint(f'Test Accuracy: {acc:.4f}')\r\n```\r\n\r\n### Graph Classification (Multiple Graphs)\r\n\r\n```python\r\nfrom torch_geometric.datasets import TUDataset\r\nfrom torch_geometric.loader import DataLoader\r\nfrom torch_geometric.nn import global_mean_pool\r\n\r\nclass GraphClassifier(torch.nn.Module):\r\n    def __init__(self, num_features, num_classes):\r\n        super().__init__()\r\n        self.conv1 = GCNConv(num_features, 64)\r\n        self.conv2 = GCNConv(64, 64)\r\n        self.lin = torch.nn.Linear(64, num_classes)\r\n\r\n    def forward(self, data):\r\n        x, edge_index, batch = data.x, data.edge_index, data.batch\r\n\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.conv2(x, edge_index)\r\n        x = F.relu(x)\r\n\r\n        # Global pooling (aggregate node features to graph-level)\r\n        x = global_mean_pool(x, batch)\r\n\r\n        x = self.lin(x)\r\n        return F.log_softmax(x, dim=1)\r\n\r\ndataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\r\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\r\n\r\nmodel = GraphClassifier(dataset.num_features, dataset.num_classes)\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\r\n\r\nmodel.train()\r\nfor epoch in range(100):\r\n    total_loss = 0\r\n    for batch in loader:\r\n        optimizer.zero_grad()\r\n        out = model(batch)\r\n        loss = F.nll_loss(out, batch.y)\r\n        loss.backward()\r\n        optimizer.step()\r\n        total_loss += loss.item()\r\n\r\n    if epoch % 10 == 0:\r\n        print(f'Epoch {epoch}, Loss: {total_loss / len(loader):.4f}')\r\n```\r\n\r\n### Large-Scale Graphs with Neighbor Sampling\r\n\r\nFor large graphs, use `NeighborLoader` to sample subgraphs:\r\n\r\n```python\r\nfrom torch_geometric.loader import NeighborLoader\r\n\r\ntrain_loader = NeighborLoader(\r\n    data,\r\n    num_neighbors=[25, 10],  # Sample 25 neighbors for 1st hop, 10 for 2nd hop\r\n    batch_size=128,\r\n    input_nodes=data.train_mask,\r\n)\r\n\r\n\r\n### Heterogeneous Graphs\r\n\r\nFor graphs with multiple node and edge types, use `HeteroData`:\r\n\r\n```python\r\nfrom torch_geometric.data import HeteroData\r\n\r\ndata = HeteroData()\r\n\r\ndata['paper'].x = torch.randn(100, 128)  # 100 papers with 128 features\r\ndata['author'].x = torch.randn(200, 64)  # 200 authors with 64 features\r\n\r\ndata['author', 'writes', 'paper'].edge_index = torch.randint(0, 200, (2, 500))\r\ndata['paper', 'cites', 'paper'].edge_index = torch.randint(0, 100, (2, 300))\r\n\r\nprint(data)\r\n```\r\n\r\nConvert homogeneous models to heterogeneous:\r\n\r\n```python\r\nfrom torch_geometric.nn import to_hetero\r\n\r\nmodel = GNN(...)\r\n\r\nmodel = to_hetero(model, data.metadata(), aggr='sum')\r\n\r\nout = model(data.x_dict, data.edge_index_dict)\r\n```\r\n\r\nOr use `HeteroConv` for custom edge-type-specific operations:\r\n\r\n```python\r\nfrom torch_geometric.nn import HeteroConv, GCNConv, SAGEConv\r\n\r\nclass HeteroGNN(torch.nn.Module):\r\n    def __init__(self, metadata):\r\n        super().__init__()\r\n        self.conv1 = HeteroConv({\r\n            ('paper', 'cites', 'paper'): GCNConv(-1, 64),\r\n            ('author', 'writes', 'paper'): SAGEConv((-1, -1), 64),\r\n        }, aggr='sum')\r\n\r\n        self.conv2 = HeteroConv({\r\n            ('paper', 'cites', 'paper'): GCNConv(64, 32),\r\n            ('author', 'writes', 'paper'): SAGEConv((64, 64), 32),\r\n        }, aggr='sum')\r\n\r\n    def forward(self, x_dict, edge_index_dict):\r\n        x_dict = self.conv1(x_dict, edge_index_dict)\r\n        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\r\n        x_dict = self.conv2(x_dict, edge_index_dict)\r\n        return x_dict\r\n```\r\n\r\n### Transforms\r\n\r\nApply transforms to modify graph structure or features:\r\n\r\n```python\r\nfrom torch_geometric.transforms import NormalizeFeatures, AddSelfLoops, Compose\r\n\r\ntransform = NormalizeFeatures()\r\ndataset = Planetoid(root='/tmp/Cora', name='Cora', transform=transform)\r\n\r\ntransform = Compose([\r\n    AddSelfLoops(),\r\n    NormalizeFeatures(),\r\n])\r\ndataset = Planetoid(root='/tmp/Cora', name='Cora', transform=transform)\r\n```\r\n\r\nCommon transforms:\r\n- **Structure**: `ToUndirected`, `AddSelfLoops`, `RemoveSelfLoops`, `KNNGraph`, `RadiusGraph`\r\n- **Features**: `NormalizeFeatures`, `NormalizeScale`, `Center`\r\n- **Sampling**: `RandomNodeSplit`, `RandomLinkSplit`\r\n- **Positional Encoding**: `AddLaplacianEigenvectorPE`, `AddRandomWalkPE`\r\n\r\nSee `references/transforms_reference.md` for the full list.\r\n\r\n### Model Explainability\r\n\r\nPyG provides explainability tools to understand model predictions:\r\n\r\n```python\r\nfrom torch_geometric.explain import Explainer, GNNExplainer\r\n\r\nexplainer = Explainer(\r\n    model=model,\r\n    algorithm=GNNExplainer(epochs=200),\r\n    explanation_type='model',  # or 'phenomenon'\r\n    node_mask_type='attributes',\r\n    edge_mask_type='object',\r\n    model_config=dict(\r\n        mode='multiclass_classification',\r\n        task_level='node',\r\n        return_type='log_probs',\r\n    ),\r\n)\r\n\r\nnode_idx = 10\r\nexplanation = explainer(data.x, data.edge_index, index=node_idx)\r\n\r\n\r\n### Check Graph Properties\r\n\r\n```python\r\nfrom torch_geometric.utils import is_undirected\r\nprint(f\"Is undirected: {is_undirected(data.edge_index)}\")\r\n\r\nfrom torch_geometric.utils import connected_components\r\nprint(f\"Connected components: {connected_components(data.edge_index)}\")\r\n\r\nfrom torch_geometric.utils import contains_self_loops\r\nprint(f\"Has self-loops: {contains_self_loops(data.edge_index)}\")\r\n```\r\n\r\n### GPU Training\r\n\r\n```python\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nmodel = model.to(device)\r\ndata = data.to(device)\r\n\r\nfor batch in loader:\r\n    batch = batch.to(device)\r\n    # Train...\r\n```\r\n\r\n### Save and Load Models\r\n\r\n```python\r\ntorch.save(model.state_dict(), 'model.pth')",
  "name": "torch-geometric",
  "id": "scientific-pkg-torch_geometric",
  "sections": {
    "Quick Start": "dataset = Planetoid(root='/tmp/Cora', name='Cora')\r\ndata = dataset[0]  # Get the first (and only) graph\r\n\r\nprint(f\"Dataset: {dataset}\")\r\nprint(f\"Nodes: {data.num_nodes}, Edges: {data.num_edges}\")\r\nprint(f\"Features: {data.num_node_features}, Classes: {dataset.num_classes}\")\r\n```",
    "Building Graph Neural Networks": "### Message Passing Paradigm\r\n\r\nGNNs in PyG follow a neighborhood aggregation scheme:\r\n1. Transform node features\r\n2. Propagate messages along edges\r\n3. Aggregate messages from neighbors\r\n4. Update node representations\r\n\r\n### Using Pre-Built Layers\r\n\r\nPyG provides 40+ convolutional layers. Common ones include:\r\n\r\n**GCNConv** (Graph Convolutional Network):\r\n```python\r\nfrom torch_geometric.nn import GCNConv\r\nimport torch.nn.functional as F\r\n\r\nclass GCN(torch.nn.Module):\r\n    def __init__(self, num_features, num_classes):\r\n        super().__init__()\r\n        self.conv1 = GCNConv(num_features, 16)\r\n        self.conv2 = GCNConv(16, num_classes)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.conv2(x, edge_index)\r\n        return F.log_softmax(x, dim=1)\r\n```\r\n\r\n**GATConv** (Graph Attention Network):\r\n```python\r\nfrom torch_geometric.nn import GATConv\r\n\r\nclass GAT(torch.nn.Module):\r\n    def __init__(self, num_features, num_classes):\r\n        super().__init__()\r\n        self.conv1 = GATConv(num_features, 8, heads=8, dropout=0.6)\r\n        self.conv2 = GATConv(8 * 8, num_classes, heads=1, concat=False, dropout=0.6)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.dropout(x, p=0.6, training=self.training)\r\n        x = F.elu(self.conv1(x, edge_index))\r\n        x = F.dropout(x, p=0.6, training=self.training)\r\n        x = self.conv2(x, edge_index)\r\n        return F.log_softmax(x, dim=1)\r\n```\r\n\r\n**GraphSAGE**:\r\n```python\r\nfrom torch_geometric.nn import SAGEConv\r\n\r\nclass GraphSAGE(torch.nn.Module):\r\n    def __init__(self, num_features, num_classes):\r\n        super().__init__()\r\n        self.conv1 = SAGEConv(num_features, 64)\r\n        self.conv2 = SAGEConv(64, num_classes)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.conv2(x, edge_index)\r\n        return F.log_softmax(x, dim=1)\r\n```\r\n\r\n### Custom Message Passing Layers\r\n\r\nFor custom layers, inherit from `MessagePassing`:\r\n\r\n```python\r\nfrom torch_geometric.nn import MessagePassing\r\nfrom torch_geometric.utils import add_self_loops, degree\r\n\r\nclass CustomConv(MessagePassing):\r\n    def __init__(self, in_channels, out_channels):\r\n        super().__init__(aggr='add')  # \"add\", \"mean\", or \"max\"\r\n        self.lin = torch.nn.Linear(in_channels, out_channels)\r\n\r\n    def forward(self, x, edge_index):\r\n        # Add self-loops to adjacency matrix\r\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\r\n\r\n        # Transform node features\r\n        x = self.lin(x)\r\n\r\n        # Compute normalization\r\n        row, col = edge_index\r\n        deg = degree(col, x.size(0), dtype=x.dtype)\r\n        deg_inv_sqrt = deg.pow(-0.5)\r\n        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\r\n\r\n        # Propagate messages\r\n        return self.propagate(edge_index, x=x, norm=norm)\r\n\r\n    def message(self, x_j, norm):\r\n        # x_j: features of source nodes\r\n        return norm.view(-1, 1) * x_j\r\n```\r\n\r\nKey methods:\r\n- **`forward()`**: Main entry point\r\n- **`message()`**: Constructs messages from source to target nodes\r\n- **`aggregate()`**: Aggregates messages (usually don't override—set `aggr` parameter)\r\n- **`update()`**: Updates node embeddings after aggregation\r\n\r\n**Variable naming convention**: Appending `_i` or `_j` to tensor names automatically maps them to target or source nodes.",
    "Training Workflows": "model.train()\r\nfor batch in train_loader:\r\n    optimizer.zero_grad()\r\n    out = model(batch)\r\n    # Only compute loss on seed nodes (first batch_size nodes)\r\n    loss = F.nll_loss(out[:batch.batch_size], batch.y[:batch.batch_size])\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\n\r\n**Important**:\r\n- Output subgraphs are directed\r\n- Node indices are relabeled (0 to batch.num_nodes - 1)\r\n- Only use seed node predictions for loss computation\r\n- Sampling beyond 2-3 hops is generally not feasible",
    "Overview": "PyTorch Geometric is a library built on PyTorch for developing and training Graph Neural Networks (GNNs). Apply this skill for deep learning on graphs and irregular structures, including mini-batch processing, multi-GPU training, and geometric deep learning applications.",
    "Common Patterns and Best Practices": "model = GCN(num_features, num_classes)\r\nmodel.load_state_dict(torch.load('model.pth'))\r\nmodel.eval()\r\n```\r\n\r\n### Layer Capabilities\r\n\r\nWhen choosing layers, consider these capabilities:\r\n- **SparseTensor**: Supports efficient sparse matrix operations\r\n- **edge_weight**: Handles one-dimensional edge weights\r\n- **edge_attr**: Processes multi-dimensional edge features\r\n- **Bipartite**: Works with bipartite graphs (different source/target dimensions)\r\n- **Lazy**: Enables initialization without specifying input dimensions\r\n\r\nSee the GNN cheatsheet at `references/layer_capabilities.md`.",
    "When to Use This Skill": "This skill should be used when working with:\r\n- **Graph-based machine learning**: Node classification, graph classification, link prediction\r\n- **Molecular property prediction**: Drug discovery, chemical property prediction\r\n- **Social network analysis**: Community detection, influence prediction\r\n- **Citation networks**: Paper classification, recommendation systems\r\n- **3D geometric data**: Point clouds, meshes, molecular structures\r\n- **Heterogeneous graphs**: Multi-type nodes and edges (e.g., knowledge graphs)\r\n- **Large-scale graph learning**: Neighbor sampling, distributed training",
    "Working with Datasets": "edges_df = pd.read_csv('edges.csv')\r\nedge_index = torch.tensor([edges_df['source'].values,\r\n                           edges_df['target'].values], dtype=torch.long)\r\n\r\ndata = Data(x=x, edge_index=edge_index)\r\n```",
    "Resources": "### Bundled References\r\n\r\nThis skill includes detailed reference documentation:\r\n\r\n- **`references/layers_reference.md`**: Complete listing of all 40+ GNN layers with descriptions and capabilities\r\n- **`references/datasets_reference.md`**: Comprehensive dataset catalog organized by category\r\n- **`references/transforms_reference.md`**: All available transforms and their use cases\r\n- **`references/api_patterns.md`**: Common API patterns and coding examples\r\n\r\n### Scripts\r\n\r\nUtility scripts are provided in `scripts/`:\r\n\r\n- **`scripts/visualize_graph.py`**: Visualize graph structure using networkx and matplotlib\r\n- **`scripts/create_gnn_template.py`**: Generate boilerplate code for common GNN architectures\r\n- **`scripts/benchmark_model.py`**: Benchmark model performance on standard datasets\r\n\r\nExecute scripts directly or read them for implementation patterns.\r\n\r\n### Official Resources\r\n\r\n- **Documentation**: https://pytorch-geometric.readthedocs.io/\r\n- **GitHub**: https://github.com/pyg-team/pytorch_geometric\r\n- **Tutorials**: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\r\n- **Examples**: https://github.com/pyg-team/pytorch_geometric/tree/master/examples",
    "Core Concepts": "edge_index = torch.tensor([[0, 1, 1, 2],\r\n                           [1, 0, 2, 1]], dtype=torch.long)\r\n```\r\n\r\n### Mini-Batch Processing\r\n\r\nPyG handles batching by creating block-diagonal adjacency matrices, concatenating multiple graphs into one large disconnected graph:\r\n\r\n- Adjacency matrices are stacked diagonally\r\n- Node features are concatenated along the node dimension\r\n- A `batch` vector maps each node to its source graph\r\n- No padding needed—computationally efficient\r\n\r\n```python\r\nfrom torch_geometric.loader import DataLoader\r\n\r\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\r\nfor batch in loader:\r\n    print(f\"Batch size: {batch.num_graphs}\")\r\n    print(f\"Total nodes: {batch.num_nodes}\")\r\n    # batch.batch maps nodes to graphs\r\n```",
    "Advanced Features": "print(f'Node {node_idx} explanation:')\r\nprint(f'Important edges: {explanation.edge_mask.topk(5).indices}')\r\nprint(f'Important features: {explanation.node_mask[node_idx].topk(5).indices}')\r\n```\r\n\r\n### Pooling Operations\r\n\r\nFor hierarchical graph representations:\r\n\r\n```python\r\nfrom torch_geometric.nn import TopKPooling, global_mean_pool\r\n\r\nclass HierarchicalGNN(torch.nn.Module):\r\n    def __init__(self, num_features, num_classes):\r\n        super().__init__()\r\n        self.conv1 = GCNConv(num_features, 64)\r\n        self.pool1 = TopKPooling(64, ratio=0.8)\r\n        self.conv2 = GCNConv(64, 64)\r\n        self.pool2 = TopKPooling(64, ratio=0.8)\r\n        self.lin = torch.nn.Linear(64, num_classes)\r\n\r\n    def forward(self, data):\r\n        x, edge_index, batch = data.x, data.edge_index, data.batch\r\n\r\n        x = F.relu(self.conv1(x, edge_index))\r\n        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\r\n\r\n        x = F.relu(self.conv2(x, edge_index))\r\n        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\r\n\r\n        x = global_mean_pool(x, batch)\r\n        x = self.lin(x)\r\n        return F.log_softmax(x, dim=1)\r\n```"
  }
}