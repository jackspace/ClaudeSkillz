{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/code-execution-patterns.md",
      "references/context-caching-guide.md",
      "references/function-calling-patterns.md",
      "references/generation-config.md",
      "references/grounding-guide.md",
      "references/models-guide.md",
      "references/multimodal-guide.md",
      "references/sdk-migration-guide.md",
      "references/streaming-patterns.md",
      "references/thinking-mode-guide.md",
      "references/top-errors.md"
    ]
  },
  "content": "**Version**: Phase 2 Complete ✅\r\n**Package**: @google/genai@1.27.0 (⚠️ NOT @google/generative-ai)\r\n**Last Updated**: 2025-10-25\r\n\r\n---\r\n\r\n\r\n### From @google/generative-ai to @google/genai\r\n\r\n#### 1. Update Package\r\n\r\n```bash\r\nnpm uninstall @google/generative-ai",
  "name": "google-gemini-api",
  "id": "google-gemini-api",
  "sections": {
    "Table of Contents": "**Phase 1 - Core Features**:\r\n1. [Quick Start](#quick-start)\r\n2. [Current Models (2025)](#current-models-2025)\r\n3. [SDK vs Fetch Approaches](#sdk-vs-fetch-approaches)\r\n4. [Text Generation](#text-generation)\r\n5. [Streaming](#streaming)\r\n6. [Multimodal Inputs](#multimodal-inputs)\r\n7. [Function Calling](#function-calling)\r\n8. [System Instructions](#system-instructions)\r\n9. [Multi-turn Chat](#multi-turn-chat)\r\n10. [Thinking Mode](#thinking-mode)\r\n11. [Generation Configuration](#generation-configuration)\r\n\r\n**Phase 2 - Advanced Features**:\r\n12. [Context Caching](#context-caching)\r\n13. [Code Execution](#code-execution)\r\n14. [Grounding with Google Search](#grounding-with-google-search)\r\n\r\n**Common Reference**:\r\n15. [Error Handling](#error-handling)\r\n16. [Rate Limits](#rate-limits)\r\n17. [SDK Migration Guide](#sdk-migration-guide)\r\n18. [Production Best Practices](#production-best-practices)\r\n\r\n---",
    "SDK Migration Guide": "npm install @google/genai@1.27.0\r\n```\r\n\r\n#### 2. Update Imports\r\n\r\n**Old (DEPRECATED):**\r\n```typescript\r\nimport { GoogleGenerativeAI } from '@google/generative-ai';\r\nconst genAI = new GoogleGenerativeAI(apiKey);\r\nconst model = genAI.getGenerativeModel({ model: 'gemini-2.5-flash' });\r\n```\r\n\r\n**New (CURRENT):**\r\n```typescript\r\nimport { GoogleGenAI } from '@google/genai';\r\nconst ai = new GoogleGenAI({ apiKey });\r\n// Use ai.models.generateContent() directly\r\n```\r\n\r\n#### 3. Update API Calls\r\n\r\n**Old:**\r\n```typescript\r\nconst result = await model.generateContent(prompt);\r\nconst response = await result.response;\r\nconst text = response.text();\r\n```\r\n\r\n**New:**\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: prompt\r\n});\r\nconst text = response.text;\r\n```\r\n\r\n#### 4. Update Streaming\r\n\r\n**Old:**\r\n```typescript\r\nconst result = await model.generateContentStream(prompt);\r\nfor await (const chunk of result.stream) {\r\n  console.log(chunk.text());\r\n}\r\n```\r\n\r\n**New:**\r\n```typescript\r\nconst response = await ai.models.generateContentStream({\r\n  model: 'gemini-2.5-flash',\r\n  contents: prompt\r\n});\r\nfor await (const chunk of response) {\r\n  console.log(chunk.text);\r\n}\r\n```\r\n\r\n#### 5. Update Chat\r\n\r\n**Old:**\r\n```typescript\r\nconst chat = model.startChat();\r\nconst result = await chat.sendMessage(message);\r\nconst response = await result.response;\r\n```\r\n\r\n**New:**\r\n```typescript\r\nconst chat = await ai.models.createChat({ model: 'gemini-2.5-flash' });\r\nconst response = await chat.sendMessage(message);\r\n// response.text is directly available\r\n```\r\n\r\n---",
    "Rate Limits": "### Free Tier (Gemini API)\r\n\r\nRate limits vary by model:\r\n\r\n**Gemini 2.5 Pro**:\r\n- Requests per minute: 5 RPM\r\n- Tokens per minute: 125,000 TPM\r\n- Requests per day: 100 RPD\r\n\r\n**Gemini 2.5 Flash**:\r\n- Requests per minute: 10 RPM\r\n- Tokens per minute: 250,000 TPM\r\n- Requests per day: 250 RPD\r\n\r\n**Gemini 2.5 Flash-Lite**:\r\n- Requests per minute: 15 RPM\r\n- Tokens per minute: 250,000 TPM\r\n- Requests per day: 1,000 RPD\r\n\r\n### Paid Tier (Tier 1)\r\n\r\nRequires billing account linked to your Google Cloud project.\r\n\r\n**Gemini 2.5 Pro**:\r\n- Requests per minute: 150 RPM\r\n- Tokens per minute: 2,000,000 TPM\r\n- Requests per day: 10,000 RPD\r\n\r\n**Gemini 2.5 Flash**:\r\n- Requests per minute: 1,000 RPM\r\n- Tokens per minute: 1,000,000 TPM\r\n- Requests per day: 10,000 RPD\r\n\r\n**Gemini 2.5 Flash-Lite**:\r\n- Requests per minute: 4,000 RPM\r\n- Tokens per minute: 4,000,000 TPM\r\n- Requests per day: Not specified\r\n\r\n### Higher Tiers (Tier 2 & 3)\r\n\r\n**Tier 2** (requires $250+ spending and 30-day wait):\r\n- Even higher limits available\r\n\r\n**Tier 3** (requires $1,000+ spending and 30-day wait):\r\n- Maximum limits available\r\n\r\n**Tips:**\r\n- Implement rate limit handling with exponential backoff\r\n- Use batch processing for high-volume tasks\r\n- Monitor usage in Google AI Studio\r\n- Choose the right model based on your rate limit needs\r\n- Official rate limits: https://ai.google.dev/gemini-api/docs/rate-limits\r\n\r\n---",
    "Grounding with Google Search": "Grounding connects the model to real-time web information, reducing hallucinations and providing up-to-date, fact-checked responses with citations.\r\n\r\n### How It Works\r\n\r\n1. Model determines if it needs current information\r\n2. Automatically performs Google Search\r\n3. Processes search results\r\n4. Incorporates findings into response\r\n5. Provides citations and source URLs\r\n\r\n### Benefits\r\n\r\n- **Real-time information**: Access to current events and data\r\n- **Reduced hallucinations**: Answers grounded in web sources\r\n- **Verifiable**: Citations allow fact-checking\r\n- **Up-to-date**: Not limited to model's training cutoff\r\n\r\n### Two Grounding APIs\r\n\r\n#### 1. Google Search (`googleSearch`) - Recommended for Gemini 2.5\r\n\r\n```typescript\r\nconst groundingTool = {\r\n  googleSearch: {}\r\n};\r\n```\r\n\r\n**Features:**\r\n- Simple configuration\r\n- Automatic search when needed\r\n- Available on all Gemini 2.5 models\r\n\r\n#### 2. Google Search Retrieval (`googleSearchRetrieval`) - Legacy (Gemini 1.5)\r\n\r\n```typescript\r\nconst retrievalTool = {\r\n  googleSearchRetrieval: {\r\n    dynamicRetrievalConfig: {\r\n      mode: 'MODE_DYNAMIC',\r\n      dynamicThreshold: 0.7 // Only search if confidence < 70%\r\n    }\r\n  }\r\n};\r\n```\r\n\r\n**Features:**\r\n- Dynamic threshold control\r\n- Used with Gemini 1.5 models\r\n- More configuration options\r\n\r\n### Basic Grounding (SDK) - Gemini 2.5\r\n\r\n```typescript\r\nimport { GoogleGenAI } from '@google/genai';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Who won the euro 2024?',\r\n  config: {\r\n    tools: [{ googleSearch: {} }]\r\n  }\r\n});\r\n\r\nconsole.log(response.text);\r\n\r\n// Check if grounding was used\r\nif (response.candidates[0].groundingMetadata) {\r\n  console.log('Search was performed!');\r\n  console.log('Sources:', response.candidates[0].groundingMetadata);\r\n}\r\n```\r\n\r\n### Basic Grounding (Fetch) - Gemini 2.5\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [\r\n        { parts: [{ text: 'Who won the euro 2024?' }] }\r\n      ],\r\n      tools: [\r\n        { google_search: {} }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\nconsole.log(data.candidates[0].content.parts[0].text);\r\n\r\nif (data.candidates[0].groundingMetadata) {\r\n  console.log('Grounding metadata:', data.candidates[0].groundingMetadata);\r\n}\r\n```\r\n\r\n### Dynamic Retrieval (SDK) - Gemini 1.5\r\n\r\n```typescript\r\nimport { GoogleGenAI, DynamicRetrievalConfigMode } from '@google/genai';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-1.5-flash',\r\n  contents: 'Who won the euro 2024?',\r\n  config: {\r\n    tools: [\r\n      {\r\n        googleSearchRetrieval: {\r\n          dynamicRetrievalConfig: {\r\n            mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,\r\n            dynamicThreshold: 0.7 // Search only if confidence < 70%\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n});\r\n\r\nconsole.log(response.text);\r\n\r\nif (!response.candidates[0].groundingMetadata) {\r\n  console.log('Model answered from its own knowledge (high confidence)');\r\n}\r\n```\r\n\r\n### Grounding Metadata Structure\r\n\r\n```typescript\r\n{\r\n  groundingMetadata: {\r\n    searchQueries: [\r\n      { text: \"euro 2024 winner\" }\r\n    ],\r\n    webPages: [\r\n      {\r\n        url: \"https://example.com/euro-2024-results\",\r\n        title: \"UEFA Euro 2024 Final Results\",\r\n        snippet: \"Spain won UEFA Euro 2024...\"\r\n      }\r\n    ],\r\n    citations: [\r\n      {\r\n        startIndex: 42,\r\n        endIndex: 47,\r\n        uri: \"https://example.com/euro-2024-results\"\r\n      }\r\n    ],\r\n    retrievalQueries: [\r\n      {\r\n        query: \"who won euro 2024 final\"\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n### Chat with Grounding (SDK)\r\n\r\n```typescript\r\nconst chat = await ai.chats.create({\r\n  model: 'gemini-2.5-flash',\r\n  config: {\r\n    tools: [{ googleSearch: {} }]\r\n  }\r\n});\r\n\r\nlet response = await chat.sendMessage('What are the latest developments in quantum computing?');\r\nconsole.log(response.text);\r\n\r\n// Check grounding sources\r\nif (response.candidates[0].groundingMetadata) {\r\n  const sources = response.candidates[0].groundingMetadata.webPages || [];\r\n  console.log(`Sources used: ${sources.length}`);\r\n  sources.forEach(source => {\r\n    console.log(`- ${source.title}: ${source.url}`);\r\n  });\r\n}\r\n\r\n// Follow-up still has grounding enabled\r\nresponse = await chat.sendMessage('Which company made the biggest breakthrough?');\r\nconsole.log(response.text);\r\n```\r\n\r\n### Combining Grounding with Function Calling\r\n\r\n```typescript\r\nconst weatherFunction = {\r\n  name: 'get_current_weather',\r\n  description: 'Get current weather for a location',\r\n  parametersJsonSchema: {\r\n    type: 'object',\r\n    properties: {\r\n      location: { type: 'string', description: 'City name' }\r\n    },\r\n    required: ['location']\r\n  }\r\n};\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'What is the weather like in the city that won Euro 2024?',\r\n  config: {\r\n    tools: [\r\n      { googleSearch: {} },\r\n      { functionDeclarations: [weatherFunction] }\r\n    ]\r\n  }\r\n});\r\n\r\n// Model will:\r\n// 1. Use Google Search to find Euro 2024 winner\r\n// 2. Call get_current_weather function with the city\r\n// 3. Combine both results in response\r\n```\r\n\r\n### Checking if Grounding was Used\r\n\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'What is 2+2?', // Model knows this without search\r\n  config: {\r\n    tools: [{ googleSearch: {} }]\r\n  }\r\n});\r\n\r\nif (!response.candidates[0].groundingMetadata) {\r\n  console.log('Model answered from its own knowledge (no search needed)');\r\n} else {\r\n  console.log('Search was performed');\r\n}\r\n```\r\n\r\n### Key Points\r\n\r\n**When to Use Grounding:**\r\n- Current events and news\r\n- Real-time data (stock prices, sports scores, weather)\r\n- Fact-checking and verification\r\n- Questions about recent developments\r\n- Information beyond model's training cutoff\r\n\r\n**When NOT to Use:**\r\n- General knowledge questions\r\n- Mathematical calculations\r\n- Code generation\r\n- Creative writing\r\n- Tasks requiring internal reasoning only\r\n\r\n**Cost Considerations:**\r\n- Grounding adds latency (search takes time)\r\n- Additional token costs for retrieved content\r\n- Use `dynamicThreshold` to control when searches happen (Gemini 1.5)\r\n\r\n**Important Notes:**\r\n- Grounding requires **Google Cloud project** (not just API key)\r\n- Search results quality depends on query phrasing\r\n- Citations may not cover all facts in response\r\n- Search is performed automatically based on confidence\r\n\r\n**Gemini 2.5 vs 1.5:**\r\n- **Gemini 2.5**: Use `googleSearch` (simple, recommended)\r\n- **Gemini 1.5**: Use `googleSearchRetrieval` with `dynamicThreshold`\r\n\r\n**Best Practices:**\r\n- Always check `groundingMetadata` to see if search was used\r\n- Display citations to users for transparency\r\n- Use specific, well-phrased questions for better search results\r\n- Combine with function calling for hybrid workflows\r\n\r\n---",
    "Generation Configuration": "Customize model behavior with generation parameters.\r\n\r\n### All Configuration Options (SDK)\r\n\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Write a creative story',\r\n  config: {\r\n    temperature: 0.9,           // Randomness (0.0-2.0, default: 1.0)\r\n    topP: 0.95,                 // Nucleus sampling (0.0-1.0)\r\n    topK: 40,                   // Top-k sampling\r\n    maxOutputTokens: 2048,      // Max tokens to generate\r\n    stopSequences: ['END'],     // Stop generation if these appear\r\n    responseMimeType: 'text/plain', // Or 'application/json' for JSON mode\r\n    candidateCount: 1           // Number of response candidates (usually 1)\r\n  }\r\n});\r\n```\r\n\r\n### All Configuration Options (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [{ parts: [{ text: 'Write a creative story' }] }],\r\n      generationConfig: {\r\n        temperature: 0.9,\r\n        topP: 0.95,\r\n        topK: 40,\r\n        maxOutputTokens: 2048,\r\n        stopSequences: ['END'],\r\n        responseMimeType: 'text/plain',\r\n        candidateCount: 1\r\n      }\r\n    }),\r\n  }\r\n);\r\n```\r\n\r\n### Parameter Guidelines\r\n\r\n| Parameter | Range | Default | Use Case |\r\n|-----------|-------|---------|----------|\r\n| **temperature** | 0.0-2.0 | 1.0 | Lower = more focused, higher = more creative |\r\n| **topP** | 0.0-1.0 | 0.95 | Nucleus sampling threshold |\r\n| **topK** | 1-100+ | 40 | Limit to top K tokens |\r\n| **maxOutputTokens** | 1-65536 | Model max | Control response length |\r\n| **stopSequences** | Array | None | Stop generation at specific strings |\r\n\r\n**Tips:**\r\n- For **factual tasks**: Use low temperature (0.0-0.3)\r\n- For **creative tasks**: Use high temperature (0.7-1.5)\r\n- **topP** and **topK** both control randomness; use one or the other (not both)\r\n- Always set **maxOutputTokens** to prevent excessive generation\r\n\r\n---",
    "Multi-turn Chat": "For conversations with history, use the SDK's chat helpers or manually manage conversation state.\r\n\r\n### SDK Chat Helpers (Recommended)\r\n\r\n```typescript\r\nconst chat = await ai.models.createChat({\r\n  model: 'gemini-2.5-flash',\r\n  systemInstruction: 'You are a helpful coding assistant.',\r\n  history: [] // Start empty or with previous messages\r\n});\r\n\r\n// Send first message\r\nconst response1 = await chat.sendMessage('What is TypeScript?');\r\nconsole.log('Assistant:', response1.text);\r\n\r\n// Send follow-up (context is automatically maintained)\r\nconst response2 = await chat.sendMessage('How do I install it?');\r\nconsole.log('Assistant:', response2.text);\r\n\r\n// Get full chat history\r\nconst history = chat.getHistory();\r\nconsole.log('Full conversation:', history);\r\n```\r\n\r\n### Manual Chat Management (Fetch)\r\n\r\n```typescript\r\nconst conversationHistory = [];\r\n\r\n// First turn\r\nconst response1 = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [\r\n        {\r\n          role: 'user',\r\n          parts: [{ text: 'What is TypeScript?' }]\r\n        }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data1 = await response1.json();\r\nconst assistantReply1 = data1.candidates[0].content.parts[0].text;\r\n\r\n// Add to history\r\nconversationHistory.push(\r\n  { role: 'user', parts: [{ text: 'What is TypeScript?' }] },\r\n  { role: 'model', parts: [{ text: assistantReply1 }] }\r\n);\r\n\r\n// Second turn (include full history)\r\nconst response2 = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [\r\n        ...conversationHistory,\r\n        { role: 'user', parts: [{ text: 'How do I install it?' }] }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n```\r\n\r\n**Message Roles:**\r\n- `user`: User messages\r\n- `model`: Assistant responses\r\n\r\n**⚠️ Important**: Chat helpers are **SDK-only**. With fetch, you must manually manage conversation history.\r\n\r\n---",
    "Context Caching": "Context caching allows you to cache frequently used content (like system instructions, large documents, or video files) to reduce costs by **up to 90%** and improve latency.\r\n\r\n### How It Works\r\n\r\n1. **Create a cache** with your repeated content\r\n2. **Reference the cache** in subsequent requests\r\n3. **Save tokens** - cached tokens cost significantly less\r\n4. **TTL management** - caches expire after specified time\r\n\r\n### Benefits\r\n\r\n- **Cost savings**: Up to 90% reduction on cached tokens\r\n- **Reduced latency**: Faster responses by reusing processed content\r\n- **Consistent context**: Same large context across multiple requests\r\n\r\n### Cache Creation (SDK)\r\n\r\n```typescript\r\nimport { GoogleGenAI } from '@google/genai';\r\nimport fs from 'fs';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\n// Create a cache for a large document\r\nconst documentText = fs.readFileSync('./large-document.txt', 'utf-8');\r\n\r\nconst cache = await ai.caches.create({\r\n  model: 'gemini-2.5-flash',\r\n  config: {\r\n    displayName: 'large-doc-cache', // Identifier for the cache\r\n    systemInstruction: 'You are an expert at analyzing legal documents.',\r\n    contents: documentText,\r\n    ttl: '3600s', // Cache for 1 hour\r\n  }\r\n});\r\n\r\nconsole.log('Cache created:', cache.name);\r\nconsole.log('Expires at:', cache.expireTime);\r\n```\r\n\r\n### Cache Creation (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  'https://generativelanguage.googleapis.com/v1beta/cachedContents',\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      model: 'models/gemini-2.5-flash',\r\n      displayName: 'large-doc-cache',\r\n      systemInstruction: {\r\n        parts: [{ text: 'You are an expert at analyzing legal documents.' }]\r\n      },\r\n      contents: [\r\n        { parts: [{ text: documentText }] }\r\n      ],\r\n      ttl: '3600s'\r\n    }),\r\n  }\r\n);\r\n\r\nconst cache = await response.json();\r\nconsole.log('Cache created:', cache.name);\r\n```\r\n\r\n### Using a Cache (SDK)\r\n\r\n```typescript\r\n// Generate content using the cache\r\nconst response = await ai.models.generateContent({\r\n  model: cache.name, // Use cache name as model\r\n  contents: 'Summarize the key points in the document'\r\n});\r\n\r\nconsole.log(response.text);\r\n```\r\n\r\n### Using a Cache (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/${cache.name}:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [\r\n        { parts: [{ text: 'Summarize the key points in the document' }] }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\nconsole.log(data.candidates[0].content.parts[0].text);\r\n```\r\n\r\n### Update Cache TTL (SDK)\r\n\r\n```typescript\r\nimport { UpdateCachedContentConfig } from '@google/genai';\r\n\r\nawait ai.caches.update({\r\n  name: cache.name,\r\n  config: {\r\n    ttl: '7200s' // Extend to 2 hours\r\n  }\r\n});\r\n```\r\n\r\n### Update Cache with Expiration Time (SDK)\r\n\r\n```typescript\r\n// Set specific expiration time (must be timezone-aware)\r\nconst in10Minutes = new Date(Date.now() + 10 * 60 * 1000);\r\n\r\nawait ai.caches.update({\r\n  name: cache.name,\r\n  config: {\r\n    expireTime: in10Minutes\r\n  }\r\n});\r\n```\r\n\r\n### List and Delete Caches (SDK)\r\n\r\n```typescript\r\n// List all caches\r\nconst caches = await ai.caches.list();\r\nfor (const cache of caches) {\r\n  console.log(cache.name, cache.displayName);\r\n}\r\n\r\n// Delete a specific cache\r\nawait ai.caches.delete({ name: cache.name });\r\n```\r\n\r\n### Caching with Video Files\r\n\r\n```typescript\r\nimport { GoogleGenAI } from '@google/genai';\r\nimport fs from 'fs';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\n// Upload video file\r\nconst videoFile = await ai.files.upload({\r\n  file: fs.createReadStream('./video.mp4')\r\n});\r\n\r\n// Wait for processing\r\nwhile (videoFile.state.name === 'PROCESSING') {\r\n  await new Promise(resolve => setTimeout(resolve, 2000));\r\n  videoFile = await ai.files.get({ name: videoFile.name });\r\n}\r\n\r\n// Create cache with video\r\nconst cache = await ai.caches.create({\r\n  model: 'gemini-2.5-flash',\r\n  config: {\r\n    displayName: 'video-analysis-cache',\r\n    systemInstruction: 'You are an expert video analyzer.',\r\n    contents: [videoFile],\r\n    ttl: '300s' // 5 minutes\r\n  }\r\n});\r\n\r\n// Use cache for multiple queries\r\nconst response1 = await ai.models.generateContent({\r\n  model: cache.name,\r\n  contents: 'What happens in the first minute?'\r\n});\r\n\r\nconst response2 = await ai.models.generateContent({\r\n  model: cache.name,\r\n  contents: 'Describe the main characters'\r\n});\r\n```\r\n\r\n### Key Points\r\n\r\n**When to Use Caching:**\r\n- Large system instructions used repeatedly\r\n- Long documents analyzed multiple times\r\n- Video/audio files queried with different prompts\r\n- Consistent context across conversation sessions\r\n\r\n**TTL Guidelines:**\r\n- Short sessions: 300s (5 min) to 3600s (1 hour)\r\n- Long sessions: 3600s (1 hour) to 86400s (24 hours)\r\n- Maximum: 7 days\r\n\r\n**Cost Savings:**\r\n- Cached input tokens: ~90% cheaper than regular tokens\r\n- Output tokens: Same price (not cached)\r\n\r\n**Important:**\r\n- You must use explicit model version suffixes (e.g., `gemini-2.5-flash-001`, NOT just `gemini-2.5-flash`)\r\n- Caches are automatically deleted after TTL expires\r\n- Update TTL before expiration to extend cache lifetime\r\n\r\n---",
    "Error Handling": "### Common Errors\r\n\r\n#### 1. Invalid API Key (401)\r\n\r\n```typescript\r\n{\r\n  error: {\r\n    code: 401,\r\n    message: 'API key not valid. Please pass a valid API key.',\r\n    status: 'UNAUTHENTICATED'\r\n  }\r\n}\r\n```\r\n\r\n**Solution**: Verify `GEMINI_API_KEY` environment variable is set correctly.\r\n\r\n#### 2. Rate Limit Exceeded (429)\r\n\r\n```typescript\r\n{\r\n  error: {\r\n    code: 429,\r\n    message: 'Resource has been exhausted (e.g. check quota).',\r\n    status: 'RESOURCE_EXHAUSTED'\r\n  }\r\n}\r\n```\r\n\r\n**Solution**: Implement exponential backoff retry strategy.\r\n\r\n#### 3. Model Not Found (404)\r\n\r\n```typescript\r\n{\r\n  error: {\r\n    code: 404,\r\n    message: 'models/gemini-3.0-flash is not found',\r\n    status: 'NOT_FOUND'\r\n  }\r\n}\r\n```\r\n\r\n**Solution**: Use correct model names: `gemini-2.5-pro`, `gemini-2.5-flash`, `gemini-2.5-flash-lite`\r\n\r\n#### 4. Context Length Exceeded (400)\r\n\r\n```typescript\r\n{\r\n  error: {\r\n    code: 400,\r\n    message: 'Request payload size exceeds the limit',\r\n    status: 'INVALID_ARGUMENT'\r\n  }\r\n}\r\n```\r\n\r\n**Solution**: Reduce input size. Gemini 2.5 models support 1,048,576 input tokens max.\r\n\r\n### Exponential Backoff Pattern\r\n\r\n```typescript\r\nasync function generateWithRetry(request, maxRetries = 3) {\r\n  for (let i = 0; i < maxRetries; i++) {\r\n    try {\r\n      return await ai.models.generateContent(request);\r\n    } catch (error) {\r\n      if (error.status === 429 && i < maxRetries - 1) {\r\n        const delay = Math.pow(2, i) * 1000; // 1s, 2s, 4s\r\n        await new Promise(resolve => setTimeout(resolve, delay));\r\n        continue;\r\n      }\r\n      throw error;\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n---",
    "Text Generation": "### Basic Text Generation (SDK)\r\n\r\n```typescript\r\nimport { GoogleGenAI } from '@google/genai';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Write a haiku about artificial intelligence'\r\n});\r\n\r\nconsole.log(response.text);\r\n```\r\n\r\n### Basic Text Generation (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [\r\n        {\r\n          parts: [\r\n            { text: 'Write a haiku about artificial intelligence' }\r\n          ]\r\n        }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\nconsole.log(data.candidates[0].content.parts[0].text);\r\n```\r\n\r\n### Response Structure\r\n\r\n```typescript\r\n{\r\n  text: string,                  // Convenience accessor for text content\r\n  candidates: [\r\n    {\r\n      content: {\r\n        parts: [\r\n          { text: string }       // Generated text\r\n        ],\r\n        role: string             // \"model\"\r\n      },\r\n      finishReason: string,      // \"STOP\" | \"MAX_TOKENS\" | \"SAFETY\" | \"OTHER\"\r\n      index: number\r\n    }\r\n  ],\r\n  usageMetadata: {\r\n    promptTokenCount: number,\r\n    candidatesTokenCount: number,\r\n    totalTokenCount: number\r\n  }\r\n}\r\n```\r\n\r\n---",
    "⚠️ CRITICAL SDK MIGRATION WARNING": "**DEPRECATED SDK**: `@google/generative-ai` (sunset November 30, 2025)\r\n**CURRENT SDK**: `@google/genai` v1.27+\r\n\r\n**If you see code using `@google/generative-ai`, it's outdated!**\r\n\r\nThis skill uses the **correct current SDK** and provides a complete migration guide.\r\n\r\n---",
    "SDK vs Fetch Approaches": "### Node.js SDK (@google/genai)\r\n\r\n**Pros:**\r\n- Type-safe with TypeScript\r\n- Easier API (simpler syntax)\r\n- Built-in chat helpers\r\n- Automatic SSE parsing for streaming\r\n- Better error handling\r\n\r\n**Cons:**\r\n- Requires Node.js or compatible runtime\r\n- Larger bundle size\r\n- May not work in all edge runtimes\r\n\r\n**Use when:** Building Node.js apps, Next.js Server Actions/Components, or any environment with Node.js compatibility\r\n\r\n### Fetch-based (Direct REST API)\r\n\r\n**Pros:**\r\n- Works in **any** JavaScript environment (Cloudflare Workers, Deno, Bun, browsers)\r\n- Minimal dependencies\r\n- Smaller bundle size\r\n- Full control over requests\r\n\r\n**Cons:**\r\n- More verbose syntax\r\n- Manual SSE parsing for streaming\r\n- No built-in chat helpers\r\n- Manual error handling\r\n\r\n**Use when:** Deploying to Cloudflare Workers, browser clients, or lightweight edge runtimes\r\n\r\n---",
    "Current Models (2025)": "### Gemini 2.5 Series (General Availability)\r\n\r\n#### gemini-2.5-pro\r\n- **Context**: 1,048,576 input tokens / 65,536 output tokens\r\n- **Description**: State-of-the-art thinking model for complex reasoning\r\n- **Best for**: Code, math, STEM, complex problem-solving\r\n- **Features**: Thinking mode (default on), function calling, multimodal, streaming\r\n- **Knowledge cutoff**: January 2025\r\n\r\n#### gemini-2.5-flash\r\n- **Context**: 1,048,576 input tokens / 65,536 output tokens\r\n- **Description**: Best price-performance workhorse model\r\n- **Best for**: Large-scale processing, low-latency, high-volume, agentic use cases\r\n- **Features**: Thinking mode (default on), function calling, multimodal, streaming\r\n- **Knowledge cutoff**: January 2025\r\n\r\n#### gemini-2.5-flash-lite\r\n- **Context**: 1,048,576 input tokens / 65,536 output tokens\r\n- **Description**: Cost-optimized, fastest 2.5 model\r\n- **Best for**: High throughput, cost-sensitive applications\r\n- **Features**: Thinking mode (default on), function calling, multimodal, streaming\r\n- **Knowledge cutoff**: January 2025\r\n\r\n### Model Feature Matrix\r\n\r\n| Feature | Pro | Flash | Flash-Lite |\r\n|---------|-----|-------|------------|\r\n| Thinking Mode | ✅ Default ON | ✅ Default ON | ✅ Default ON |\r\n| Function Calling | ✅ | ✅ | ✅ |\r\n| Multimodal | ✅ | ✅ | ✅ |\r\n| Streaming | ✅ | ✅ | ✅ |\r\n| System Instructions | ✅ | ✅ | ✅ |\r\n| Context Window | 1,048,576 in | 1,048,576 in | 1,048,576 in |\r\n| Output Tokens | 65,536 max | 65,536 max | 65,536 max |\r\n\r\n### ⚠️ Context Window Correction\r\n\r\n**ACCURATE**: Gemini 2.5 models support **1,048,576 input tokens** (NOT 2M!)\r\n**OUTDATED**: Only Gemini 1.5 Pro (previous generation) had 2M token context window\r\n\r\n**Common mistake**: Claiming Gemini 2.5 has 2M tokens. It doesn't. This skill prevents this error.\r\n\r\n---",
    "Multimodal Inputs": "Gemini 2.5 models support text + images + video + audio + PDFs in the same request.\r\n\r\n### Images (Vision)\r\n\r\n#### SDK Approach\r\n\r\n```typescript\r\nimport { GoogleGenAI } from '@google/genai';\r\nimport fs from 'fs';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\n// From file\r\nconst imageData = fs.readFileSync('/path/to/image.jpg');\r\nconst base64Image = imageData.toString('base64');\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: [\r\n    {\r\n      parts: [\r\n        { text: 'What is in this image?' },\r\n        {\r\n          inlineData: {\r\n            data: base64Image,\r\n            mimeType: 'image/jpeg'\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n\r\nconsole.log(response.text);\r\n```\r\n\r\n#### Fetch Approach\r\n\r\n```typescript\r\nconst imageData = fs.readFileSync('/path/to/image.jpg');\r\nconst base64Image = imageData.toString('base64');\r\n\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [\r\n        {\r\n          parts: [\r\n            { text: 'What is in this image?' },\r\n            {\r\n              inlineData: {\r\n                data: base64Image,\r\n                mimeType: 'image/jpeg'\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\nconsole.log(data.candidates[0].content.parts[0].text);\r\n```\r\n\r\n**Supported Image Formats:**\r\n- JPEG (`.jpg`, `.jpeg`)\r\n- PNG (`.png`)\r\n- WebP (`.webp`)\r\n- HEIC (`.heic`)\r\n- HEIF (`.heif`)\r\n\r\n**Max Image Size**: 20MB per image\r\n\r\n### Video\r\n\r\n```typescript\r\n// Video must be < 2 minutes for inline data\r\nconst videoData = fs.readFileSync('/path/to/video.mp4');\r\nconst base64Video = videoData.toString('base64');\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: [\r\n    {\r\n      parts: [\r\n        { text: 'Describe what happens in this video' },\r\n        {\r\n          inlineData: {\r\n            data: base64Video,\r\n            mimeType: 'video/mp4'\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n\r\nconsole.log(response.text);\r\n```\r\n\r\n**Supported Video Formats:**\r\n- MP4 (`.mp4`)\r\n- MPEG (`.mpeg`)\r\n- MOV (`.mov`)\r\n- AVI (`.avi`)\r\n- FLV (`.flv`)\r\n- MPG (`.mpg`)\r\n- WebM (`.webm`)\r\n- WMV (`.wmv`)\r\n\r\n**Max Video Length (inline)**: 2 minutes\r\n**Max Video Size**: 2GB (use File API for larger files - Phase 2)\r\n\r\n### Audio\r\n\r\n```typescript\r\nconst audioData = fs.readFileSync('/path/to/audio.mp3');\r\nconst base64Audio = audioData.toString('base64');\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: [\r\n    {\r\n      parts: [\r\n        { text: 'Transcribe and summarize this audio' },\r\n        {\r\n          inlineData: {\r\n            data: base64Audio,\r\n            mimeType: 'audio/mp3'\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n\r\nconsole.log(response.text);\r\n```\r\n\r\n**Supported Audio Formats:**\r\n- MP3 (`.mp3`)\r\n- WAV (`.wav`)\r\n- FLAC (`.flac`)\r\n- AAC (`.aac`)\r\n- OGG (`.ogg`)\r\n- OPUS (`.opus`)\r\n\r\n**Max Audio Size**: 20MB\r\n\r\n### PDFs\r\n\r\n```typescript\r\nconst pdfData = fs.readFileSync('/path/to/document.pdf');\r\nconst base64Pdf = pdfData.toString('base64');\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: [\r\n    {\r\n      parts: [\r\n        { text: 'Summarize the key points in this PDF' },\r\n        {\r\n          inlineData: {\r\n            data: base64Pdf,\r\n            mimeType: 'application/pdf'\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n\r\nconsole.log(response.text);\r\n```\r\n\r\n**Max PDF Size**: 30MB\r\n**PDF Limitations**: Text-based PDFs work best; scanned images may have lower accuracy\r\n\r\n### Multiple Inputs\r\n\r\nYou can combine multiple modalities in one request:\r\n\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: [\r\n    {\r\n      parts: [\r\n        { text: 'Compare these two images and describe the differences:' },\r\n        { inlineData: { data: base64Image1, mimeType: 'image/jpeg' } },\r\n        { inlineData: { data: base64Image2, mimeType: 'image/jpeg' } }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n```\r\n\r\n---",
    "Production Best Practices": "### 1. Always Do\r\n\r\n✅ **Use @google/genai** (NOT @google/generative-ai)\r\n✅ **Set maxOutputTokens** to prevent excessive generation\r\n✅ **Implement rate limit handling** with exponential backoff\r\n✅ **Use environment variables** for API keys (never hardcode)\r\n✅ **Validate inputs** before sending to API (save costs)\r\n✅ **Use streaming** for better UX on long responses\r\n✅ **Choose the right model** based on your needs (Pro for complex reasoning, Flash for balance, Flash-Lite for speed)\r\n✅ **Handle errors gracefully** with try-catch\r\n✅ **Monitor token usage** for cost control\r\n✅ **Use correct model names**: gemini-2.5-pro/flash/flash-lite\r\n\r\n### 2. Never Do\r\n\r\n❌ **Never use @google/generative-ai** (deprecated!)\r\n❌ **Never hardcode API keys** in code\r\n❌ **Never claim 2M context** for Gemini 2.5 (it's 1,048,576 input tokens)\r\n❌ **Never expose API keys** in client-side code\r\n❌ **Never skip error handling** (always try-catch)\r\n❌ **Never use generic rate limits** (each model has different limits - check official docs)\r\n❌ **Never send PII** without user consent\r\n❌ **Never trust user input** without validation\r\n❌ **Never ignore rate limits** (will get 429 errors)\r\n❌ **Never use old model names** like gemini-1.5-pro (use 2.5 models)\r\n\r\n### 3. Security\r\n\r\n- **API Key Storage**: Use environment variables or secret managers\r\n- **Server-Side Only**: Never expose API keys in browser JavaScript\r\n- **Input Validation**: Sanitize all user inputs before API calls\r\n- **Rate Limiting**: Implement your own rate limits to prevent abuse\r\n- **Error Messages**: Don't expose API keys or sensitive data in error logs\r\n\r\n### 4. Cost Optimization\r\n\r\n- **Choose Right Model**: Use Flash for most tasks, Pro only when needed\r\n- **Set Token Limits**: Use maxOutputTokens to control costs\r\n- **Batch Requests**: Process multiple items efficiently\r\n- **Cache Results**: Store responses when appropriate\r\n- **Monitor Usage**: Track token consumption in Google Cloud Console\r\n\r\n### 5. Performance\r\n\r\n- **Use Streaming**: Better perceived latency for long responses\r\n- **Parallel Requests**: Use Promise.all() for independent calls\r\n- **Edge Deployment**: Deploy to Cloudflare Workers for low latency\r\n- **Connection Pooling**: Reuse HTTP connections when possible\r\n\r\n---",
    "Thinking Mode": "Gemini 2.5 models have **thinking mode enabled by default** for enhanced quality. You can configure the thinking budget.\r\n\r\n### Configure Thinking Budget (SDK)\r\n\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Solve this complex math problem: ...',\r\n  config: {\r\n    thinkingConfig: {\r\n      thinkingBudget: 8192 // Max tokens for thinking (default: model-dependent)\r\n    }\r\n  }\r\n});\r\n```\r\n\r\n### Configure Thinking Budget (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [{ parts: [{ text: 'Solve this complex math problem: ...' }] }],\r\n      generationConfig: {\r\n        thinkingConfig: {\r\n          thinkingBudget: 8192\r\n        }\r\n      }\r\n    }),\r\n  }\r\n);\r\n```\r\n\r\n**Key Points:**\r\n- Thinking mode is **always enabled** on Gemini 2.5 models (cannot be disabled)\r\n- Higher thinking budgets allow more internal reasoning (may increase latency)\r\n- Default budget varies by model (usually sufficient for most tasks)\r\n- Only increase budget for very complex reasoning tasks\r\n\r\n---",
    "Function Calling": "Gemini supports function calling (tool use) to connect models with external APIs and systems.\r\n\r\n### Basic Function Calling (SDK)\r\n\r\n```typescript\r\nimport { GoogleGenAI, FunctionCallingConfigMode } from '@google/genai';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\n// Define function declarations\r\nconst getCurrentWeather = {\r\n  name: 'get_current_weather',\r\n  description: 'Get the current weather for a location',\r\n  parametersJsonSchema: {\r\n    type: 'object',\r\n    properties: {\r\n      location: {\r\n        type: 'string',\r\n        description: 'City name, e.g. San Francisco'\r\n      },\r\n      unit: {\r\n        type: 'string',\r\n        enum: ['celsius', 'fahrenheit']\r\n      }\r\n    },\r\n    required: ['location']\r\n  }\r\n};\r\n\r\n// Make request with tools\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'What\\'s the weather in Tokyo?',\r\n  config: {\r\n    tools: [\r\n      { functionDeclarations: [getCurrentWeather] }\r\n    ]\r\n  }\r\n});\r\n\r\n// Check if model wants to call a function\r\nconst functionCall = response.candidates[0].content.parts[0].functionCall;\r\n\r\nif (functionCall) {\r\n  console.log('Function to call:', functionCall.name);\r\n  console.log('Arguments:', functionCall.args);\r\n\r\n  // Execute the function (your implementation)\r\n  const weatherData = await fetchWeather(functionCall.args.location);\r\n\r\n  // Send function result back to model\r\n  const finalResponse = await ai.models.generateContent({\r\n    model: 'gemini-2.5-flash',\r\n    contents: [\r\n      'What\\'s the weather in Tokyo?',\r\n      response.candidates[0].content, // Original assistant response with function call\r\n      {\r\n        parts: [\r\n          {\r\n            functionResponse: {\r\n              name: functionCall.name,\r\n              response: weatherData\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    ],\r\n    config: {\r\n      tools: [\r\n        { functionDeclarations: [getCurrentWeather] }\r\n      ]\r\n    }\r\n  });\r\n\r\n  console.log(finalResponse.text);\r\n}\r\n```\r\n\r\n### Function Calling (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [\r\n        { parts: [{ text: 'What\\'s the weather in Tokyo?' }] }\r\n      ],\r\n      tools: [\r\n        {\r\n          functionDeclarations: [\r\n            {\r\n              name: 'get_current_weather',\r\n              description: 'Get the current weather for a location',\r\n              parameters: {\r\n                type: 'object',\r\n                properties: {\r\n                  location: {\r\n                    type: 'string',\r\n                    description: 'City name'\r\n                  }\r\n                },\r\n                required: ['location']\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\nconst functionCall = data.candidates[0]?.content?.parts[0]?.functionCall;\r\n\r\nif (functionCall) {\r\n  // Execute function and send result back (same flow as SDK)\r\n}\r\n```\r\n\r\n### Parallel Function Calling\r\n\r\nGemini can call multiple independent functions simultaneously:\r\n\r\n```typescript\r\nconst tools = [\r\n  {\r\n    functionDeclarations: [\r\n      {\r\n        name: 'get_weather',\r\n        description: 'Get weather for a location',\r\n        parametersJsonSchema: {\r\n          type: 'object',\r\n          properties: {\r\n            location: { type: 'string' }\r\n          },\r\n          required: ['location']\r\n        }\r\n      },\r\n      {\r\n        name: 'get_population',\r\n        description: 'Get population of a city',\r\n        parametersJsonSchema: {\r\n          type: 'object',\r\n          properties: {\r\n            city: { type: 'string' }\r\n          },\r\n          required: ['city']\r\n        }\r\n      }\r\n    ]\r\n  }\r\n];\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'What is the weather and population of Tokyo?',\r\n  config: { tools }\r\n});\r\n\r\n// Model may return MULTIPLE function calls in parallel\r\nconst functionCalls = response.candidates[0].content.parts.filter(\r\n  part => part.functionCall\r\n);\r\n\r\nconsole.log(`Model wants to call ${functionCalls.length} functions in parallel`);\r\n```\r\n\r\n### Function Calling Modes\r\n\r\n```typescript\r\nimport { FunctionCallingConfigMode } from '@google/genai';\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'What\\'s the weather?',\r\n  config: {\r\n    tools: [{ functionDeclarations: [getCurrentWeather] }],\r\n    toolConfig: {\r\n      functionCallingConfig: {\r\n        mode: FunctionCallingConfigMode.ANY, // Force function call\r\n        // mode: FunctionCallingConfigMode.AUTO, // Model decides (default)\r\n        // mode: FunctionCallingConfigMode.NONE, // Never call functions\r\n        allowedFunctionNames: ['get_current_weather'] // Optional: restrict to specific functions\r\n      }\r\n    }\r\n  }\r\n});\r\n```\r\n\r\n**Modes:**\r\n- `AUTO` (default): Model decides whether to call functions\r\n- `ANY`: Force model to call at least one function\r\n- `NONE`: Disable function calling for this request\r\n\r\n---",
    "Streaming": "### Streaming with SDK (Async Iteration)\r\n\r\n```typescript\r\nconst response = await ai.models.generateContentStream({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Write a 200-word story about time travel'\r\n});\r\n\r\nfor await (const chunk of response) {\r\n  process.stdout.write(chunk.text);\r\n}\r\n```\r\n\r\n### Streaming with Fetch (SSE Parsing)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [{ parts: [{ text: 'Write a 200-word story about time travel' }] }]\r\n    }),\r\n  }\r\n);\r\n\r\nconst reader = response.body.getReader();\r\nconst decoder = new TextDecoder();\r\nlet buffer = '';\r\n\r\nwhile (true) {\r\n  const { done, value } = await reader.read();\r\n  if (done) break;\r\n\r\n  buffer += decoder.decode(value, { stream: true });\r\n  const lines = buffer.split('\\n');\r\n  buffer = lines.pop() || '';\r\n\r\n  for (const line of lines) {\r\n    if (line.trim() === '' || line.startsWith('data: [DONE]')) continue;\r\n    if (!line.startsWith('data: ')) continue;\r\n\r\n    try {\r\n      const data = JSON.parse(line.slice(6));\r\n      const text = data.candidates[0]?.content?.parts[0]?.text;\r\n      if (text) {\r\n        process.stdout.write(text);\r\n      }\r\n    } catch (e) {\r\n      // Skip invalid JSON\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Key Points:**\r\n- Use `streamGenerateContent` endpoint (not `generateContent`)\r\n- Parse Server-Sent Events (SSE) format: `data: {json}\\n\\n`\r\n- Handle incomplete chunks in buffer\r\n- Skip empty lines and `[DONE]` markers\r\n\r\n---",
    "Quick Reference": "### Installation\r\n```bash\r\nnpm install @google/genai@1.27.0\r\n```\r\n\r\n### Environment\r\n```bash\r\nexport GEMINI_API_KEY=\"...\"\r\n```\r\n\r\n### Models (2025)\r\n- `gemini-2.5-pro` (1,048,576 in / 65,536 out) - Best for complex reasoning\r\n- `gemini-2.5-flash` (1,048,576 in / 65,536 out) - Best price-performance balance\r\n- `gemini-2.5-flash-lite` (1,048,576 in / 65,536 out) - Fastest, most cost-effective\r\n\r\n### Basic Generation\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Your prompt here'\r\n});\r\nconsole.log(response.text);\r\n```\r\n\r\n### Streaming\r\n```typescript\r\nconst response = await ai.models.generateContentStream({...});\r\nfor await (const chunk of response) {\r\n  console.log(chunk.text);\r\n}\r\n```\r\n\r\n### Multimodal\r\n```typescript\r\ncontents: [\r\n  {\r\n    parts: [\r\n      { text: 'What is this?' },\r\n      { inlineData: { data: base64Image, mimeType: 'image/jpeg' } }\r\n    ]\r\n  }\r\n]\r\n```\r\n\r\n### Function Calling\r\n```typescript\r\nconfig: {\r\n  tools: [{ functionDeclarations: [...] }]\r\n}\r\n```\r\n\r\n---\r\n\r\n**Last Updated**: 2025-10-25\r\n**Production Validated**: All features tested with @google/genai@1.27.0\r\n**Phase**: 2 Complete ✅ (All Core + Advanced Features)",
    "System Instructions": "System instructions guide the model's behavior and set context. They are **separate** from the conversation messages.\r\n\r\n### SDK Approach\r\n\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  systemInstruction: 'You are a helpful AI assistant that always responds in the style of a pirate. Use nautical terminology and end sentences with \"arrr\".',\r\n  contents: 'Explain what a database is'\r\n});\r\n\r\nconsole.log(response.text);\r\n// Output: \"Ahoy there! A database be like a treasure chest...\"\r\n```\r\n\r\n### Fetch Approach\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      systemInstruction: {\r\n        parts: [\r\n          { text: 'You are a helpful AI assistant that always responds in the style of a pirate.' }\r\n        ]\r\n      },\r\n      contents: [\r\n        { parts: [{ text: 'Explain what a database is' }] }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n```\r\n\r\n**Key Points:**\r\n- System instructions are **NOT** part of `contents` array\r\n- They are set once at the **top level** of the request\r\n- They persist for the entire conversation (when using multi-turn chat)\r\n- They don't count as user or model messages\r\n\r\n---",
    "Code Execution": "Gemini models can generate and execute Python code to solve problems requiring computation, data analysis, or visualization.\r\n\r\n### How It Works\r\n\r\n1. Model generates executable Python code\r\n2. Code runs in secure sandbox\r\n3. Results are returned to the model\r\n4. Model incorporates results into response\r\n\r\n### Supported Operations\r\n\r\n- Mathematical calculations\r\n- Data analysis and statistics\r\n- File processing (CSV, JSON, etc.)\r\n- Chart and graph generation\r\n- Algorithm implementation\r\n- Data transformations\r\n\r\n### Available Python Packages\r\n\r\n**Standard Library:**\r\n- `math`, `statistics`, `random`, `datetime`, `json`, `csv`, `re`\r\n- `collections`, `itertools`, `functools`\r\n\r\n**Data Science:**\r\n- `numpy`, `pandas`, `scipy`\r\n\r\n**Visualization:**\r\n- `matplotlib`, `seaborn`\r\n\r\n**Note**: Limited package availability compared to full Python environment\r\n\r\n### Basic Code Execution (SDK)\r\n\r\n```typescript\r\nimport { GoogleGenAI, Tool, ToolCodeExecution } from '@google/genai';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'What is the sum of the first 50 prime numbers? Generate and run code for the calculation.',\r\n  config: {\r\n    tools: [{ codeExecution: {} }]\r\n  }\r\n});\r\n\r\n// Parse response parts\r\nfor (const part of response.candidates[0].content.parts) {\r\n  if (part.text) {\r\n    console.log('Text:', part.text);\r\n  }\r\n  if (part.executableCode) {\r\n    console.log('Generated Code:', part.executableCode.code);\r\n  }\r\n  if (part.codeExecutionResult) {\r\n    console.log('Execution Output:', part.codeExecutionResult.output);\r\n  }\r\n}\r\n```\r\n\r\n### Basic Code Execution (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      tools: [{ code_execution: {} }],\r\n      contents: [\r\n        {\r\n          parts: [\r\n            { text: 'What is the sum of the first 50 prime numbers? Generate and run code.' }\r\n          ]\r\n        }\r\n      ]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\n\r\nfor (const part of data.candidates[0].content.parts) {\r\n  if (part.text) {\r\n    console.log('Text:', part.text);\r\n  }\r\n  if (part.executableCode) {\r\n    console.log('Code:', part.executableCode.code);\r\n  }\r\n  if (part.codeExecutionResult) {\r\n    console.log('Result:', part.codeExecutionResult.output);\r\n  }\r\n}\r\n```\r\n\r\n### Chat with Code Execution (SDK)\r\n\r\n```typescript\r\nconst chat = await ai.chats.create({\r\n  model: 'gemini-2.5-flash',\r\n  config: {\r\n    tools: [{ codeExecution: {} }]\r\n  }\r\n});\r\n\r\nlet response = await chat.sendMessage('I have a math question for you.');\r\nconsole.log(response.text);\r\n\r\nresponse = await chat.sendMessage(\r\n  'Calculate the Fibonacci sequence up to the 20th number and sum them.'\r\n);\r\n\r\n// Model will generate and execute code, then provide answer\r\nfor (const part of response.candidates[0].content.parts) {\r\n  if (part.text) console.log(part.text);\r\n  if (part.executableCode) console.log('Code:', part.executableCode.code);\r\n  if (part.codeExecutionResult) console.log('Output:', part.codeExecutionResult.output);\r\n}\r\n```\r\n\r\n### Data Analysis Example\r\n\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: `\r\n    Analyze this sales data and calculate:\r\n    1. Total revenue\r\n    2. Average sale price\r\n    3. Best-selling month\r\n\r\n    Data (CSV format):\r\n    month,sales,revenue\r\n    Jan,150,45000\r\n    Feb,200,62000\r\n    Mar,175,53000\r\n    Apr,220,68000\r\n  `,\r\n  config: {\r\n    tools: [{ codeExecution: {} }]\r\n  }\r\n});\r\n\r\n// Model will generate pandas/numpy code to analyze data\r\nfor (const part of response.candidates[0].content.parts) {\r\n  if (part.text) console.log(part.text);\r\n  if (part.executableCode) console.log('Analysis Code:', part.executableCode.code);\r\n  if (part.codeExecutionResult) console.log('Results:', part.codeExecutionResult.output);\r\n}\r\n```\r\n\r\n### Visualization Example\r\n\r\n```typescript\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Create a bar chart showing the distribution of prime numbers under 100 by their last digit. Generate the chart and describe the pattern.',\r\n  config: {\r\n    tools: [{ codeExecution: {} }]\r\n  }\r\n});\r\n\r\n// Model generates matplotlib code, executes it, and describes results\r\nfor (const part of response.candidates[0].content.parts) {\r\n  if (part.text) console.log(part.text);\r\n  if (part.executableCode) console.log('Chart Code:', part.executableCode.code);\r\n  if (part.codeExecutionResult) {\r\n    // Note: Chart image data would be in output\r\n    console.log('Execution completed');\r\n  }\r\n}\r\n```\r\n\r\n### Response Structure\r\n\r\n```typescript\r\n{\r\n  candidates: [\r\n    {\r\n      content: {\r\n        parts: [\r\n          { text: \"I'll calculate that for you.\" },\r\n          {\r\n            executableCode: {\r\n              language: \"PYTHON\",\r\n              code: \"def is_prime(n):\\n  if n <= 1:\\n    return False\\n  ...\"\r\n            }\r\n          },\r\n          {\r\n            codeExecutionResult: {\r\n              outcome: \"OUTCOME_OK\", // or \"OUTCOME_FAILED\"\r\n              output: \"5117\\n\"\r\n            }\r\n          },\r\n          { text: \"The sum of the first 50 prime numbers is 5117.\" }\r\n        ]\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Error Handling\r\n\r\n```typescript\r\nfor (const part of response.candidates[0].content.parts) {\r\n  if (part.codeExecutionResult) {\r\n    if (part.codeExecutionResult.outcome === 'OUTCOME_FAILED') {\r\n      console.error('Code execution failed:', part.codeExecutionResult.output);\r\n    } else {\r\n      console.log('Success:', part.codeExecutionResult.output);\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### Key Points\r\n\r\n**When to Use Code Execution:**\r\n- Complex mathematical calculations\r\n- Data analysis and statistics\r\n- Algorithm implementations\r\n- File parsing and processing\r\n- Chart generation\r\n- Computational problems\r\n\r\n**Limitations:**\r\n- Sandbox environment (limited file system access)\r\n- Limited Python package availability\r\n- Execution timeout limits\r\n- No network access from code\r\n- No persistent state between executions\r\n\r\n**Best Practices:**\r\n- Specify what calculation or analysis you need clearly\r\n- Request code generation explicitly (\"Generate and run code...\")\r\n- Check `outcome` field for errors\r\n- Use for deterministic computations, not for general programming\r\n\r\n**Important:**\r\n- Available on all Gemini 2.5 models (Pro, Flash, Flash-Lite)\r\n- Code runs in isolated sandbox for security\r\n- Supports Python with standard library and common data science packages\r\n\r\n---",
    "Status": "**✅ Phase 1 Complete**:\r\n- ✅ Text Generation (basic + streaming)\r\n- ✅ Multimodal Inputs (images, video, audio, PDFs)\r\n- ✅ Function Calling (basic + parallel execution)\r\n- ✅ System Instructions & Multi-turn Chat\r\n- ✅ Thinking Mode Configuration\r\n- ✅ Generation Parameters (temperature, top-p, top-k, stop sequences)\r\n- ✅ Both Node.js SDK (@google/genai) and fetch approaches\r\n\r\n**✅ Phase 2 Complete**:\r\n- ✅ Context Caching (cost optimization with TTL-based caching)\r\n- ✅ Code Execution (built-in Python interpreter and sandbox)\r\n- ✅ Grounding with Google Search (real-time web information + citations)\r\n\r\n**📦 Separate Skills**:\r\n- **Embeddings**: See `google-gemini-embeddings` skill for text-embedding-004\r\n\r\n---",
    "Quick Start": "### Installation\r\n\r\n**CORRECT SDK:**\r\n```bash\r\nnpm install @google/genai@1.27.0\r\n```\r\n\r\n**❌ WRONG (DEPRECATED):**\r\n```bash\r\nnpm install @google/generative-ai  # DO NOT USE!\r\n```\r\n\r\n### Environment Setup\r\n\r\n```bash\r\nexport GEMINI_API_KEY=\"...\"\r\n```\r\n\r\nOr create `.env` file:\r\n```\r\nGEMINI_API_KEY=...\r\n```\r\n\r\n### First Text Generation (Node.js SDK)\r\n\r\n```typescript\r\nimport { GoogleGenAI } from '@google/genai';\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst response = await ai.models.generateContent({\r\n  model: 'gemini-2.5-flash',\r\n  contents: 'Explain quantum computing in simple terms'\r\n});\r\n\r\nconsole.log(response.text);\r\n```\r\n\r\n### First Text Generation (Fetch - Cloudflare Workers)\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent`,\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      'x-goog-api-key': env.GEMINI_API_KEY,\r\n    },\r\n    body: JSON.stringify({\r\n      contents: [{ parts: [{ text: 'Explain quantum computing in simple terms' }] }]\r\n    }),\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\nconsole.log(data.candidates[0].content.parts[0].text);\r\n```\r\n\r\n---"
  }
}