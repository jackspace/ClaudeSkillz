{
  "description": "Comprehensive toolkit for survival analysis and time-to-event modeling in Python using scikit-survival. Use this skill when working with censored survival data, performing time-to-event analysis, fitting Cox models, Random Survival Forests, Gradient Boosting models, or Survival SVMs, evaluating survival predictions with concordance index or Brier score, handling competing risks, or implementing any survival analysis workflow with the scikit-survival library.",
  "references": {
    "files": [
      "references/competing-risks.md",
      "references/cox-models.md",
      "references/data-handling.md",
      "references/ensemble-models.md",
      "references/evaluation-metrics.md",
      "references/svm-models.md"
    ]
  },
  "content": "### 1. Model Types and Selection\r\n\r\nscikit-survival provides multiple model families, each suited for different scenarios:\r\n\r\n#### Cox Proportional Hazards Models\r\n**Use for**: Standard survival analysis with interpretable coefficients\r\n- `CoxPHSurvivalAnalysis`: Basic Cox model\r\n- `CoxnetSurvivalAnalysis`: Penalized Cox with elastic net for high-dimensional data\r\n- `IPCRidge`: Ridge regression for accelerated failure time models\r\n\r\n**See**: `references/cox-models.md` for detailed guidance on Cox models, regularization, and interpretation\r\n\r\n#### Ensemble Methods\r\n**Use for**: High predictive performance with complex non-linear relationships\r\n- `RandomSurvivalForest`: Robust, non-parametric ensemble method\r\n- `GradientBoostingSurvivalAnalysis`: Tree-based boosting for maximum performance\r\n- `ComponentwiseGradientBoostingSurvivalAnalysis`: Linear boosting with feature selection\r\n- `ExtraSurvivalTrees`: Extremely randomized trees for additional regularization\r\n\r\n**See**: `references/ensemble-models.md` for comprehensive guidance on ensemble methods, hyperparameter tuning, and when to use each model\r\n\r\n#### Survival Support Vector Machines\r\n**Use for**: Medium-sized datasets with margin-based learning\r\n- `FastSurvivalSVM`: Linear SVM optimized for speed\r\n- `FastKernelSurvivalSVM`: Kernel SVM for non-linear relationships\r\n- `HingeLossSurvivalSVM`: SVM with hinge loss\r\n- `ClinicalKernelTransform`: Specialized kernel for clinical + molecular data\r\n\r\n**See**: `references/svm-models.md` for detailed SVM guidance, kernel selection, and hyperparameter tuning\r\n\r\n#### Model Selection Decision Tree\r\n\r\n```\r\nStart\r\n├─ High-dimensional data (p > n)?\r\n│  ├─ Yes → CoxnetSurvivalAnalysis (elastic net)\r\n│  └─ No → Continue\r\n│\r\n├─ Need interpretable coefficients?\r\n│  ├─ Yes → CoxPHSurvivalAnalysis or ComponentwiseGradientBoostingSurvivalAnalysis\r\n│  └─ No → Continue\r\n│\r\n├─ Complex non-linear relationships expected?\r\n│  ├─ Yes\r\n│  │  ├─ Large dataset (n > 1000) → GradientBoostingSurvivalAnalysis\r\n│  │  ├─ Medium dataset → RandomSurvivalForest or FastKernelSurvivalSVM\r\n│  │  └─ Small dataset → RandomSurvivalForest\r\n│  └─ No → CoxPHSurvivalAnalysis or FastSurvivalSVM\r\n│\r\n└─ For maximum performance → Try multiple models and compare\r\n```\r\n\r\n### 2. Data Preparation and Preprocessing\r\n\r\nBefore modeling, properly prepare survival data:\r\n\r\n#### Creating Survival Outcomes\r\n```python\r\nfrom sksurv.util import Surv\r\n\r\ny = Surv.from_arrays(event=event_array, time=time_array)\r\n\r\ny = Surv.from_dataframe('event', 'time', df)\r\n```\r\n\r\n#### Essential Preprocessing Steps\r\n1. **Handle missing values**: Imputation strategies for features\r\n2. **Encode categorical variables**: One-hot encoding or label encoding\r\n3. **Standardize features**: Critical for SVMs and regularized Cox models\r\n4. **Validate data quality**: Check for negative times, sufficient events per feature\r\n5. **Train-test split**: Maintain similar censoring rates across splits\r\n\r\n**See**: `references/data-handling.md` for complete preprocessing workflows, data validation, and best practices\r\n\r\n### 3. Model Evaluation\r\n\r\nProper evaluation is critical for survival models. Use appropriate metrics that account for censoring:\r\n\r\n#### Concordance Index (C-index)\r\nPrimary metric for ranking/discrimination:\r\n- **Harrell's C-index**: Use for low censoring (<40%)\r\n- **Uno's C-index**: Use for moderate to high censoring (>40%) - more robust\r\n\r\n```python\r\nfrom sksurv.metrics import concordance_index_censored, concordance_index_ipcw\r\n\r\nc_harrell = concordance_index_censored(y_test['event'], y_test['time'], risk_scores)[0]\r\n\r\nc_uno = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\r\n```\r\n\r\n#### Time-Dependent AUC\r\nEvaluate discrimination at specific time points:\r\n\r\n```python\r\nfrom sksurv.metrics import cumulative_dynamic_auc\r\n\r\ntimes = [365, 730, 1095]  # 1, 2, 3 years\r\nauc, mean_auc = cumulative_dynamic_auc(y_train, y_test, risk_scores, times)\r\n```\r\n\r\n#### Brier Score\r\nAssess both discrimination and calibration:\r\n\r\n```python\r\nfrom sksurv.metrics import integrated_brier_score\r\n\r\nibs = integrated_brier_score(y_train, y_test, survival_functions, times)\r\n```\r\n\r\n**See**: `references/evaluation-metrics.md` for comprehensive evaluation guidance, metric selection, and using scorers with cross-validation\r\n\r\n### 4. Competing Risks Analysis\r\n\r\nHandle situations with multiple mutually exclusive event types:\r\n\r\n```python\r\nfrom sksurv.nonparametric import cumulative_incidence_competing_risks\r\n\r\n\r\n### Workflow 1: Standard Survival Analysis\r\n\r\n```python\r\nfrom sksurv.datasets import load_breast_cancer\r\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis\r\nfrom sksurv.metrics import concordance_index_ipcw\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nX, y = load_breast_cancer()\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\nscaler = StandardScaler()\r\nX_train_scaled = scaler.fit_transform(X_train)\r\nX_test_scaled = scaler.transform(X_test)\r\n\r\nestimator = CoxPHSurvivalAnalysis()\r\nestimator.fit(X_train_scaled, y_train)\r\n\r\nrisk_scores = estimator.predict(X_test_scaled)\r\n\r\nc_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\r\nprint(f\"C-index: {c_index:.3f}\")\r\n```\r\n\r\n### Workflow 2: High-Dimensional Data with Feature Selection\r\n\r\n```python\r\nfrom sksurv.linear_model import CoxnetSurvivalAnalysis\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sksurv.metrics import as_concordance_index_ipcw_scorer\r\n\r\nestimator = CoxnetSurvivalAnalysis(l1_ratio=0.9)  # Lasso-like\r\n\r\nparam_grid = {'alpha_min_ratio': [0.01, 0.001]}\r\ncv = GridSearchCV(estimator, param_grid,\r\n                  scoring=as_concordance_index_ipcw_scorer(), cv=5)\r\ncv.fit(X, y)\r\n\r\nbest_model = cv.best_estimator_\r\nselected_features = np.where(best_model.coef_ != 0)[0]\r\n```\r\n\r\n### Workflow 3: Ensemble Method for Maximum Performance\r\n\r\n```python\r\nfrom sksurv.ensemble import GradientBoostingSurvivalAnalysis\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\nparam_grid = {\r\n    'learning_rate': [0.01, 0.05, 0.1],\r\n    'n_estimators': [100, 200, 300],\r\n    'max_depth': [3, 5, 7]\r\n}\r\n\r\ngbs = GradientBoostingSurvivalAnalysis()\r\ncv = GridSearchCV(gbs, param_grid, cv=5,\r\n                  scoring=as_concordance_index_ipcw_scorer(), n_jobs=-1)\r\ncv.fit(X_train, y_train)\r\n\r\nbest_model = cv.best_estimator_\r\nrisk_scores = best_model.predict(X_test)\r\nc_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\r\n```\r\n\r\n### Workflow 4: Comprehensive Model Comparison\r\n\r\n```python\r\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis\r\nfrom sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\r\nfrom sksurv.svm import FastSurvivalSVM\r\nfrom sksurv.metrics import concordance_index_ipcw, integrated_brier_score\r\n\r\nmodels = {\r\n    'Cox': CoxPHSurvivalAnalysis(),\r\n    'RSF': RandomSurvivalForest(n_estimators=100, random_state=42),\r\n    'GBS': GradientBoostingSurvivalAnalysis(random_state=42),\r\n    'SVM': FastSurvivalSVM(random_state=42)\r\n}\r\n\r\nresults = {}\r\nfor name, model in models.items():\r\n    model.fit(X_train_scaled, y_train)\r\n    risk_scores = model.predict(X_test_scaled)\r\n    c_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\r\n    results[name] = c_index\r\n    print(f\"{name}: C-index = {c_index:.3f}\")\r\n\r\n\r\nscikit-survival fully integrates with scikit-learn's ecosystem:\r\n\r\n```python\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\r\n\r\npipeline = Pipeline([\r\n    ('scaler', StandardScaler()),\r\n    ('model', CoxPHSurvivalAnalysis())\r\n])\r\n\r\nscores = cross_val_score(pipeline, X, y, cv=5,\r\n                         scoring=as_concordance_index_ipcw_scorer())\r\n\r\n\r\n```python\r\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis, IPCRidge\r\nfrom sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\r\nfrom sksurv.svm import FastSurvivalSVM, FastKernelSurvivalSVM\r\nfrom sksurv.tree import SurvivalTree\r\n\r\nfrom sksurv.metrics import (\r\n    concordance_index_censored,\r\n    concordance_index_ipcw,\r\n    cumulative_dynamic_auc,\r\n    brier_score,\r\n    integrated_brier_score,\r\n    as_concordance_index_ipcw_scorer,\r\n    as_integrated_brier_score_scorer\r\n)\r\n\r\nfrom sksurv.nonparametric import (\r\n    kaplan_meier_estimator,\r\n    nelson_aalen_estimator,\r\n    cumulative_incidence_competing_risks\r\n)\r\n\r\nfrom sksurv.util import Surv\r\nfrom sksurv.preprocessing import OneHotEncoder, encode_categorical\r\nfrom sksurv.datasets import load_gbsg2, load_breast_cancer, load_veterans_lung_cancer",
  "name": "scikit-survival",
  "id": "scientific-pkg-scikit-survival",
  "sections": {
    "Best Practices": "1. **Always standardize features** for SVMs and regularized Cox models\r\n2. **Use Uno's C-index** instead of Harrell's when censoring > 40%\r\n3. **Report multiple evaluation metrics** (C-index, integrated Brier score, time-dependent AUC)\r\n4. **Check proportional hazards assumption** for Cox models\r\n5. **Use cross-validation** for hyperparameter tuning with appropriate scorers\r\n6. **Validate data quality** before modeling (check for negative times, sufficient events per feature)\r\n7. **Compare multiple model types** to find best performance\r\n8. **Use permutation importance** for Random Survival Forests (not built-in importance)\r\n9. **Consider competing risks** when multiple event types exist\r\n10. **Document censoring mechanism** and rates in analysis",
    "Quick Reference: Key Imports": "from sksurv.kernels import ClinicalKernelTransform\r\n```",
    "Common Pitfalls to Avoid": "1. **Using Harrell's C-index with high censoring** → Use Uno's C-index\r\n2. **Not standardizing features for SVMs** → Always standardize\r\n3. **Forgetting to pass y_train to concordance_index_ipcw** → Required for IPCW calculation\r\n4. **Treating competing events as censored** → Use competing risks methods\r\n5. **Not checking for sufficient events per feature** → Rule of thumb: 10+ events per feature\r\n6. **Using built-in feature importance for RSF** → Use permutation importance\r\n7. **Ignoring proportional hazards assumption** → Validate or use alternative models\r\n8. **Not using appropriate scorers in cross-validation** → Use as_concordance_index_ipcw_scorer()",
    "Reference Files": "This skill includes detailed reference files for specific topics:\r\n\r\n- **`references/cox-models.md`**: Complete guide to Cox proportional hazards models, penalized Cox (CoxNet), IPCRidge, regularization strategies, and interpretation\r\n- **`references/ensemble-models.md`**: Random Survival Forests, Gradient Boosting, hyperparameter tuning, feature importance, and model selection\r\n- **`references/evaluation-metrics.md`**: Concordance index (Harrell's vs Uno's), time-dependent AUC, Brier score, comprehensive evaluation pipelines\r\n- **`references/data-handling.md`**: Data loading, preprocessing workflows, handling missing data, feature encoding, validation checks\r\n- **`references/svm-models.md`**: Survival Support Vector Machines, kernel selection, clinical kernel transform, hyperparameter tuning\r\n- **`references/competing-risks.md`**: Competing risks analysis, cumulative incidence functions, cause-specific hazard models\r\n\r\nLoad these reference files when detailed information is needed for specific tasks.",
    "Overview": "scikit-survival is a Python library for survival analysis built on top of scikit-learn. It provides specialized tools for time-to-event analysis, handling the unique challenge of censored data where some observations are only partially known.\r\n\r\nSurvival analysis aims to establish connections between covariates and the time of an event, accounting for censored records (particularly right-censored data from studies where participants don't experience events during observation periods).",
    "Typical Workflows": "best_model_name = max(results, key=results.get)\r\nprint(f\"\\nBest model: {best_model_name}\")\r\n```",
    "When to Use This Skill": "Use this skill when:\r\n- Performing survival analysis or time-to-event modeling\r\n- Working with censored data (right-censored, left-censored, or interval-censored)\r\n- Fitting Cox proportional hazards models (standard or penalized)\r\n- Building ensemble survival models (Random Survival Forests, Gradient Boosting)\r\n- Training Survival Support Vector Machines\r\n- Evaluating survival model performance (concordance index, Brier score, time-dependent AUC)\r\n- Estimating Kaplan-Meier or Nelson-Aalen curves\r\n- Analyzing competing risks\r\n- Preprocessing survival data or handling missing values in survival datasets\r\n- Conducting any analysis using the scikit-survival library",
    "Additional Resources": "- **Official Documentation**: https://scikit-survival.readthedocs.io/\r\n- **GitHub Repository**: https://github.com/sebp/scikit-survival\r\n- **Built-in Datasets**: Use `sksurv.datasets` for practice datasets (GBSG2, WHAS500, veterans lung cancer, etc.)\r\n- **API Reference**: Complete list of classes and functions at https://scikit-survival.readthedocs.io/en/stable/api/index.html",
    "Core Capabilities": "time_points, cif_event1, cif_event2 = cumulative_incidence_competing_risks(y)\r\n```\r\n\r\n**Use competing risks when**:\r\n- Multiple mutually exclusive event types exist (e.g., death from different causes)\r\n- Occurrence of one event prevents others\r\n- Need probability estimates for specific event types\r\n\r\n**See**: `references/competing-risks.md` for detailed competing risks methods, cause-specific hazard models, and interpretation\r\n\r\n### 5. Non-parametric Estimation\r\n\r\nEstimate survival functions without parametric assumptions:\r\n\r\n#### Kaplan-Meier Estimator\r\n```python\r\nfrom sksurv.nonparametric import kaplan_meier_estimator\r\n\r\ntime, survival_prob = kaplan_meier_estimator(y['event'], y['time'])\r\n```\r\n\r\n#### Nelson-Aalen Estimator\r\n```python\r\nfrom sksurv.nonparametric import nelson_aalen_estimator\r\n\r\ntime, cumulative_hazard = nelson_aalen_estimator(y['event'], y['time'])\r\n```",
    "Integration with scikit-learn": "param_grid = {'model__alpha': [0.1, 1.0, 10.0]}\r\ncv = GridSearchCV(pipeline, param_grid, cv=5)\r\ncv.fit(X, y)\r\n```"
  }
}