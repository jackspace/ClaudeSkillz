{
  "sections": {
    "When to Invoke This Skill": "Invoke this skill when users ask about:\r\n- ComfyUI workflow creation\r\n- Node configuration and connections\r\n- Model installation for ComfyUI\r\n- OmniGen2 setup\r\n- Workflow optimization\r\n- VRAM management in ComfyUI\r\n- Troubleshooting ComfyUI errors\r\n- Sampler and scheduler settings\r\n- ControlNet integration\r\n- Batch processing in ComfyUI",
    "Usage Examples": "```\r\n\r\n**Step 3: Verify in WebUI**\r\n\r\n1. Open ComfyUI web interface\r\n2. Check that models appear in dropdowns:\r\n   - VAE loader should show \"ae.safetensors\"\r\n   - Text encoder nodes should show \"qwen_2.5_vl_fp16.safetensors\"\r\n   - Diffusion model loader should show \"omnigen2_fp16.safetensors\"\r\n\r\n**Step 4: Create Workflow**\r\n\r\nUse OmniGen2-specific nodes:\r\n- Load the OmniGen2 diffusion model\r\n- Load Qwen text encoder\r\n- Load Lumina VAE\r\n- Configure sampler for OmniGen2\r\n\r\n### Example 2: Creating a Basic Workflow\r\n\r\n**User:** \"How do I create a simple txt2img workflow in ComfyUI?\"\r\n\r\n**Assistant Response:**\r\nI'll help you create a basic txt2img workflow.\r\n\r\n**Nodes to Add:**\r\n\r\n1. **Right-click → Add Node → loaders → Load Checkpoint**\r\n   - Select your SD model\r\n\r\n2. **Right-click → Add Node → conditioning → CLIP Text Encode (Prompt)** × 2\r\n   - One for positive prompt\r\n   - One for negative prompt\r\n\r\n3. **Right-click → Add Node → latent → Empty Latent Image**\r\n   - Set width: 512\r\n   - Set height: 512\r\n   - Batch size: 1\r\n\r\n4. **Right-click → Add Node → sampling → KSampler**\r\n   - Seed: -1 (random)\r\n   - Steps: 20-40\r\n   - CFG: 4.5 (for photorealism)\r\n   - Sampler: dpmpp_2m_karras\r\n   - Scheduler: karras\r\n\r\n5. **Right-click → Add Node → latent → VAE Decode**\r\n\r\n6. **Right-click → Add Node → image → Save Image**\r\n\r\n**Connections:**\r\n\r\n```\r\nLoad Checkpoint [MODEL] → KSampler [model]\r\nLoad Checkpoint [CLIP] → CLIP Text Encode (pos) [clip]\r\nLoad Checkpoint [CLIP] → CLIP Text Encode (neg) [clip]\r\nLoad Checkpoint [VAE] → VAE Decode [vae]\r\n\r\nCLIP Text Encode (pos) [CONDITIONING] → KSampler [positive]\r\nCLIP Text Encode (neg) [CONDITIONING] → KSampler [negative]\r\n\r\nEmpty Latent Image [LATENT] → KSampler [latent_image]\r\n\r\nKSampler [LATENT] → VAE Decode [samples]\r\nVAE Decode [IMAGE] → Save Image [images]\r\n```\r\n\r\n**Usage:**\r\n1. Enter your prompt in positive CLIP Text Encode\r\n2. Enter negative prompt in negative CLIP Text Encode\r\n3. Click \"Queue Prompt\" to generate\r\n\r\n### Example 3: Troubleshooting OOM Errors\r\n\r\n**User:** \"ComfyUI keeps crashing with out of memory errors. How do I fix this?\"\r\n\r\n**Assistant Response:**\r\nOOM errors in ComfyUI can be resolved with these strategies:\r\n\r\n**1. Reduce Resolution**\r\n- Change Empty Latent Image dimensions:\r\n  - From 768×768 → 512×512\r\n  - From 1024×1024 → 768×768\r\n- Lower resolution = less VRAM usage\r\n\r\n**2. Reduce Batch Size**\r\n- In Empty Latent Image node:\r\n  - Set batch_size to 1\r\n- Generate images one at a time\r\n\r\n**3. Use VRAM-Efficient Models**\r\n- Use pruned models (smaller file size)\r\n- Use fp16 models instead of fp32\r\n- Example: `v1-5-pruned-emaonly.safetensors` vs full versions\r\n\r\n**4. ComfyUI Launch Args**\r\nAdd these flags when starting ComfyUI:\r\n```bash\r\n--lowvram          # Aggressive VRAM optimization\r\n--medvram          # Moderate VRAM optimization\r\n--normalvram       # Normal VRAM usage\r\n--cpu              # Use CPU instead of GPU (slow but no VRAM)\r\n```\r\n\r\n**5. Clear Cache**\r\n- Restart ComfyUI between generations\r\n- Clear browser cache\r\n- Delete temporary files\r\n\r\n**6. Check Node Efficiency**\r\n- Remove unnecessary nodes\r\n- Avoid multiple simultaneous samplers\r\n- Use single VAE decode instead of multiple\r\n\r\n**For 2GB VRAM:**\r\n- Stick to 512×512 resolution\r\n- Use --lowvram flag\r\n- Generate single images (batch=1)\r\n- Use pruned/optimized models",
    "Capabilities": "This skill provides expert assistance with:\r\n\r\n1. **Workflow Setup & Management**\r\n   - Creating custom workflows from scratch\r\n   - Importing and modifying existing workflows\r\n   - Workflow organization and structure\r\n   - Node connection best practices\r\n\r\n2. **Node Configuration**\r\n   - Model loaders (Checkpoints, LoRAs, VAEs)\r\n   - Samplers and schedulers\r\n   - Conditioning nodes (prompts)\r\n   - Image processing nodes\r\n   - ControlNet integration\r\n\r\n3. **Model Management**\r\n   - Installing models (checkpoints, VAEs, text encoders, diffusion models)\r\n   - Model organization in directories\r\n   - Model format compatibility\r\n   - OmniGen2 setup\r\n\r\n4. **Performance Optimization**\r\n   - VRAM management strategies\r\n   - Batch processing optimization\r\n   - Workflow efficiency improvements\r\n   - Node caching strategies\r\n\r\n5. **Troubleshooting**\r\n   - Model loading errors\r\n   - Node connection issues\r\n   - OOM (Out of Memory) errors\r\n   - Workflow execution failures",
    "Best Practices": "1. **Workflow Organization**\r\n   - Group related nodes together\r\n   - Use reroute nodes for clean connections\r\n   - Add note nodes to document workflow sections\r\n   - Save workflows with descriptive names\r\n\r\n2. **Node Connections**\r\n   - Always connect correct output to correct input types\r\n   - Use color coding: MODEL, CLIP, VAE, CONDITIONING, LATENT, IMAGE\r\n   - Double-check all connections before queuing\r\n\r\n3. **Model Management**\r\n   - Keep models organized in proper subdirectories\r\n   - Use descriptive filenames\r\n   - Delete unused models to save space\r\n   - Verify checksums after downloading\r\n\r\n4. **Performance**\r\n   - Start with lower resolutions for testing\r\n   - Use batch generation only when VRAM allows\r\n   - Cache models by reusing same checkpoint across generations\r\n   - Close other GPU applications\r\n\r\n5. **Troubleshooting**\r\n   - Check ComfyUI console for error messages\r\n   - Verify model files are in correct directories\r\n   - Ensure model formats are compatible\r\n   - Test with simple workflow first",
    "Advanced Techniques": "### Batch Processing\r\n\r\nUse batch nodes to generate multiple variations:\r\n```\r\nBatch Size: 4 in Empty Latent Image\r\n→ Generates 4 images per queue\r\n```\r\n\r\n### Seed Control\r\n\r\nFor consistent results:\r\n```\r\nFixed Seed: Use specific number (e.g., 12345)\r\nRandom Seed: Use -1\r\nSeed Increment: Batch Size controls seed increment\r\n```\r\n\r\n### LoRA Stacking\r\n\r\nApply multiple LoRAs:\r\n```\r\nLoad Checkpoint → Load LoRA (1) → Load LoRA (2) → KSampler\r\nSet strength: 0.5-1.0 per LoRA\r\n```",
    "Quick Reference": "### KSampler Settings for Photorealism\r\n\r\n```\r\nseed: -1 (random)\r\nsteps: 20-40\r\ncfg: 4.5 (photorealism breakthrough)\r\nsampler_name: dpmpp_2m_karras\r\nscheduler: karras\r\ndenoise: 1.0\r\n```\r\n\r\n### Resolution Limits by VRAM\r\n\r\n| VRAM | Max Resolution | Batch Size |\r\n|------|---------------|------------|\r\n| 2GB | 512×512 | 1 |\r\n| 4GB | 768×768 | 1-2 |\r\n| 6GB | 1024×1024 | 1-2 |\r\n| 8GB+ | 1024×1024+ | 2-4 |\r\n\r\n### Common Node Shortcuts\r\n\r\n- **Ctrl + Enter**: Queue Prompt\r\n- **Ctrl + Shift + Enter**: Queue Prompt (front of queue)\r\n- **Double Click**: Add node (search)\r\n- **Ctrl + D**: Duplicate selected nodes\r\n- **Delete**: Remove selected nodes",
    "Key Knowledge Base": "```",
    "Common Workflows": "### Basic txt2img Workflow\r\n\r\n**Nodes Required:**\r\n1. Load Checkpoint\r\n2. CLIP Text Encode (Prompt) x2 (positive & negative)\r\n3. Empty Latent Image\r\n4. KSampler\r\n5. VAE Decode\r\n6. Save Image\r\n\r\n**Connection Flow:**\r\n```\r\nLoad Checkpoint → CLIP (positive/negative) → KSampler\r\nEmpty Latent → KSampler → VAE Decode → Save Image\r\nLoad Checkpoint → VAE Decode\r\n```\r\n\r\n### img2img Workflow\r\n\r\n**Additional Nodes:**\r\n1. Load Image\r\n2. VAE Encode\r\n\r\n**Connection Flow:**\r\n```\r\nLoad Image → VAE Encode → KSampler\r\n(Rest similar to txt2img)\r\n```\r\n\r\n### ControlNet Workflow\r\n\r\n**Additional Nodes:**\r\n1. Load ControlNet Model\r\n2. Apply ControlNet\r\n3. Preprocessor nodes (depends on ControlNet type)",
    "Additional Resources": "- ComfyUI GitHub: https://github.com/comfyanonymous/ComfyUI\r\n- Custom Nodes Registry: https://github.com/ltdrdata/ComfyUI-Manager\r\n- OmniGen2 Documentation: Hugging Face model pages\r\n- Download Script: `/root/homelab/scripts/download-comfyui-omnigen2-models.sh`"
  },
  "content": "Expert guidance for ComfyUI workflow creation, node configuration, and optimization.\r\n\r\n\r\n### ComfyUI Directory Structure\r\n\r\n```\r\n/srv/comfyui/\r\n├── models/\r\n│   ├── checkpoints/          # SD models (.safetensors, .ckpt)\r\n│   ├── vae/                  # VAE models\r\n│   ├── loras/                # LoRA models\r\n│   ├── text_encoders/        # Text encoder models\r\n│   ├── diffusion_models/     # Diffusion models\r\n│   ├── controlnet/           # ControlNet models\r\n│   ├── upscale_models/       # Upscaler models\r\n│   └── embeddings/           # Textual inversion embeddings\r\n├── input/                    # Input images\r\n├── output/                   # Generated images\r\n└── custom_nodes/             # Custom node extensions\r\n```\r\n\r\n### Essential Nodes\r\n\r\n**Loading Nodes:**\r\n- `Load Checkpoint` - Load SD models\r\n- `Load VAE` - Load VAE models\r\n- `Load LoRA` - Load LoRA models\r\n\r\n**Conditioning Nodes:**\r\n- `CLIP Text Encode (Prompt)` - Positive and negative prompts\r\n- `Conditioning Combine` - Merge multiple conditioning\r\n\r\n**Sampling Nodes:**\r\n- `KSampler` - Main sampler node\r\n- `KSampler Advanced` - Advanced sampling control\r\n\r\n**Image Nodes:**\r\n- `VAE Decode` - Convert latent to image\r\n- `VAE Encode` - Convert image to latent\r\n- `Save Image` - Save output\r\n\r\n**Model Nodes:**\r\n- `Model Sampling Discrete` - Sampling configuration\r\n- `Model Sampling SD3` - SD3-specific sampling\r\n\r\n### OmniGen2 Setup\r\n\r\n**Required Models:**\r\n1. **VAE:** `ae.safetensors` → `/srv/comfyui/models/vae/`\r\n2. **Text Encoder:** `qwen_2.5_vl_fp16.safetensors` → `/srv/comfyui/models/text_encoders/`\r\n3. **Diffusion Model:** `omnigen2_fp16.safetensors` → `/srv/comfyui/models/diffusion_models/`\r\n\r\n**Download Script:**\r\n```bash\r\n/root/homelab/scripts/download-comfyui-omnigen2-models.sh\r\n\r\nmkdir -p /srv/comfyui/models/{vae,text_encoders,diffusion_models}\r\nwget -O /srv/comfyui/models/vae/ae.safetensors \\\r\n  \"https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/split_files/vae/ae.safetensors\"\r\n\r\n### Example 1: Setting Up OmniGen2\r\n\r\n**User:** \"Help me set up OmniGen2 in ComfyUI\"\r\n\r\n**Assistant Response:**\r\nI'll guide you through setting up OmniGen2 in ComfyUI.\r\n\r\n**Step 1: Download Models**\r\n\r\nRun the download script:\r\n```bash\r\nbash /root/homelab/scripts/download-comfyui-omnigen2-models.sh\r\n```\r\n\r\nOr download manually:\r\n- VAE: ae.safetensors → /srv/comfyui/models/vae/\r\n- Text Encoder: qwen_2.5_vl_fp16.safetensors → /srv/comfyui/models/text_encoders/\r\n- Diffusion Model: omnigen2_fp16.safetensors → /srv/comfyui/models/diffusion_models/\r\n\r\n**Step 2: Restart ComfyUI**\r\n\r\n```bash\r\ndocker restart comfyui-container\r\n\r\nsudo systemctl restart comfyui",
  "id": "comfyui-workflow-helper",
  "name": "comfyui-workflow-helper",
  "description": ""
}