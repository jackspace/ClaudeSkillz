{
  "description": "\"Bayesian modeling with PyMC. Build hierarchical models, MCMC (NUTS), variational inference, LOO/WAIC comparison, posterior checks, for probabilistic programming and inference.\"",
  "references": {
    "files": [
      "references/distributions.md",
      "references/sampling_inference.md",
      "references/workflows.md"
    ]
  },
  "content": "Follow this workflow for building and validating Bayesian models:\r\n\r\n### 1. Data Preparation\r\n\r\n```python\r\nimport pymc as pm\r\nimport arviz as az\r\nimport numpy as np\r\n\r\nX = ...  # Predictors\r\ny = ...  # Outcomes\r\n\r\nX_mean = X.mean(axis=0)\r\nX_std = X.std(axis=0)\r\nX_scaled = (X - X_mean) / X_std\r\n```\r\n\r\n**Key practices:**\r\n- Standardize continuous predictors (improves sampling efficiency)\r\n- Center outcomes when possible\r\n- Handle missing data explicitly (treat as parameters)\r\n- Use named dimensions with `coords` for clarity\r\n\r\n### 2. Model Building\r\n\r\n```python\r\ncoords = {\r\n    'predictors': ['var1', 'var2', 'var3'],\r\n    'obs_id': np.arange(len(y))\r\n}\r\n\r\nwith pm.Model(coords=coords) as model:\r\n    # Priors\r\n    alpha = pm.Normal('alpha', mu=0, sigma=1)\r\n    beta = pm.Normal('beta', mu=0, sigma=1, dims='predictors')\r\n    sigma = pm.HalfNormal('sigma', sigma=1)\r\n\r\n    # Linear predictor\r\n    mu = alpha + pm.math.dot(X_scaled, beta)\r\n\r\n    # Likelihood\r\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y, dims='obs_id')\r\n```\r\n\r\n**Key practices:**\r\n- Use weakly informative priors (not flat priors)\r\n- Use `HalfNormal` or `Exponential` for scale parameters\r\n- Use named dimensions (`dims`) instead of `shape` when possible\r\n- Use `pm.Data()` for values that will be updated for predictions\r\n\r\n### 3. Prior Predictive Check\r\n\r\n**Always validate priors before fitting:**\r\n\r\n```python\r\nwith model:\r\n    prior_pred = pm.sample_prior_predictive(samples=1000, random_seed=42)\r\n\r\naz.plot_ppc(prior_pred, group='prior')\r\n```\r\n\r\n**Check:**\r\n- Do prior predictions span reasonable values?\r\n- Are extreme values plausible given domain knowledge?\r\n- If priors generate implausible data, adjust and re-check\r\n\r\n### 4. Fit Model\r\n\r\n```python\r\nwith model:\r\n    # Optional: Quick exploration with ADVI\r\n    # approx = pm.fit(n=20000)\r\n\r\n    # Full MCMC inference\r\n    idata = pm.sample(\r\n        draws=2000,\r\n        tune=1000,\r\n        chains=4,\r\n        target_accept=0.9,\r\n        random_seed=42,\r\n        idata_kwargs={'log_likelihood': True}  # For model comparison\r\n    )\r\n```\r\n\r\n**Key parameters:**\r\n- `draws=2000`: Number of samples per chain\r\n- `tune=1000`: Warmup samples (discarded)\r\n- `chains=4`: Run 4 chains for convergence checking\r\n- `target_accept=0.9`: Higher for difficult posteriors (0.95-0.99)\r\n- Include `log_likelihood=True` for model comparison\r\n\r\n### 5. Check Diagnostics\r\n\r\n**Use the diagnostic script:**\r\n\r\n```python\r\nfrom scripts.model_diagnostics import check_diagnostics\r\n\r\nresults = check_diagnostics(idata, var_names=['alpha', 'beta', 'sigma'])\r\n```\r\n\r\n**Check:**\r\n- **R-hat < 1.01**: Chains have converged\r\n- **ESS > 400**: Sufficient effective samples\r\n- **No divergences**: NUTS sampled successfully\r\n- **Trace plots**: Chains should mix well (fuzzy caterpillar)\r\n\r\n**If issues arise:**\r\n- Divergences → Increase `target_accept=0.95`, use non-centered parameterization\r\n- Low ESS → Sample more draws, reparameterize to reduce correlation\r\n- High R-hat → Run longer, check for multimodality\r\n\r\n### 6. Posterior Predictive Check\r\n\r\n**Validate model fit:**\r\n\r\n```python\r\nwith model:\r\n    pm.sample_posterior_predictive(idata, extend_inferencedata=True, random_seed=42)\r\n\r\naz.plot_ppc(idata)\r\n```\r\n\r\n**Check:**\r\n- Do posterior predictions capture observed data patterns?\r\n- Are systematic deviations evident (model misspecification)?\r\n- Consider alternative models if fit is poor\r\n\r\n### 7. Analyze Results\r\n\r\n```python\r\nprint(az.summary(idata, var_names=['alpha', 'beta', 'sigma']))\r\n\r\naz.plot_posterior(idata, var_names=['alpha', 'beta', 'sigma'])\r\n\r\naz.plot_forest(idata, var_names=['beta'], combined=True)\r\n```\r\n\r\n### 8. Make Predictions\r\n\r\n```python\r\nX_new = ...  # New predictor values\r\nX_new_scaled = (X_new - X_mean) / X_std\r\n\r\nwith model:\r\n    pm.set_data({'X_scaled': X_new_scaled})\r\n    post_pred = pm.sample_posterior_predictive(\r\n        idata.posterior,\r\n        var_names=['y_obs'],\r\n        random_seed=42\r\n    )\r\n\r\n\r\n### Comparing Models\r\n\r\nUse LOO or WAIC for model comparison:\r\n\r\n```python\r\nfrom scripts.model_comparison import compare_models, check_loo_reliability\r\n\r\nmodels = {\r\n    'Model1': idata1,\r\n    'Model2': idata2,\r\n    'Model3': idata3\r\n}\r\n\r\ncomparison = compare_models(models, ic='loo')",
  "name": "pymc-bayesian-modeling",
  "id": "scientific-pkg-pymc",
  "sections": {
    "Common Issues and Solutions": "### Divergences\r\n\r\n**Symptom:** `idata.sample_stats.diverging.sum() > 0`\r\n\r\n**Solutions:**\r\n1. Increase `target_accept=0.95` or `0.99`\r\n2. Use non-centered parameterization (hierarchical models)\r\n3. Add stronger priors to constrain parameters\r\n4. Check for model misspecification\r\n\r\n### Low Effective Sample Size\r\n\r\n**Symptom:** `ESS < 400`\r\n\r\n**Solutions:**\r\n1. Sample more draws: `draws=5000`\r\n2. Reparameterize to reduce posterior correlation\r\n3. Use QR decomposition for regression with correlated predictors\r\n\r\n### High R-hat\r\n\r\n**Symptom:** `R-hat > 1.01`\r\n\r\n**Solutions:**\r\n1. Run longer chains: `tune=2000, draws=5000`\r\n2. Check for multimodality\r\n3. Improve initialization with ADVI\r\n\r\n### Slow Sampling\r\n\r\n**Solutions:**\r\n1. Use ADVI initialization\r\n2. Reduce model complexity\r\n3. Increase parallelization: `cores=8, chains=8`\r\n4. Use variational inference if appropriate",
    "Overview": "PyMC is a Python library for Bayesian modeling and probabilistic programming. Build, fit, validate, and compare Bayesian models using PyMC's modern API (version 5.x+), including hierarchical models, MCMC sampling (NUTS), variational inference, and model comparison (LOO, WAIC).",
    "Common Model Patterns": "### Linear Regression\r\n\r\nFor continuous outcomes with linear relationships:\r\n\r\n```python\r\nwith pm.Model() as linear_model:\r\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\r\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=n_predictors)\r\n    sigma = pm.HalfNormal('sigma', sigma=1)\r\n\r\n    mu = alpha + pm.math.dot(X, beta)\r\n    y = pm.Normal('y', mu=mu, sigma=sigma, observed=y_obs)\r\n```\r\n\r\n**Use template:** `assets/linear_regression_template.py`\r\n\r\n### Logistic Regression\r\n\r\nFor binary outcomes:\r\n\r\n```python\r\nwith pm.Model() as logistic_model:\r\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\r\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=n_predictors)\r\n\r\n    logit_p = alpha + pm.math.dot(X, beta)\r\n    y = pm.Bernoulli('y', logit_p=logit_p, observed=y_obs)\r\n```\r\n\r\n### Hierarchical Models\r\n\r\nFor grouped data (use non-centered parameterization):\r\n\r\n```python\r\nwith pm.Model(coords={'groups': group_names}) as hierarchical_model:\r\n    # Hyperpriors\r\n    mu_alpha = pm.Normal('mu_alpha', mu=0, sigma=10)\r\n    sigma_alpha = pm.HalfNormal('sigma_alpha', sigma=1)\r\n\r\n    # Group-level (non-centered)\r\n    alpha_offset = pm.Normal('alpha_offset', mu=0, sigma=1, dims='groups')\r\n    alpha = pm.Deterministic('alpha', mu_alpha + sigma_alpha * alpha_offset, dims='groups')\r\n\r\n    # Observation-level\r\n    mu = alpha[group_idx]\r\n    sigma = pm.HalfNormal('sigma', sigma=1)\r\n    y = pm.Normal('y', mu=mu, sigma=sigma, observed=y_obs)\r\n```\r\n\r\n**Use template:** `assets/hierarchical_model_template.py`\r\n\r\n**Critical:** Always use non-centered parameterization for hierarchical models to avoid divergences.\r\n\r\n### Poisson Regression\r\n\r\nFor count data:\r\n\r\n```python\r\nwith pm.Model() as poisson_model:\r\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\r\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=n_predictors)\r\n\r\n    log_lambda = alpha + pm.math.dot(X, beta)\r\n    y = pm.Poisson('y', mu=pm.math.exp(log_lambda), observed=y_obs)\r\n```\r\n\r\nFor overdispersed counts, use `NegativeBinomial` instead.\r\n\r\n### Time Series\r\n\r\nFor autoregressive processes:\r\n\r\n```python\r\nwith pm.Model() as ar_model:\r\n    sigma = pm.HalfNormal('sigma', sigma=1)\r\n    rho = pm.Normal('rho', mu=0, sigma=0.5, shape=ar_order)\r\n    init_dist = pm.Normal.dist(mu=0, sigma=sigma)\r\n\r\n    y = pm.AR('y', rho=rho, sigma=sigma, init_dist=init_dist, observed=y_obs)\r\n```",
    "Best Practices": "### Model Building\r\n\r\n1. **Always standardize predictors** for better sampling\r\n2. **Use weakly informative priors** (not flat)\r\n3. **Use named dimensions** (`dims`) for clarity\r\n4. **Non-centered parameterization** for hierarchical models\r\n5. **Check prior predictive** before fitting\r\n\r\n### Sampling\r\n\r\n1. **Run multiple chains** (at least 4) for convergence\r\n2. **Use `target_accept=0.9`** as baseline (higher if needed)\r\n3. **Include `log_likelihood=True`** for model comparison\r\n4. **Set random seed** for reproducibility\r\n\r\n### Validation\r\n\r\n1. **Check diagnostics** before interpretation (R-hat, ESS, divergences)\r\n2. **Posterior predictive check** for model validation\r\n3. **Compare multiple models** when appropriate\r\n4. **Report uncertainty** (HDI intervals, not just point estimates)\r\n\r\n### Workflow\r\n\r\n1. Start simple, add complexity gradually\r\n2. Prior predictive check → Fit → Diagnostics → Posterior predictive check\r\n3. Iterate on model specification based on checks\r\n4. Document assumptions and prior choices",
    "When to Use This Skill": "This skill should be used when:\r\n- Building Bayesian models (linear/logistic regression, hierarchical models, time series, etc.)\r\n- Performing MCMC sampling or variational inference\r\n- Conducting prior/posterior predictive checks\r\n- Diagnosing sampling issues (divergences, convergence, ESS)\r\n- Comparing multiple models using information criteria (LOO, WAIC)\r\n- Implementing uncertainty quantification through Bayesian methods\r\n- Working with hierarchical/multilevel data structures\r\n- Handling missing data or measurement error in a principled way",
    "Resources": "This skill includes:\r\n\r\n### References (`references/`)\r\n\r\n- **`distributions.md`**: Comprehensive catalog of PyMC distributions organized by category (continuous, discrete, multivariate, mixture, time series). Use when selecting priors or likelihoods.\r\n\r\n- **`sampling_inference.md`**: Detailed guide to sampling algorithms (NUTS, Metropolis, SMC), variational inference (ADVI, SVGD), and handling sampling issues. Use when encountering convergence problems or choosing inference methods.\r\n\r\n- **`workflows.md`**: Complete workflow examples and code patterns for common model types, data preparation, prior selection, and model validation. Use as a cookbook for standard Bayesian analyses.\r\n\r\n### Scripts (`scripts/`)\r\n\r\n- **`model_diagnostics.py`**: Automated diagnostic checking and report generation. Functions: `check_diagnostics()` for quick checks, `create_diagnostic_report()` for comprehensive analysis with plots.\r\n\r\n- **`model_comparison.py`**: Model comparison utilities using LOO/WAIC. Functions: `compare_models()`, `check_loo_reliability()`, `model_averaging()`.\r\n\r\n### Templates (`assets/`)\r\n\r\n- **`linear_regression_template.py`**: Complete template for Bayesian linear regression with full workflow (data prep, prior checks, fitting, diagnostics, predictions).\r\n\r\n- **`hierarchical_model_template.py`**: Complete template for hierarchical/multilevel models with non-centered parameterization and group-level analysis.",
    "Distribution Selection Guide": "### For Priors\r\n\r\n**Scale parameters** (σ, τ):\r\n- `pm.HalfNormal('sigma', sigma=1)` - Default choice\r\n- `pm.Exponential('sigma', lam=1)` - Alternative\r\n- `pm.Gamma('sigma', alpha=2, beta=1)` - More informative\r\n\r\n**Unbounded parameters**:\r\n- `pm.Normal('theta', mu=0, sigma=1)` - For standardized data\r\n- `pm.StudentT('theta', nu=3, mu=0, sigma=1)` - Robust to outliers\r\n\r\n**Positive parameters**:\r\n- `pm.LogNormal('theta', mu=0, sigma=1)`\r\n- `pm.Gamma('theta', alpha=2, beta=1)`\r\n\r\n**Probabilities**:\r\n- `pm.Beta('p', alpha=2, beta=2)` - Weakly informative\r\n- `pm.Uniform('p', lower=0, upper=1)` - Non-informative (use sparingly)\r\n\r\n**Correlation matrices**:\r\n- `pm.LKJCorr('corr', n=n_vars, eta=2)` - eta=1 uniform, eta>1 prefers identity\r\n\r\n### For Likelihoods\r\n\r\n**Continuous outcomes**:\r\n- `pm.Normal('y', mu=mu, sigma=sigma)` - Default for continuous data\r\n- `pm.StudentT('y', nu=nu, mu=mu, sigma=sigma)` - Robust to outliers\r\n\r\n**Count data**:\r\n- `pm.Poisson('y', mu=lambda)` - Equidispersed counts\r\n- `pm.NegativeBinomial('y', mu=mu, alpha=alpha)` - Overdispersed counts\r\n- `pm.ZeroInflatedPoisson('y', psi=psi, mu=mu)` - Excess zeros\r\n\r\n**Binary outcomes**:\r\n- `pm.Bernoulli('y', p=p)` or `pm.Bernoulli('y', logit_p=logit_p)`\r\n\r\n**Categorical outcomes**:\r\n- `pm.Categorical('y', p=probs)`\r\n\r\n**See:** `references/distributions.md` for comprehensive distribution reference",
    "Model Comparison": "check_loo_reliability(models)\r\n```\r\n\r\n**Interpretation:**\r\n- **Δloo < 2**: Models are similar, choose simpler model\r\n- **2 < Δloo < 4**: Weak evidence for better model\r\n- **4 < Δloo < 10**: Moderate evidence\r\n- **Δloo > 10**: Strong evidence for better model\r\n\r\n**Check Pareto-k values:**\r\n- k < 0.7: LOO reliable\r\n- k > 0.7: Consider WAIC or k-fold CV\r\n\r\n### Model Averaging\r\n\r\nWhen models are similar, average predictions:\r\n\r\n```python\r\nfrom scripts.model_comparison import model_averaging\r\n\r\naveraged_pred, weights = model_averaging(models, var_name='y_obs')\r\n```",
    "Sampling and Inference": "### MCMC with NUTS\r\n\r\nDefault and recommended for most models:\r\n\r\n```python\r\nidata = pm.sample(\r\n    draws=2000,\r\n    tune=1000,\r\n    chains=4,\r\n    target_accept=0.9,\r\n    random_seed=42\r\n)\r\n```\r\n\r\n**Adjust when needed:**\r\n- Divergences → `target_accept=0.95` or higher\r\n- Slow sampling → Use ADVI for initialization\r\n- Discrete parameters → Use `pm.Metropolis()` for discrete vars\r\n\r\n### Variational Inference\r\n\r\nFast approximation for exploration or initialization:\r\n\r\n```python\r\nwith model:\r\n    approx = pm.fit(n=20000, method='advi')\r\n\r\n    # Use for initialization\r\n    start = approx.sample(return_inferencedata=False)[0]\r\n    idata = pm.sample(start=start)\r\n```\r\n\r\n**Trade-offs:**\r\n- Much faster than MCMC\r\n- Approximate (may underestimate uncertainty)\r\n- Good for large models or quick exploration\r\n\r\n**See:** `references/sampling_inference.md` for detailed sampling guide",
    "Quick Reference": "### Model Building\r\n```python\r\nwith pm.Model(coords={'var': names}) as model:\r\n    # Priors\r\n    param = pm.Normal('param', mu=0, sigma=1, dims='var')\r\n    # Likelihood\r\n    y = pm.Normal('y', mu=..., sigma=..., observed=data)\r\n```\r\n\r\n### Sampling\r\n```python\r\nidata = pm.sample(draws=2000, tune=1000, chains=4, target_accept=0.9)\r\n```\r\n\r\n### Diagnostics\r\n```python\r\nfrom scripts.model_diagnostics import check_diagnostics\r\ncheck_diagnostics(idata)\r\n```\r\n\r\n### Model Comparison\r\n```python\r\nfrom scripts.model_comparison import compare_models\r\ncompare_models({'m1': idata1, 'm2': idata2}, ic='loo')\r\n```\r\n\r\n### Predictions\r\n```python\r\nwith model:\r\n    pm.set_data({'X': X_new})\r\n    pred = pm.sample_posterior_predictive(idata.posterior)\r\n```",
    "Diagnostic Scripts": "### Comprehensive Diagnostics\r\n\r\n```python\r\nfrom scripts.model_diagnostics import create_diagnostic_report\r\n\r\ncreate_diagnostic_report(\r\n    idata,\r\n    var_names=['alpha', 'beta', 'sigma'],\r\n    output_dir='diagnostics/'\r\n)\r\n```\r\n\r\nCreates:\r\n- Trace plots\r\n- Rank plots (mixing check)\r\n- Autocorrelation plots\r\n- Energy plots\r\n- ESS evolution\r\n- Summary statistics CSV\r\n\r\n### Quick Diagnostic Check\r\n\r\n```python\r\nfrom scripts.model_diagnostics import check_diagnostics\r\n\r\nresults = check_diagnostics(idata)\r\n```\r\n\r\nChecks R-hat, ESS, divergences, and tree depth.",
    "Standard Bayesian Workflow": "y_pred_mean = post_pred.posterior_predictive['y_obs'].mean(dim=['chain', 'draw'])\r\ny_pred_hdi = az.hdi(post_pred.posterior_predictive, var_names=['y_obs'])\r\n```",
    "Additional Notes": "- PyMC integrates with ArviZ for visualization and diagnostics\r\n- Use `pm.model_to_graphviz(model)` to visualize model structure\r\n- Save results with `idata.to_netcdf('results.nc')`\r\n- Load with `az.from_netcdf('results.nc')`\r\n- For very large models, consider minibatch ADVI or data subsampling"
  }
}