{
  "sections": {
    "Good Tests": "| Quality | Good | Bad |\r\n|---------|------|-----|\r\n| **Minimal** | One thing. \"and\" in name? Split it. | `test('validates email and domain and whitespace')` |\r\n| **Clear** | Name describes behavior | `test('test1')` |\r\n| **Shows intent** | Demonstrates desired API | Obscures what code should do |",
    "Common Rationalizations": "| Excuse | Reality |\r\n|--------|---------|\r\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\r\n| \"I'll test after\" | Tests passing immediately prove nothing. |\r\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\r\n| \"Already manually tested\" | Ad-hoc ≠ systematic. No record, can't re-run. |\r\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Keeping unverified code is technical debt. |\r\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\r\n| \"Need to explore first\" | Fine. Throw away exploration, start with TDD. |\r\n| \"Test hard = design unclear\" | Listen to test. Hard to test = hard to use. |\r\n| \"TDD will slow me down\" | TDD faster than debugging. Pragmatic = test-first. |\r\n| \"Manual test faster\" | Manual doesn't prove edge cases. You'll re-test every change. |\r\n| \"Existing code has no tests\" | You're improving it. Add tests for existing code. |",
    "The Iron Law": "```\r\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\r\n```\r\n\r\nWrite code before the test? Delete it. Start over.\r\n\r\n**No exceptions:**\r\n- Don't keep it as \"reference\"\r\n- Don't \"adapt\" it while writing tests\r\n- Don't look at it\r\n- Delete means delete\r\n\r\nImplement fresh from tests. Period.",
    "Overview": "Write the test first. Watch it fail. Write minimal code to pass.\r\n\r\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\r\n\r\n**Violating the letter of the rules is violating the spirit of the rules.**",
    "When Stuck": "| Problem | Solution |\r\n|---------|----------|\r\n| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |\r\n| Test too complicated | Design too complicated. Simplify interface. |\r\n| Must mock everything | Code too coupled. Use dependency injection. |\r\n| Test setup huge | Extract helpers. Still complex? Simplify design. |",
    "Red Flags - STOP and Start Over": "- Code before test\r\n- Test after implementation\r\n- Test passes immediately\r\n- Can't explain why test failed\r\n- Tests added \"later\"\r\n- Rationalizing \"just this once\"\r\n- \"I already manually tested it\"\r\n- \"Tests after achieve the same purpose\"\r\n- \"It's about spirit not ritual\"\r\n- \"Keep as reference\" or \"adapt existing code\"\r\n- \"Already spent X hours, deleting is wasteful\"\r\n- \"TDD is dogmatic, I'm being pragmatic\"\r\n- \"This is different because...\"\r\n\r\n**All of these mean: Delete code. Start over with TDD.**",
    "Debugging Integration": "Bug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.\r\n\r\nNever fix bugs without a test.",
    "When to Use": "**Always:**\r\n- New features\r\n- Bug fixes\r\n- Refactoring\r\n- Behavior changes\r\n\r\n**Exceptions (ask your human partner):**\r\n- Throwaway prototypes\r\n- Generated code\r\n- Configuration files\r\n\r\nThinking \"skip TDD just this once\"? Stop. That's rationalization.",
    "Verification Checklist": "Before marking work complete:\r\n\r\n- [ ] Every new function/method has a test\r\n- [ ] Watched each test fail before implementing\r\n- [ ] Each test failed for expected reason (feature missing, not typo)\r\n- [ ] Wrote minimal code to pass each test\r\n- [ ] All tests pass\r\n- [ ] Output pristine (no errors, warnings)\r\n- [ ] Tests use real code (mocks only if unavoidable)\r\n- [ ] Edge cases and errors covered\r\n\r\nCan't check all boxes? You skipped TDD. Start over.",
    "Example: Bug Fix": "**Bug:** Empty email accepted\r\n\r\n**RED**\r\n```typescript\r\ntest('rejects empty email', async () => {\r\n  const result = await submitForm({ email: '' });\r\n  expect(result.error).toBe('Email required');\r\n});\r\n```\r\n\r\n**Verify RED**\r\n```bash\r\n$ npm test\r\nFAIL: expected 'Email required', got undefined\r\n```\r\n\r\n**GREEN**\r\n```typescript\r\nfunction submitForm(data: FormData) {\r\n  if (!data.email?.trim()) {\r\n    return { error: 'Email required' };\r\n  }\r\n  // ...\r\n}\r\n```\r\n\r\n**Verify GREEN**\r\n```bash\r\n$ npm test\r\nPASS\r\n```\r\n\r\n**REFACTOR**\r\nExtract validation for multiple fields if needed.",
    "Red-Green-Refactor": "```dot\r\ndigraph tdd_cycle {\r\n    rankdir=LR;\r\n    red [label=\"RED\\nWrite failing test\", shape=box, style=filled, fillcolor=\"#ffcccc\"];\r\n    verify_red [label=\"Verify fails\\ncorrectly\", shape=diamond];\r\n    green [label=\"GREEN\\nMinimal code\", shape=box, style=filled, fillcolor=\"#ccffcc\"];\r\n    verify_green [label=\"Verify passes\\nAll green\", shape=diamond];\r\n    refactor [label=\"REFACTOR\\nClean up\", shape=box, style=filled, fillcolor=\"#ccccff\"];\r\n    next [label=\"Next\", shape=ellipse];\r\n\r\n    red -> verify_red;\r\n    verify_red -> green [label=\"yes\"];\r\n    verify_red -> red [label=\"wrong\\nfailure\"];\r\n    green -> verify_green;\r\n    verify_green -> refactor [label=\"yes\"];\r\n    verify_green -> green [label=\"no\"];\r\n    refactor -> verify_green [label=\"stay\\ngreen\"];\r\n    verify_green -> next;\r\n    next -> red;\r\n}\r\n```\r\n\r\n### RED - Write Failing Test\r\n\r\nWrite one minimal test showing what should happen.\r\n\r\n<Good>\r\n```typescript\r\ntest('retries failed operations 3 times', async () => {\r\n  let attempts = 0;\r\n  const operation = () => {\r\n    attempts++;\r\n    if (attempts < 3) throw new Error('fail');\r\n    return 'success';\r\n  };\r\n\r\n  const result = await retryOperation(operation);\r\n\r\n  expect(result).toBe('success');\r\n  expect(attempts).toBe(3);\r\n});\r\n```\r\nClear name, tests real behavior, one thing\r\n</Good>\r\n\r\n<Bad>\r\n```typescript\r\ntest('retry works', async () => {\r\n  const mock = jest.fn()\r\n    .mockRejectedValueOnce(new Error())\r\n    .mockRejectedValueOnce(new Error())\r\n    .mockResolvedValueOnce('success');\r\n  await retryOperation(mock);\r\n  expect(mock).toHaveBeenCalledTimes(3);\r\n});\r\n```\r\nVague name, tests mock not code\r\n</Bad>\r\n\r\n**Requirements:**\r\n- One behavior\r\n- Clear name\r\n- Real code (no mocks unless unavoidable)\r\n\r\n### Verify RED - Watch It Fail\r\n\r\n**MANDATORY. Never skip.**\r\n\r\n```bash\r\nnpm test path/to/test.test.ts\r\n```\r\n\r\nConfirm:\r\n- Test fails (not errors)\r\n- Failure message is expected\r\n- Fails because feature missing (not typos)\r\n\r\n**Test passes?** You're testing existing behavior. Fix test.\r\n\r\n**Test errors?** Fix error, re-run until it fails correctly.\r\n\r\n### GREEN - Minimal Code\r\n\r\nWrite simplest code to pass the test.\r\n\r\n<Good>\r\n```typescript\r\nasync function retryOperation<T>(fn: () => Promise<T>): Promise<T> {\r\n  for (let i = 0; i < 3; i++) {\r\n    try {\r\n      return await fn();\r\n    } catch (e) {\r\n      if (i === 2) throw e;\r\n    }\r\n  }\r\n  throw new Error('unreachable');\r\n}\r\n```\r\nJust enough to pass\r\n</Good>\r\n\r\n<Bad>\r\n```typescript\r\nasync function retryOperation<T>(\r\n  fn: () => Promise<T>,\r\n  options?: {\r\n    maxRetries?: number;\r\n    backoff?: 'linear' | 'exponential';\r\n    onRetry?: (attempt: number) => void;\r\n  }\r\n): Promise<T> {\r\n  // YAGNI\r\n}\r\n```\r\nOver-engineered\r\n</Bad>\r\n\r\nDon't add features, refactor other code, or \"improve\" beyond the test.\r\n\r\n### Verify GREEN - Watch It Pass\r\n\r\n**MANDATORY.**\r\n\r\n```bash\r\nnpm test path/to/test.test.ts\r\n```\r\n\r\nConfirm:\r\n- Test passes\r\n- Other tests still pass\r\n- Output pristine (no errors, warnings)\r\n\r\n**Test fails?** Fix code, not test.\r\n\r\n**Other tests fail?** Fix now.\r\n\r\n### REFACTOR - Clean Up\r\n\r\nAfter green only:\r\n- Remove duplication\r\n- Improve names\r\n- Extract helpers\r\n\r\nKeep tests green. Don't add behavior.\r\n\r\n### Repeat\r\n\r\nNext failing test for next feature.",
    "Why Order Matters": "**\"I'll write tests after to verify it works\"**\r\n\r\nTests written after code pass immediately. Passing immediately proves nothing:\r\n- Might test wrong thing\r\n- Might test implementation, not behavior\r\n- Might miss edge cases you forgot\r\n- You never saw it catch the bug\r\n\r\nTest-first forces you to see the test fail, proving it actually tests something.\r\n\r\n**\"I already manually tested all the edge cases\"**\r\n\r\nManual testing is ad-hoc. You think you tested everything but:\r\n- No record of what you tested\r\n- Can't re-run when code changes\r\n- Easy to forget cases under pressure\r\n- \"It worked when I tried it\" ≠ comprehensive\r\n\r\nAutomated tests are systematic. They run the same way every time.\r\n\r\n**\"Deleting X hours of work is wasteful\"**\r\n\r\nSunk cost fallacy. The time is already gone. Your choice now:\r\n- Delete and rewrite with TDD (X more hours, high confidence)\r\n- Keep it and add tests after (30 min, low confidence, likely bugs)\r\n\r\nThe \"waste\" is keeping code you can't trust. Working code without real tests is technical debt.\r\n\r\n**\"TDD is dogmatic, being pragmatic means adapting\"**\r\n\r\nTDD IS pragmatic:\r\n- Finds bugs before commit (faster than debugging after)\r\n- Prevents regressions (tests catch breaks immediately)\r\n- Documents behavior (tests show how to use code)\r\n- Enables refactoring (change freely, tests catch breaks)\r\n\r\n\"Pragmatic\" shortcuts = debugging in production = slower.\r\n\r\n**\"Tests after achieve the same goals - it's spirit not ritual\"**\r\n\r\nNo. Tests-after answer \"What does this do?\" Tests-first answer \"What should this do?\"\r\n\r\nTests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.\r\n\r\nTests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).\r\n\r\n30 minutes of tests after ≠ TDD. You get coverage, lose proof tests work.",
    "Final Rule": "```\r\nProduction code → test exists and failed first\r\nOtherwise → not TDD\r\n```\r\n\r\nNo exceptions without your human partner's permission."
  },
  "id": "test-driven-development_obra",
  "name": "test-driven-development",
  "description": "Use when implementing any feature or bugfix, before writing implementation code - write the test first, watch it fail, write minimal code to pass; ensures tests actually verify behavior by requiring failure first"
}