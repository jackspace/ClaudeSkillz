{
  "description": "\"Chunked N-D arrays for cloud storage. Compressed arrays, parallel I/O, S3/GCS integration, NumPy/Dask/Xarray compatible, for large-scale scientific computing pipelines.\"",
  "references": {
    "files": [
      "references/api_reference.md"
    ]
  },
  "content": "### Installation\r\n\r\n```python\r\npip install zarr\r\n\r\nconda install --channel conda-forge zarr\r\n```\r\n\r\nRequires Python 3.11+. For cloud storage support, install additional packages:\r\n```python\r\npip install s3fs  # For S3\r\npip install gcsfs  # For Google Cloud Storage\r\n```\r\n\r\n### Basic Array Creation\r\n\r\n```python\r\nimport zarr\r\nimport numpy as np\r\n\r\nz = zarr.create_array(\r\n    store=\"data/my_array.zarr\",\r\n    shape=(10000, 10000),\r\n    chunks=(1000, 1000),\r\n    dtype=\"f4\"\r\n)\r\n\r\nz[:, :] = np.random.random((10000, 10000))\r\n\r\n\r\n### Creating Arrays\r\n\r\nZarr provides multiple convenience functions for array creation:\r\n\r\n```python\r\nz = zarr.zeros(shape=(10000, 10000), chunks=(1000, 1000), dtype='f4',\r\n               store='data.zarr')\r\n\r\nz = zarr.ones((5000, 5000), chunks=(500, 500))\r\nz = zarr.full((1000, 1000), fill_value=42, chunks=(100, 100))\r\n\r\ndata = np.arange(10000).reshape(100, 100)\r\nz = zarr.array(data, chunks=(10, 10), store='data.zarr')\r\n\r\nz2 = zarr.zeros_like(z)  # Matches shape, chunks, dtype of z\r\n```\r\n\r\n### Opening Existing Arrays\r\n\r\n```python\r\nz = zarr.open_array('data.zarr', mode='r+')\r\n\r\nz = zarr.open_array('data.zarr', mode='r')\r\n\r\nz = zarr.open('data.zarr')  # Returns Array or Group\r\n```\r\n\r\n### Reading and Writing Data\r\n\r\nZarr arrays support NumPy-like indexing:\r\n\r\n```python\r\nz[:] = 42\r\n\r\nz[0, :] = np.arange(100)\r\nz[10:20, 50:60] = np.random.random((10, 10))\r\n\r\ndata = z[0:100, 0:100]\r\nrow = z[5, :]\r\n\r\nz.vindex[[0, 5, 10], [2, 8, 15]]  # Coordinate indexing\r\nz.oindex[0:10, [5, 10, 15]]       # Orthogonal indexing\r\nz.blocks[0, 0]                     # Block/chunk indexing\r\n```\r\n\r\n### Resizing and Appending\r\n\r\n```python\r\nz.resize(15000, 15000)  # Expands or shrinks dimensions\r\n\r\n\r\nChunking is critical for performance. Choose chunk sizes and shapes based on access patterns.\r\n\r\n### Chunk Size Guidelines\r\n\r\n- **Minimum chunk size**: 1 MB recommended for optimal performance\r\n- **Balance**: Larger chunks = fewer metadata operations; smaller chunks = better parallel access\r\n- **Memory consideration**: Entire chunks must fit in memory during compression\r\n\r\n```python\r\nz = zarr.zeros(\r\n    shape=(10000, 10000),\r\n    chunks=(512, 512),  # ~1MB chunks\r\n    dtype='f4'\r\n)\r\n```\r\n\r\n### Aligning Chunks with Access Patterns\r\n\r\n**Critical**: Chunk shape dramatically affects performance based on how data is accessed.\r\n\r\n```python\r\nz = zarr.zeros((10000, 10000), chunks=(10, 10000))  # Chunk spans columns\r\n\r\nz = zarr.zeros((10000, 10000), chunks=(10000, 10))  # Chunk spans rows\r\n\r\nz = zarr.zeros((10000, 10000), chunks=(1000, 1000))  # Square chunks\r\n```\r\n\r\n**Performance example**: For a (200, 200, 200) array, reading along the first dimension:\r\n- Using chunks (1, 200, 200): ~107ms\r\n- Using chunks (200, 200, 1): ~1.65ms (65× faster!)\r\n\r\n### Sharding for Large-Scale Storage\r\n\r\nWhen arrays have millions of small chunks, use sharding to group chunks into larger storage objects:\r\n\r\n```python\r\nfrom zarr.codecs import ShardingCodec, BytesCodec\r\nfrom zarr.codecs.blosc import BloscCodec\r\n\r\n\r\nZarr applies compression per chunk to reduce storage while maintaining fast access.\r\n\r\n### Configuring Compression\r\n\r\n```python\r\nfrom zarr.codecs.blosc import BloscCodec\r\nfrom zarr.codecs import GzipCodec, ZstdCodec\r\n\r\nz = zarr.zeros((1000, 1000), chunks=(100, 100))  # Uses default compression\r\n\r\nz = zarr.create_array(\r\n    store='data.zarr',\r\n    shape=(1000, 1000),\r\n    chunks=(100, 100),\r\n    dtype='f4',\r\n    codecs=[BloscCodec(cname='zstd', clevel=5, shuffle='shuffle')]\r\n)\r\n\r\n\r\nz = zarr.create_array(\r\n    store='data.zarr',\r\n    shape=(1000, 1000),\r\n    chunks=(100, 100),\r\n    dtype='f4',\r\n    codecs=[GzipCodec(level=6)]\r\n)\r\n\r\nz = zarr.create_array(\r\n    store='data.zarr',\r\n    shape=(1000, 1000),\r\n    chunks=(100, 100),\r\n    dtype='f4',\r\n    codecs=[BytesCodec()]  # No compression\r\n)\r\n```\r\n\r\n### Compression Performance Tips\r\n\r\n- **Blosc** (default): Fast compression/decompression, good for interactive workloads\r\n- **Zstandard**: Better compression ratios, slightly slower than LZ4\r\n- **Gzip**: Maximum compression, slower performance\r\n- **LZ4**: Fastest compression, lower ratios\r\n- **Shuffle**: Enable shuffle filter for better compression on numeric data\r\n\r\n```python\r\ncodecs=[BloscCodec(cname='zstd', clevel=5, shuffle='shuffle')]\r\n\r\ncodecs=[BloscCodec(cname='lz4', clevel=1)]\r\n\r\n\r\nZarr supports multiple storage backends through a flexible storage interface.\r\n\r\n### Local Filesystem (Default)\r\n\r\n```python\r\nfrom zarr.storage import LocalStore\r\n\r\nstore = LocalStore('data/my_array.zarr')\r\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\r\n\r\nz = zarr.open_array('data/my_array.zarr', mode='w', shape=(1000, 1000),\r\n                    chunks=(100, 100))\r\n```\r\n\r\n### In-Memory Storage\r\n\r\n```python\r\nfrom zarr.storage import MemoryStore\r\n\r\nstore = MemoryStore()\r\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\r\n\r\n```\r\n\r\n### ZIP File Storage\r\n\r\n```python\r\nfrom zarr.storage import ZipStore\r\n\r\nstore = ZipStore('data.zip', mode='w')\r\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\r\nz[:] = np.random.random((1000, 1000))\r\nstore.close()  # IMPORTANT: Must close ZipStore\r\n\r\nstore = ZipStore('data.zip', mode='r')\r\nz = zarr.open_array(store=store)\r\ndata = z[:]\r\nstore.close()\r\n```\r\n\r\n### Cloud Storage (S3, GCS)\r\n\r\n```python\r\nimport s3fs\r\nimport zarr\r\n\r\ns3 = s3fs.S3FileSystem(anon=False)  # Use credentials\r\nstore = s3fs.S3Map(root='my-bucket/path/to/array.zarr', s3=s3)\r\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\r\nz[:] = data\r\n\r\n\r\nGroups organize multiple arrays hierarchically, similar to directories or HDF5 groups.\r\n\r\n### Creating and Using Groups\r\n\r\n```python\r\nroot = zarr.group(store='data/hierarchy.zarr')\r\n\r\ntemperature = root.create_group('temperature')\r\nprecipitation = root.create_group('precipitation')\r\n\r\ntemp_array = temperature.create_array(\r\n    name='t2m',\r\n    shape=(365, 720, 1440),\r\n    chunks=(1, 720, 1440),\r\n    dtype='f4'\r\n)\r\n\r\nprecip_array = precipitation.create_array(\r\n    name='prcp',\r\n    shape=(365, 720, 1440),\r\n    chunks=(1, 720, 1440),\r\n    dtype='f4'\r\n)\r\n\r\narray = root['temperature/t2m']\r\n\r\nprint(root.tree())\r\n```\r\n\r\n### H5py-Compatible API\r\n\r\nZarr provides an h5py-compatible interface for familiar HDF5 users:\r\n\r\n```python\r\nroot = zarr.group('data.zarr')\r\ndataset = root.create_dataset('my_data', shape=(1000, 1000), chunks=(100, 100),\r\n                              dtype='f4')\r\n\r\n\r\nAttach custom metadata to arrays and groups using attributes:\r\n\r\n```python\r\nz = zarr.zeros((1000, 1000), chunks=(100, 100))\r\nz.attrs['description'] = 'Temperature data in Kelvin'\r\nz.attrs['units'] = 'K'\r\nz.attrs['created'] = '2024-01-15'\r\nz.attrs['processing_version'] = 2.1\r\n\r\nprint(z.attrs['units'])  # Output: K\r\n\r\nroot = zarr.group('data.zarr')\r\nroot.attrs['project'] = 'Climate Analysis'\r\nroot.attrs['institution'] = 'Research Institute'\r\n\r\n\r\n### NumPy Integration\r\n\r\nZarr arrays implement the NumPy array interface:\r\n\r\n```python\r\nimport numpy as np\r\nimport zarr\r\n\r\nz = zarr.zeros((1000, 1000), chunks=(100, 100))\r\n\r\nresult = np.sum(z, axis=0)  # NumPy operates on Zarr array\r\nmean = np.mean(z[:100, :100])\r\n\r\nnumpy_array = z[:]  # Loads entire array into memory\r\n```\r\n\r\n### Dask Integration\r\n\r\nDask provides lazy, parallel computation on Zarr arrays:\r\n\r\n```python\r\nimport dask.array as da\r\nimport zarr\r\n\r\nz = zarr.open('data.zarr', mode='w', shape=(100000, 100000),\r\n              chunks=(1000, 1000), dtype='f4')\r\n\r\ndask_array = da.from_zarr('data.zarr')\r\n\r\nresult = dask_array.mean(axis=0).compute()  # Parallel computation\r\n\r\nlarge_array = da.random.random((100000, 100000), chunks=(1000, 1000))\r\nda.to_zarr(large_array, 'output.zarr')\r\n```\r\n\r\n**Benefits**:\r\n- Process datasets larger than memory\r\n- Automatic parallel computation across chunks\r\n- Efficient I/O with chunked storage\r\n\r\n### Xarray Integration\r\n\r\nXarray provides labeled, multidimensional arrays with Zarr backend:\r\n\r\n```python\r\nimport xarray as xr\r\nimport zarr\r\n\r\nds = xr.open_zarr('data.zarr')\r\n\r\nprint(ds)\r\n\r\ntemperature = ds['temperature']\r\n\r\nsubset = ds.sel(time='2024-01', lat=slice(30, 60))\r\n\r\nds.to_zarr('output.zarr')\r\n\r\n\r\n### Thread-Safe Operations\r\n\r\n```python\r\nfrom zarr import ThreadSynchronizer\r\nimport zarr\r\n\r\nsynchronizer = ThreadSynchronizer()\r\nz = zarr.open_array('data.zarr', mode='r+', shape=(10000, 10000),\r\n                    chunks=(1000, 1000), synchronizer=synchronizer)\r\n\r\n```\r\n\r\n### Process-Safe Operations\r\n\r\n```python\r\nfrom zarr import ProcessSynchronizer\r\nimport zarr\r\n\r\nsynchronizer = ProcessSynchronizer('sync_data.sync')\r\nz = zarr.open_array('data.zarr', mode='r+', shape=(10000, 10000),\r\n                    chunks=(1000, 1000), synchronizer=synchronizer)\r\n\r\n\r\nFor hierarchical stores with many arrays, consolidate metadata into a single file to reduce I/O operations:\r\n\r\n```python\r\nimport zarr\r\n\r\nroot = zarr.group('data.zarr')\r\n\r\nzarr.consolidate_metadata('data.zarr')\r\n\r\n\r\n### Checklist for Optimal Performance\r\n\r\n1. **Chunk Size**: Aim for 1-10 MB per chunk\r\n   ```python\r\n   # For float32: 1MB = 262,144 elements\r\n   chunks = (512, 512)  # 512×512×4 bytes = ~1MB\r\n   ```\r\n\r\n2. **Chunk Shape**: Align with access patterns\r\n   ```python\r\n   # Row-wise access → chunk spans columns: (small, large)\r\n   # Column-wise access → chunk spans rows: (large, small)\r\n   # Random access → balanced: (medium, medium)\r\n   ```\r\n\r\n3. **Compression**: Choose based on workload\r\n   ```python\r\n   # Interactive/fast: BloscCodec(cname='lz4')\r\n   # Balanced: BloscCodec(cname='zstd', clevel=5)\r\n   # Maximum compression: GzipCodec(level=9)\r\n   ```\r\n\r\n4. **Storage Backend**: Match to environment\r\n   ```python\r\n   # Local: LocalStore (default)\r\n   # Cloud: S3Map/GCSMap with consolidated metadata\r\n   # Temporary: MemoryStore\r\n   ```\r\n\r\n5. **Sharding**: Use for large-scale datasets\r\n   ```python\r\n   # When you have millions of small chunks\r\n   shards=(10*chunk_size, 10*chunk_size)\r\n   ```\r\n\r\n6. **Parallel I/O**: Use Dask for large operations\r\n   ```python\r\n   import dask.array as da\r\n   dask_array = da.from_zarr('data.zarr')\r\n   result = dask_array.compute(scheduler='threads', num_workers=8)\r\n   ```\r\n\r\n### Profiling and Debugging\r\n\r\n```python\r\nprint(z.info)\r\n\r\n\r\n\r\n### Pattern: Time Series Data\r\n\r\n```python\r\nz = zarr.open('timeseries.zarr', mode='a',\r\n              shape=(0, 720, 1440),  # Start with 0 time steps\r\n              chunks=(1, 720, 1440),  # One time step per chunk\r\n              dtype='f4')\r\n\r\nnew_data = np.random.random((1, 720, 1440))\r\nz.append(new_data, axis=0)\r\n```\r\n\r\n### Pattern: Large Matrix Operations\r\n\r\n```python\r\nimport dask.array as da\r\n\r\nz = zarr.open('matrix.zarr', mode='w',\r\n              shape=(100000, 100000),\r\n              chunks=(1000, 1000),\r\n              dtype='f8')\r\n\r\ndask_z = da.from_zarr('matrix.zarr')\r\nresult = (dask_z @ dask_z.T).compute()  # Parallel matrix multiply\r\n```\r\n\r\n### Pattern: Cloud-Native Workflow\r\n\r\n```python\r\nimport s3fs\r\nimport zarr\r\n\r\ns3 = s3fs.S3FileSystem()\r\nstore = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\r\n\r\nz = zarr.open_array(store=store, mode='w',\r\n                    shape=(10000, 10000),\r\n                    chunks=(500, 500),  # ~1MB chunks\r\n                    dtype='f4')\r\nz[:] = data\r\n\r\nzarr.consolidate_metadata(store)\r\n\r\nstore_read = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\r\nz_read = zarr.open_consolidated(store_read)\r\nsubset = z_read[0:100, 0:100]\r\n```\r\n\r\n### Pattern: Format Conversion\r\n\r\n```python\r\nimport h5py\r\nimport zarr\r\n\r\nwith h5py.File('data.h5', 'r') as h5:\r\n    dataset = h5['dataset_name']\r\n    z = zarr.array(dataset[:],\r\n                   chunks=(1000, 1000),\r\n                   store='data.zarr')\r\n\r\nimport numpy as np\r\ndata = np.load('data.npy')\r\nz = zarr.array(data, chunks='auto', store='data.zarr')\r\n\r\n\r\n### Issue: Slow Performance\r\n\r\n**Diagnosis**: Check chunk size and alignment\r\n```python\r\nprint(z.chunks)  # Are chunks appropriate size?\r\nprint(z.info)    # Check compression ratio\r\n```\r\n\r\n**Solutions**:\r\n- Increase chunk size to 1-10 MB\r\n- Align chunks with access pattern\r\n- Try different compression codecs\r\n- Use Dask for parallel operations\r\n\r\n### Issue: High Memory Usage\r\n\r\n**Cause**: Loading entire array or large chunks into memory\r\n\r\n**Solutions**:\r\n```python\r\nfor i in range(0, z.shape[0], 1000):\r\n    chunk = z[i:i+1000, :]\r\n    process(chunk)\r\n\r\nimport dask.array as da\r\ndask_z = da.from_zarr('data.zarr')\r\nresult = dask_z.mean().compute()  # Processes in chunks\r\n```\r\n\r\n### Issue: Cloud Storage Latency\r\n\r\n**Solutions**:\r\n```python\r\nzarr.consolidate_metadata(store)\r\nz = zarr.open_consolidated(store)\r\n\r\nchunks = (2000, 2000)  # Larger chunks for cloud\r\n\r\nshards = (10000, 10000)  # Groups many chunks\r\n```\r\n\r\n### Issue: Concurrent Write Conflicts\r\n\r\n**Solution**: Use synchronizers or ensure non-overlapping writes\r\n```python\r\nfrom zarr import ProcessSynchronizer\r\n\r\nsync = ProcessSynchronizer('sync.sync')\r\nz = zarr.open_array('data.zarr', mode='r+', synchronizer=sync)",
  "name": "zarr-python",
  "id": "scientific-pkg-zarr-python",
  "sections": {
    "Performance Optimization": "print(f\"Compressed size: {z.nbytes_stored / 1e6:.2f} MB\")\r\nprint(f\"Uncompressed size: {z.nbytes / 1e6:.2f} MB\")\r\nprint(f\"Compression ratio: {z.nbytes / z.nbytes_stored:.2f}x\")\r\n```",
    "Groups and Hierarchies": "grp = root.require_group('subgroup')\r\narr = grp.require_dataset('array', shape=(500, 500), chunks=(50, 50), dtype='i4')\r\n```",
    "Compression": "codecs=[GzipCodec(level=9)]\r\n```",
    "Overview": "Zarr is a Python library for storing large N-dimensional arrays with chunking and compression. Apply this skill for efficient parallel I/O, cloud-native workflows, and seamless integration with NumPy, Dask, and Xarray.",
    "Storage Backends": "import gcsfs\r\ngcs = gcsfs.GCSFileSystem(project='my-project')\r\nstore = gcsfs.GCSMap(root='my-bucket/path/to/array.zarr', gcs=gcs)\r\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\r\n```\r\n\r\n**Cloud Storage Best Practices**:\r\n- Use consolidated metadata to reduce latency: `zarr.consolidate_metadata(store)`\r\n- Align chunk sizes with cloud object sizing (typically 5-100 MB optimal)\r\n- Enable parallel writes using Dask for large-scale data\r\n- Consider sharding to reduce number of objects",
    "Chunking Strategies": "z = zarr.create_array(\r\n    store='data.zarr',\r\n    shape=(100000, 100000),\r\n    chunks=(100, 100),  # Small chunks for access\r\n    shards=(1000, 1000),  # Groups 100 chunks per shard\r\n    dtype='f4'\r\n)\r\n```\r\n\r\n**Benefits**:\r\n- Reduces file system overhead from millions of small files\r\n- Improves cloud storage performance (fewer object requests)\r\n- Prevents filesystem block size waste\r\n\r\n**Important**: Entire shards must fit in memory before writing.",
    "Common Patterns and Best Practices": "import xarray as xr\r\nds = xr.open_zarr('data.zarr')\r\nds.to_netcdf('data.nc')\r\n```",
    "Attributes and Metadata": "z2 = zarr.open('data.zarr')\r\nprint(z2.attrs['description'])\r\n```\r\n\r\n**Important**: Attributes must be JSON-serializable (strings, numbers, lists, dicts, booleans, null).",
    "Consolidated Metadata": "root = zarr.open_consolidated('data.zarr')\r\n```\r\n\r\n**Benefits**:\r\n- Reduces metadata read operations from N (one per array) to 1\r\n- Critical for cloud storage (reduces latency)\r\n- Speeds up `tree()` operations and group traversal\r\n\r\n**Cautions**:\r\n- Metadata can become stale if arrays update without re-consolidation\r\n- Not suitable for frequently-updated datasets\r\n- Multi-writer scenarios may have inconsistent reads",
    "Parallel Computing and Synchronization": "```\r\n\r\n**Note**:\r\n- Concurrent reads require no synchronization\r\n- Synchronization only needed for writes that may span chunk boundaries\r\n- Each process/thread writing to separate chunks needs no synchronization",
    "Integration with NumPy, Dask, and Xarray": "ds = xr.Dataset(\r\n    {\r\n        'temperature': (['time', 'lat', 'lon'], data),\r\n        'precipitation': (['time', 'lat', 'lon'], data2)\r\n    },\r\n    coords={\r\n        'time': pd.date_range('2024-01-01', periods=365),\r\n        'lat': np.arange(-90, 91, 1),\r\n        'lon': np.arange(-180, 180, 1)\r\n    }\r\n)\r\nds.to_zarr('climate_data.zarr')\r\n```\r\n\r\n**Benefits**:\r\n- Named dimensions and coordinates\r\n- Label-based indexing and selection\r\n- Integration with pandas for time series\r\n- NetCDF-like interface familiar to climate/geospatial scientists",
    "Common Issues and Solutions": "```",
    "Core Operations": "z.append(np.random.random((1000, 10000)), axis=0)  # Adds rows\r\n```",
    "Additional Resources": "For detailed API documentation, advanced usage, and the latest updates:\r\n\r\n- **Official Documentation**: https://zarr.readthedocs.io/\r\n- **Zarr Specifications**: https://zarr-specs.readthedocs.io/\r\n- **GitHub Repository**: https://github.com/zarr-developers/zarr-python\r\n- **Community Chat**: https://gitter.im/zarr-developers/community\r\n\r\n**Related Libraries**:\r\n- **Xarray**: https://docs.xarray.dev/ (labeled arrays)\r\n- **Dask**: https://docs.dask.org/ (parallel computing)\r\n- **NumCodecs**: https://numcodecs.readthedocs.io/ (compression codecs)",
    "Quick Start": "data = z[0:100, 0:100]  # Returns NumPy array\r\n```"
  }
}