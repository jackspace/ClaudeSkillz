{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/dimension-guide.md",
      "references/model-comparison.md",
      "references/rag-patterns.md",
      "references/top-errors.md",
      "references/vectorize-integration.md"
    ]
  },
  "content": "**Complete production-ready guide for Google Gemini embeddings API**\r\n\r\nThis skill provides comprehensive coverage of the `gemini-embedding-001` model for generating text embeddings, including SDK usage, REST API patterns, batch processing, RAG integration with Cloudflare Vectorize, and advanced use cases like semantic search and document clustering.\r\n\r\n---",
  "name": "google-gemini-embeddings",
  "id": "google-gemini-embeddings",
  "sections": {
    "Table of Contents": "1. [Quick Start](#1-quick-start)\r\n2. [gemini-embedding-001 Model](#2-gemini-embedding-001-model)\r\n3. [Basic Embeddings](#3-basic-embeddings)\r\n4. [Batch Embeddings](#4-batch-embeddings)\r\n5. [Task Types](#5-task-types)\r\n6. [RAG Patterns](#6-rag-patterns)\r\n7. [Semantic Search](#7-semantic-search)\r\n8. [Document Clustering](#8-document-clustering)\r\n9. [Error Handling](#9-error-handling)\r\n10. [Best Practices](#10-best-practices)\r\n\r\n---",
    "5. Task Types": "The `taskType` parameter optimizes embeddings for specific use cases. **Always specify a task type for best results.**\r\n\r\n### Available Task Types (8 total)\r\n\r\n| Task Type | Use Case | Example |\r\n|-----------|----------|---------|\r\n| **RETRIEVAL_QUERY** | User search queries | \"How do I fix a flat tire?\" |\r\n| **RETRIEVAL_DOCUMENT** | Documents to be indexed/searched | Product descriptions, articles |\r\n| **SEMANTIC_SIMILARITY** | Comparing text similarity | Duplicate detection, clustering |\r\n| **CLASSIFICATION** | Categorizing texts | Spam detection, sentiment analysis |\r\n| **CLUSTERING** | Grouping similar texts | Topic modeling, content organization |\r\n| **CODE_RETRIEVAL_QUERY** | Code search queries | \"function to sort array\" |\r\n| **QUESTION_ANSWERING** | Questions seeking answers | FAQ matching |\r\n| **FACT_VERIFICATION** | Verifying claims with evidence | Fact-checking systems |\r\n\r\n### When to Use Which\r\n\r\n**RAG Systems** (Retrieval Augmented Generation):\r\n```typescript\r\n// When embedding user queries\r\nconst queryEmbedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: userQuery,\r\n  config: { taskType: 'RETRIEVAL_QUERY' } // ← Use RETRIEVAL_QUERY\r\n});\r\n\r\n// When embedding documents for indexing\r\nconst docEmbedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: documentText,\r\n  config: { taskType: 'RETRIEVAL_DOCUMENT' } // ← Use RETRIEVAL_DOCUMENT\r\n});\r\n```\r\n\r\n**Semantic Search**:\r\n```typescript\r\nconst embedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text,\r\n  config: { taskType: 'SEMANTIC_SIMILARITY' }\r\n});\r\n```\r\n\r\n**Document Clustering**:\r\n```typescript\r\nconst embedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text,\r\n  config: { taskType: 'CLUSTERING' }\r\n});\r\n```\r\n\r\n### Impact on Quality\r\n\r\nUsing the correct task type **significantly improves** retrieval quality:\r\n\r\n```typescript\r\n// ❌ BAD: No task type specified\r\nconst embedding1 = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: userQuery\r\n});\r\n\r\n// ✅ GOOD: Task type specified\r\nconst embedding2 = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: userQuery,\r\n  config: { taskType: 'RETRIEVAL_QUERY' }\r\n});\r\n```\r\n\r\n**Result**: Using the right task type can improve search relevance by 10-30%.\r\n\r\n---",
    "Related Skills": "- **google-gemini-api** - Main Gemini API for text/image generation\r\n- **cloudflare-vectorize** - Vector database for storing embeddings\r\n- **cloudflare-workers-ai** - Workers AI embeddings (BGE models)\r\n\r\n---",
    "License": "MIT License - Free to use in personal and commercial projects.\r\n\r\n---\r\n\r\n**Questions or Issues?**\r\n\r\n- GitHub: https://github.com/jezweb/claude-skills\r\n- Email: jeremy@jezweb.net",
    "7. Semantic Search": "Semantic search finds content based on **meaning**, not just keyword matching.\r\n\r\n### Cosine Similarity\r\n\r\nCosine similarity measures how similar two embeddings are (range: -1 to 1, where 1 = identical):\r\n\r\n```typescript\r\nfunction cosineSimilarity(a: number[], b: number[]): number {\r\n  if (a.length !== b.length) {\r\n    throw new Error('Vectors must have same length');\r\n  }\r\n\r\n  let dotProduct = 0;\r\n  let magnitudeA = 0;\r\n  let magnitudeB = 0;\r\n\r\n  for (let i = 0; i < a.length; i++) {\r\n    dotProduct += a[i] * b[i];\r\n    magnitudeA += a[i] * a[i];\r\n    magnitudeB += b[i] * b[i];\r\n  }\r\n\r\n  if (magnitudeA === 0 || magnitudeB === 0) {\r\n    return 0;\r\n  }\r\n\r\n  return dotProduct / (Math.sqrt(magnitudeA) * Math.sqrt(magnitudeB));\r\n}\r\n```\r\n\r\n### Vector Normalization\r\n\r\nNormalize vectors to unit length for faster similarity calculations:\r\n\r\n```typescript\r\nfunction normalizeVector(vector: number[]): number[] {\r\n  const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));\r\n\r\n  if (magnitude === 0) {\r\n    return vector;\r\n  }\r\n\r\n  return vector.map(val => val / magnitude);\r\n}\r\n\r\n// Normalized vectors allow dot product instead of cosine similarity\r\nfunction dotProduct(a: number[], b: number[]): number {\r\n  return a.reduce((sum, val, i) => sum + val * b[i], 0);\r\n}\r\n```\r\n\r\n### Top-K Search\r\n\r\nFind the most similar documents to a query:\r\n\r\n```typescript\r\ninterface Document {\r\n  id: string;\r\n  text: string;\r\n  embedding: number[];\r\n}\r\n\r\nfunction topKSimilar(\r\n  queryEmbedding: number[],\r\n  documents: Document[],\r\n  k: number = 5\r\n): Array<{ document: Document; similarity: number }> {\r\n  // Calculate similarity for each document\r\n  const similarities = documents.map(doc => ({\r\n    document: doc,\r\n    similarity: cosineSimilarity(queryEmbedding, doc.embedding)\r\n  }));\r\n\r\n  // Sort by similarity (descending) and return top K\r\n  return similarities\r\n    .sort((a, b) => b.similarity - a.similarity)\r\n    .slice(0, k);\r\n}\r\n\r\n// Usage\r\nconst results = topKSimilar(queryEmbedding, documents, 5);\r\n\r\nresults.forEach(result => {\r\n  console.log(`Similarity: ${result.similarity.toFixed(4)}`);\r\n  console.log(`Text: ${result.document.text}\\n`);\r\n});\r\n```\r\n\r\n### Complete Semantic Search Example\r\n\r\nSee `templates/semantic-search.ts` for full implementation with Gemini API.\r\n\r\n---",
    "Success Metrics": "**Token Savings**: ~60% compared to manual implementation\r\n**Errors Prevented**: 8 documented errors with solutions\r\n**Production Tested**: ✅ Verified in RAG applications\r\n**Package Version**: @google/genai@1.27.0\r\n**Last Updated**: 2025-10-25\r\n\r\n---",
    "2. gemini-embedding-001 Model": "### Model Specifications\r\n\r\n**Current Model**: `gemini-embedding-001` (stable, production-ready)\r\n- **Status**: Stable\r\n- **Experimental**: `gemini-embedding-exp-03-07` (deprecated October 2025, do not use)\r\n\r\n### Dimensions\r\n\r\nThe model supports flexible output dimensionality using **Matryoshka Representation Learning**:\r\n\r\n| Dimension | Use Case | Storage | Performance |\r\n|-----------|----------|---------|-------------|\r\n| **768** | Recommended for most use cases | Low | Fast |\r\n| **1536** | Balance between accuracy and efficiency | Medium | Medium |\r\n| **3072** | Maximum accuracy (default) | High | Slower |\r\n| 128-3071 | Custom (any value in range) | Variable | Variable |\r\n\r\n**Default**: 3072 dimensions\r\n**Recommended**: 768, 1536, or 3072 for optimal performance\r\n\r\n### Context Window\r\n\r\n- **Input Limit**: 2,048 tokens per text\r\n- **Input Type**: Text only (no images, audio, or video)\r\n\r\n### Rate Limits\r\n\r\n| Tier | RPM | TPM | RPD | Requirements |\r\n|------|-----|-----|-----|--------------|\r\n| **Free** | 100 | 30,000 | 1,000 | No billing account |\r\n| **Tier 1** | 3,000 | 1,000,000 | - | Billing account linked |\r\n| **Tier 2** | 5,000 | 5,000,000 | - | $250+ spending, 30-day wait |\r\n| **Tier 3** | 10,000 | 10,000,000 | - | $1,000+ spending, 30-day wait |\r\n\r\n**RPM** = Requests Per Minute\r\n**TPM** = Tokens Per Minute\r\n**RPD** = Requests Per Day\r\n\r\n### Output Format\r\n\r\n```typescript\r\n{\r\n  embedding: {\r\n    values: number[] // Array of floating-point numbers\r\n  }\r\n}\r\n```\r\n\r\n---",
    "10. Best Practices": "### Always Do\r\n\r\n✅ **Specify Task Type**\r\n```typescript\r\n// Task type optimizes embeddings for your use case\r\nconst embedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text,\r\n  config: { taskType: 'RETRIEVAL_QUERY' } // ← Always specify\r\n});\r\n```\r\n\r\n✅ **Match Dimensions with Vectorize**\r\n```typescript\r\n// Ensure embeddings match your Vectorize index dimensions\r\nconst embedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text,\r\n  config: { outputDimensionality: 768 } // ← Match index\r\n});\r\n```\r\n\r\n✅ **Implement Rate Limiting**\r\n```typescript\r\n// Use exponential backoff for 429 errors\r\nasync function embedWithBackoff(text: string) {\r\n  // Implementation from Error Handling section\r\n}\r\n```\r\n\r\n✅ **Cache Embeddings**\r\n```typescript\r\n// Cache embeddings to avoid redundant API calls\r\nconst cache = new Map<string, number[]>();\r\n\r\nasync function getCachedEmbedding(text: string): Promise<number[]> {\r\n  if (cache.has(text)) {\r\n    return cache.get(text)!;\r\n  }\r\n\r\n  const response = await ai.models.embedContent({\r\n    model: 'gemini-embedding-001',\r\n    content: text,\r\n    config: { taskType: 'SEMANTIC_SIMILARITY' }\r\n  });\r\n\r\n  const embedding = response.embedding.values;\r\n  cache.set(text, embedding);\r\n  return embedding;\r\n}\r\n```\r\n\r\n✅ **Use Batch API for Multiple Texts**\r\n```typescript\r\n// Single batch request vs multiple individual requests\r\nconst embeddings = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  contents: texts, // Array of texts\r\n  config: { taskType: 'RETRIEVAL_DOCUMENT' }\r\n});\r\n```\r\n\r\n### Never Do\r\n\r\n❌ **Don't Skip Task Type**\r\n```typescript\r\n// Reduces quality by 10-30%\r\nconst embedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text\r\n  // Missing taskType!\r\n});\r\n```\r\n\r\n❌ **Don't Mix Different Dimensions**\r\n```typescript\r\n// Can't compare embeddings with different dimensions\r\nconst emb1 = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text1,\r\n  config: { outputDimensionality: 768 }\r\n});\r\n\r\nconst emb2 = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text2,\r\n  config: { outputDimensionality: 1536 } // Different dimensions!\r\n});\r\n\r\n// ❌ Can't calculate similarity between different dimensions\r\nconst similarity = cosineSimilarity(emb1.embedding.values, emb2.embedding.values);\r\n```\r\n\r\n❌ **Don't Use Wrong Task Type for RAG**\r\n```typescript\r\n// Reduces search quality\r\nconst queryEmbedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: query,\r\n  config: { taskType: 'RETRIEVAL_DOCUMENT' } // Wrong! Should be RETRIEVAL_QUERY\r\n});\r\n```\r\n\r\n---",
    "9. Error Handling": "### Common Errors\r\n\r\n**1. API Key Missing or Invalid**\r\n\r\n```typescript\r\n// ❌ Error: API key not set\r\nconst ai = new GoogleGenAI({});\r\n\r\n// ✅ Correct\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nif (!process.env.GEMINI_API_KEY) {\r\n  throw new Error('GEMINI_API_KEY environment variable not set');\r\n}\r\n```\r\n\r\n**2. Dimension Mismatch**\r\n\r\n```typescript\r\n// ❌ Error: Embedding has 3072 dims, Vectorize expects 768\r\nconst embedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text\r\n  // No outputDimensionality specified → defaults to 3072\r\n});\r\n\r\nawait env.VECTORIZE.insert([{\r\n  id: '1',\r\n  values: embedding.embedding.values // 3072 dims, but index is 768!\r\n}]);\r\n\r\n// ✅ Correct: Match dimensions\r\nconst embedding = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: text,\r\n  config: { outputDimensionality: 768 } // ← Match index dimensions\r\n});\r\n```\r\n\r\n**3. Rate Limiting**\r\n\r\n```typescript\r\n// ❌ Error: 429 Too Many Requests\r\nfor (let i = 0; i < 1000; i++) {\r\n  await ai.models.embedContent({ /* ... */ }); // Exceeds 100 RPM on free tier\r\n}\r\n\r\n// ✅ Correct: Implement rate limiting\r\nasync function embedWithRetry(text: string, maxRetries = 3) {\r\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\r\n    try {\r\n      return await ai.models.embedContent({\r\n        model: 'gemini-embedding-001',\r\n        content: text,\r\n        config: { taskType: 'SEMANTIC_SIMILARITY' }\r\n      });\r\n    } catch (error: any) {\r\n      if (error.status === 429 && attempt < maxRetries - 1) {\r\n        const delay = Math.pow(2, attempt) * 1000; // Exponential backoff\r\n        await new Promise(resolve => setTimeout(resolve, delay));\r\n        continue;\r\n      }\r\n      throw error;\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nSee `references/top-errors.md` for all 8 documented errors with detailed solutions.\r\n\r\n---",
    "Official Documentation": "- **Embeddings Guide**: https://ai.google.dev/gemini-api/docs/embeddings\r\n- **Model Spec**: https://ai.google.dev/gemini-api/docs/models/gemini#gemini-embedding-001\r\n- **Rate Limits**: https://ai.google.dev/gemini-api/docs/rate-limits\r\n- **SDK Reference**: https://www.npmjs.com/package/@google/genai\r\n- **Context7 Library ID**: `/websites/ai_google_dev_gemini-api`\r\n\r\n---",
    "1. Quick Start": "### Installation\r\n\r\nInstall the Google Generative AI SDK:\r\n\r\n```bash\r\nnpm install @google/genai@^1.27.0\r\n```\r\n\r\nFor TypeScript projects:\r\n\r\n```bash\r\nnpm install -D typescript@^5.0.0\r\n```\r\n\r\n### Environment Setup\r\n\r\nSet your Gemini API key as an environment variable:\r\n\r\n```bash\r\nexport GEMINI_API_KEY=\"your-api-key-here\"\r\n```\r\n\r\nGet your API key from: https://aistudio.google.com/apikey\r\n\r\n### First Embedding Example\r\n\r\n```typescript\r\nimport { GoogleGenAI } from \"@google/genai\";\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst response = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: 'What is the meaning of life?',\r\n  config: {\r\n    taskType: 'RETRIEVAL_QUERY',\r\n    outputDimensionality: 768\r\n  }\r\n});\r\n\r\nconsole.log(response.embedding.values); // [0.012, -0.034, ...]\r\nconsole.log(response.embedding.values.length); // 768\r\n```\r\n\r\n**Result**: A 768-dimension embedding vector representing the semantic meaning of the text.\r\n\r\n---",
    "8. Document Clustering": "Clustering groups similar documents together automatically.\r\n\r\n### K-Means Clustering\r\n\r\n```typescript\r\ninterface Cluster {\r\n  centroid: number[];\r\n  documents: number[][];\r\n}\r\n\r\nfunction kMeansClustering(\r\n  embeddings: number[][],\r\n  k: number = 3,\r\n  maxIterations: number = 100\r\n): Cluster[] {\r\n  // 1. Initialize centroids randomly\r\n  const centroids: number[][] = [];\r\n  for (let i = 0; i < k; i++) {\r\n    centroids.push(embeddings[Math.floor(Math.random() * embeddings.length)]);\r\n  }\r\n\r\n  // 2. Iterate until convergence\r\n  for (let iter = 0; iter < maxIterations; iter++) {\r\n    // Assign each embedding to nearest centroid\r\n    const clusters: number[][][] = Array(k).fill(null).map(() => []);\r\n\r\n    embeddings.forEach(embedding => {\r\n      let minDistance = Infinity;\r\n      let closestCluster = 0;\r\n\r\n      centroids.forEach((centroid, i) => {\r\n        const distance = 1 - cosineSimilarity(embedding, centroid);\r\n        if (distance < minDistance) {\r\n          minDistance = distance;\r\n          closestCluster = i;\r\n        }\r\n      });\r\n\r\n      clusters[closestCluster].push(embedding);\r\n    });\r\n\r\n    // Update centroids\r\n    let changed = false;\r\n    clusters.forEach((cluster, i) => {\r\n      if (cluster.length > 0) {\r\n        const newCentroid = cluster[0].map((_, dim) =>\r\n          cluster.reduce((sum, emb) => sum + emb[dim], 0) / cluster.length\r\n        );\r\n\r\n        if (cosineSimilarity(centroids[i], newCentroid) < 0.9999) {\r\n          changed = true;\r\n        }\r\n\r\n        centroids[i] = newCentroid;\r\n      }\r\n    });\r\n\r\n    if (!changed) break;\r\n  }\r\n\r\n  // Build final clusters\r\n  const finalClusters: Cluster[] = centroids.map((centroid, i) => ({\r\n    centroid,\r\n    documents: embeddings.filter(emb =>\r\n      cosineSimilarity(emb, centroid) === Math.max(\r\n        ...centroids.map(c => cosineSimilarity(emb, c))\r\n      )\r\n    )\r\n  }));\r\n\r\n  return finalClusters;\r\n}\r\n```\r\n\r\nSee `templates/clustering.ts` for complete implementation with examples.\r\n\r\n### Use Cases\r\n\r\n1. **Content Organization**: Automatically group similar articles, products, or documents\r\n2. **Duplicate Detection**: Find near-duplicate content\r\n3. **Topic Modeling**: Discover themes in large text corpora\r\n4. **Customer Feedback Analysis**: Group similar customer complaints or feature requests\r\n\r\n---",
    "6. RAG Patterns": "**RAG** (Retrieval Augmented Generation) combines vector search with LLM generation to create AI systems that answer questions using custom knowledge bases.\r\n\r\n### Complete RAG Workflow\r\n\r\n```\r\n1. Document Ingestion\r\n   ├── Chunk documents into smaller pieces\r\n   ├── Generate embeddings (RETRIEVAL_DOCUMENT)\r\n   └── Store in Vectorize\r\n\r\n2. Query Processing\r\n   ├── User submits query\r\n   ├── Generate query embedding (RETRIEVAL_QUERY)\r\n   └── Search Vectorize for similar documents\r\n\r\n3. Response Generation\r\n   ├── Retrieve top-k similar documents\r\n   ├── Pass documents as context to LLM\r\n   └── Stream response to user\r\n```\r\n\r\n### Document Ingestion Pipeline\r\n\r\n```typescript\r\nimport { GoogleGenAI } from \"@google/genai\";\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\n// 1. Chunk document into smaller pieces\r\nfunction chunkDocument(text: string, chunkSize: number = 500): string[] {\r\n  const words = text.split(' ');\r\n  const chunks: string[] = [];\r\n\r\n  for (let i = 0; i < words.length; i += chunkSize) {\r\n    chunks.push(words.slice(i, i + chunkSize).join(' '));\r\n  }\r\n\r\n  return chunks;\r\n}\r\n\r\n// 2. Generate embeddings for chunks\r\nasync function embedChunks(chunks: string[]): Promise<number[][]> {\r\n  const response = await ai.models.embedContent({\r\n    model: 'gemini-embedding-001',\r\n    contents: chunks,\r\n    config: {\r\n      taskType: 'RETRIEVAL_DOCUMENT', // ← Documents for indexing\r\n      outputDimensionality: 768 // ← Match Vectorize index dimensions\r\n    }\r\n  });\r\n\r\n  return response.embeddings.map(e => e.values);\r\n}\r\n\r\n// 3. Store in Cloudflare Vectorize\r\nasync function storeInVectorize(\r\n  env: Env,\r\n  chunks: string[],\r\n  embeddings: number[][]\r\n) {\r\n  const vectors = chunks.map((chunk, i) => ({\r\n    id: `doc-${Date.now()}-${i}`,\r\n    values: embeddings[i],\r\n    metadata: { text: chunk }\r\n  }));\r\n\r\n  await env.VECTORIZE.insert(vectors);\r\n}\r\n\r\n// Complete pipeline\r\nasync function ingestDocument(env: Env, documentText: string) {\r\n  const chunks = chunkDocument(documentText, 500);\r\n  const embeddings = await embedChunks(chunks);\r\n  await storeInVectorize(env, chunks, embeddings);\r\n\r\n  console.log(`Ingested ${chunks.length} chunks`);\r\n}\r\n```\r\n\r\n### Query Flow (Retrieve + Generate)\r\n\r\n```typescript\r\nasync function ragQuery(env: Env, userQuery: string): Promise<string> {\r\n  // 1. Embed user query\r\n  const queryResponse = await ai.models.embedContent({\r\n    model: 'gemini-embedding-001',\r\n    content: userQuery,\r\n    config: {\r\n      taskType: 'RETRIEVAL_QUERY', // ← Query, not document\r\n      outputDimensionality: 768\r\n    }\r\n  });\r\n\r\n  const queryEmbedding = queryResponse.embedding.values;\r\n\r\n  // 2. Search Vectorize for similar documents\r\n  const results = await env.VECTORIZE.query(queryEmbedding, {\r\n    topK: 5,\r\n    returnMetadata: true\r\n  });\r\n\r\n  // 3. Extract context from top results\r\n  const context = results.matches\r\n    .map(match => match.metadata.text)\r\n    .join('\\n\\n');\r\n\r\n  // 4. Generate response with context\r\n  const response = await ai.models.generateContent({\r\n    model: 'gemini-2.5-flash',\r\n    contents: `Context:\\n${context}\\n\\nQuestion: ${userQuery}\\n\\nAnswer based on the context above:`\r\n  });\r\n\r\n  return response.text;\r\n}\r\n```\r\n\r\n### Integration with Cloudflare Vectorize\r\n\r\n**Create Vectorize Index** (768 dimensions for Gemini):\r\n\r\n```bash\r\nnpx wrangler vectorize create gemini-embeddings --dimensions 768 --metric cosine\r\n```\r\n\r\n**Bind in wrangler.jsonc**:\r\n\r\n```jsonc\r\n{\r\n  \"name\": \"my-rag-app\",\r\n  \"main\": \"src/index.ts\",\r\n  \"compatibility_date\": \"2025-10-25\",\r\n  \"vectorize\": {\r\n    \"bindings\": [\r\n      {\r\n        \"binding\": \"VECTORIZE\",\r\n        \"index_name\": \"gemini-embeddings\"\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n**Complete RAG Worker**:\r\n\r\nSee `templates/rag-with-vectorize.ts` for full implementation.\r\n\r\n---",
    "Using Bundled Resources": "### Templates (templates/)\r\n\r\n- `package.json` - Package configuration with verified versions\r\n- `basic-embeddings.ts` - Single text embedding with SDK\r\n- `embeddings-fetch.ts` - Fetch-based for Cloudflare Workers\r\n- `batch-embeddings.ts` - Batch processing with rate limiting\r\n- `rag-with-vectorize.ts` - Complete RAG implementation with Vectorize\r\n- `semantic-search.ts` - Cosine similarity and top-K search\r\n- `clustering.ts` - K-means clustering implementation\r\n\r\n### References (references/)\r\n\r\n- `model-comparison.md` - Compare Gemini vs OpenAI vs Workers AI embeddings\r\n- `vectorize-integration.md` - Cloudflare Vectorize setup and patterns\r\n- `rag-patterns.md` - Complete RAG implementation strategies\r\n- `dimension-guide.md` - Choosing the right dimensions (768 vs 1536 vs 3072)\r\n- `top-errors.md` - 8 common errors and detailed solutions\r\n\r\n### Scripts (scripts/)\r\n\r\n- `check-versions.sh` - Verify @google/genai package version is current\r\n\r\n---",
    "3. Basic Embeddings": "### SDK Approach (Node.js)\r\n\r\n**Single text embedding**:\r\n\r\n```typescript\r\nimport { GoogleGenAI } from \"@google/genai\";\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst response = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: 'The quick brown fox jumps over the lazy dog',\r\n  config: {\r\n    taskType: 'SEMANTIC_SIMILARITY',\r\n    outputDimensionality: 768\r\n  }\r\n});\r\n\r\nconsole.log(response.embedding.values);\r\n// [0.00388, -0.00762, 0.01543, ...]\r\n```\r\n\r\n### Fetch Approach (Cloudflare Workers)\r\n\r\n**For Workers/edge environments without SDK support**:\r\n\r\n```typescript\r\nexport default {\r\n  async fetch(request: Request, env: Env): Promise<Response> {\r\n    const apiKey = env.GEMINI_API_KEY;\r\n    const text = \"What is the meaning of life?\";\r\n\r\n    const response = await fetch(\r\n      'https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent',\r\n      {\r\n        method: 'POST',\r\n        headers: {\r\n          'x-goog-api-key': apiKey,\r\n          'Content-Type': 'application/json'\r\n        },\r\n        body: JSON.stringify({\r\n          content: {\r\n            parts: [{ text }]\r\n          },\r\n          taskType: 'RETRIEVAL_QUERY',\r\n          outputDimensionality: 768\r\n        })\r\n      }\r\n    );\r\n\r\n    const data = await response.json();\r\n\r\n    // Response format:\r\n    // {\r\n    //   embedding: {\r\n    //     values: [0.012, -0.034, ...]\r\n    //   }\r\n    // }\r\n\r\n    return new Response(JSON.stringify(data), {\r\n      headers: { 'Content-Type': 'application/json' }\r\n    });\r\n  }\r\n};\r\n```\r\n\r\n### Response Parsing\r\n\r\n```typescript\r\ninterface EmbeddingResponse {\r\n  embedding: {\r\n    values: number[];\r\n  };\r\n}\r\n\r\nconst response: EmbeddingResponse = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  content: 'Sample text',\r\n  config: { taskType: 'SEMANTIC_SIMILARITY' }\r\n});\r\n\r\nconst embedding: number[] = response.embedding.values;\r\nconst dimensions: number = embedding.length; // 3072 by default\r\n```\r\n\r\n---",
    "4. Batch Embeddings": "### Multiple Texts in One Request (SDK)\r\n\r\nGenerate embeddings for multiple texts simultaneously:\r\n\r\n```typescript\r\nimport { GoogleGenAI } from \"@google/genai\";\r\n\r\nconst ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\r\n\r\nconst texts = [\r\n  \"What is the meaning of life?\",\r\n  \"How does photosynthesis work?\",\r\n  \"Tell me about the history of the internet.\"\r\n];\r\n\r\nconst response = await ai.models.embedContent({\r\n  model: 'gemini-embedding-001',\r\n  contents: texts, // Array of strings\r\n  config: {\r\n    taskType: 'RETRIEVAL_DOCUMENT',\r\n    outputDimensionality: 768\r\n  }\r\n});\r\n\r\n// Process each embedding\r\nresponse.embeddings.forEach((embedding, index) => {\r\n  console.log(`Text ${index}: ${texts[index]}`);\r\n  console.log(`Embedding: ${embedding.values.slice(0, 5)}...`);\r\n  console.log(`Dimensions: ${embedding.values.length}`);\r\n});\r\n```\r\n\r\n### Batch REST API (fetch)\r\n\r\nUse the `batchEmbedContents` endpoint:\r\n\r\n```typescript\r\nconst response = await fetch(\r\n  'https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents',\r\n  {\r\n    method: 'POST',\r\n    headers: {\r\n      'x-goog-api-key': apiKey,\r\n      'Content-Type': 'application/json'\r\n    },\r\n    body: JSON.stringify({\r\n      requests: texts.map(text => ({\r\n        model: 'models/gemini-embedding-001',\r\n        content: {\r\n          parts: [{ text }]\r\n        },\r\n        taskType: 'RETRIEVAL_DOCUMENT'\r\n      }))\r\n    })\r\n  }\r\n);\r\n\r\nconst data = await response.json();\r\n// data.embeddings: Array of {values: number[]}\r\n```\r\n\r\n### Chunking for Rate Limits\r\n\r\nWhen processing large datasets, chunk requests to stay within rate limits:\r\n\r\n```typescript\r\nasync function batchEmbedWithRateLimit(\r\n  texts: string[],\r\n  batchSize: number = 100, // Free tier: 100 RPM\r\n  delayMs: number = 60000 // 1 minute delay between batches\r\n): Promise<number[][]> {\r\n  const allEmbeddings: number[][] = [];\r\n\r\n  for (let i = 0; i < texts.length; i += batchSize) {\r\n    const batch = texts.slice(i, i + batchSize);\r\n\r\n    console.log(`Processing batch ${i / batchSize + 1} (${batch.length} texts)`);\r\n\r\n    const response = await ai.models.embedContent({\r\n      model: 'gemini-embedding-001',\r\n      contents: batch,\r\n      config: {\r\n        taskType: 'RETRIEVAL_DOCUMENT',\r\n        outputDimensionality: 768\r\n      }\r\n    });\r\n\r\n    allEmbeddings.push(...response.embeddings.map(e => e.values));\r\n\r\n    // Wait before next batch (except last batch)\r\n    if (i + batchSize < texts.length) {\r\n      await new Promise(resolve => setTimeout(resolve, delayMs));\r\n    }\r\n  }\r\n\r\n  return allEmbeddings;\r\n}\r\n\r\n// Usage\r\nconst embeddings = await batchEmbedWithRateLimit(documents, 100);\r\n```\r\n\r\n### Performance Optimization\r\n\r\n**Tips**:\r\n1. Use batch API when embedding multiple texts (single request vs multiple requests)\r\n2. Choose lower dimensions (768) for faster processing and less storage\r\n3. Implement exponential backoff for rate limit errors\r\n4. Cache embeddings to avoid redundant API calls\r\n\r\n---"
  }
}