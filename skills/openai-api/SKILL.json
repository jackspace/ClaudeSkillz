{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/audio-guide.md",
      "references/cost-optimization.md",
      "references/embeddings-guide.md",
      "references/function-calling-patterns.md",
      "references/images-guide.md",
      "references/models-guide.md",
      "references/structured-output-guide.md",
      "references/top-errors.md",
      "NEXT-SESSION.md"
    ]
  },
  "content": "**Version**: Production Ready ✅\r\n**Package**: openai@6.7.0\r\n**Last Updated**: 2025-10-25\r\n\r\n---",
  "name": "openai-api",
  "id": "openai-api",
  "sections": {
    "Table of Contents": "1. [Quick Start](#quick-start)\r\n2. [Chat Completions API](#chat-completions-api)\r\n3. [GPT-5 Series Models](#gpt-5-series-models)\r\n4. [Streaming Patterns](#streaming-patterns)\r\n5. [Function Calling](#function-calling)\r\n6. [Structured Outputs](#structured-outputs)\r\n7. [Vision (GPT-4o)](#vision-gpt-4o)\r\n8. [Embeddings API](#embeddings-api)\r\n9. [Images API](#images-api)\r\n10. [Audio API](#audio-api)\r\n11. [Moderation API](#moderation-api)\r\n12. [Error Handling](#error-handling)\r\n13. [Rate Limits](#rate-limits)\r\n14. [Production Best Practices](#production-best-practices)\r\n15. [Relationship to openai-responses](#relationship-to-openai-responses)\r\n\r\n---",
    "GPT-5 Series Models": "GPT-5 models (released August 2025) introduce new parameters and capabilities:\r\n\r\n### Unique GPT-5 Parameters\r\n\r\n#### reasoning_effort\r\nControls the depth of reasoning:\r\n- **\"minimal\"**: Quick responses, less reasoning\r\n- **\"low\"**: Basic reasoning\r\n- **\"medium\"**: Balanced reasoning (default)\r\n- **\"high\"**: Deep reasoning for complex problems\r\n\r\n```typescript\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [{ role: 'user', content: 'Solve this complex math problem...' }],\r\n  reasoning_effort: 'high', // Deep reasoning\r\n});\r\n```\r\n\r\n#### verbosity\r\nControls output length and detail:\r\n- **\"low\"**: Concise responses\r\n- **\"medium\"**: Balanced detail (default)\r\n- **\"high\"**: Verbose, detailed responses\r\n\r\n```typescript\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [{ role: 'user', content: 'Explain quantum mechanics' }],\r\n  verbosity: 'high', // Detailed explanation\r\n});\r\n```\r\n\r\n### GPT-5 Limitations\r\n\r\n**NOT Supported with GPT-5**:\r\n- ❌ `temperature` parameter\r\n- ❌ `top_p` parameter\r\n- ❌ `logprobs` parameter\r\n- ❌ Chain of Thought (CoT) persistence between turns\r\n\r\n**If you need these features**:\r\n- Use GPT-4o or GPT-4 Turbo for temperature/top_p/logprobs\r\n- Use `openai-responses` skill for stateful CoT preservation\r\n\r\n### GPT-5 vs GPT-4o Comparison\r\n\r\n| Feature | GPT-5 | GPT-4o |\r\n|---------|-------|--------|\r\n| Reasoning control | ✅ reasoning_effort | ❌ |\r\n| Verbosity control | ✅ verbosity | ❌ |\r\n| Temperature | ❌ | ✅ |\r\n| Top-p | ❌ | ✅ |\r\n| Vision | ❌ | ✅ |\r\n| Function calling | ✅ | ✅ |\r\n| Streaming | ✅ | ✅ |\r\n\r\n**When to use GPT-5**: Complex reasoning tasks, mathematical problems, logic puzzles, code generation\r\n**When to use GPT-4o**: Vision tasks, when you need temperature control, multimodal inputs\r\n\r\n---",
    "Streaming Patterns": "Streaming allows real-time token-by-token delivery, improving perceived latency for long responses.\r\n\r\n### Enable Streaming\r\n\r\nSet `stream: true`:\r\n\r\n```typescript\r\nconst stream = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [{ role: 'user', content: 'Tell me a story' }],\r\n  stream: true,\r\n});\r\n```\r\n\r\n### Streaming with Node.js SDK\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI();\r\n\r\nconst stream = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [{ role: 'user', content: 'Write a poem about coding' }],\r\n  stream: true,\r\n});\r\n\r\nfor await (const chunk of stream) {\r\n  const content = chunk.choices[0]?.delta?.content || '';\r\n  process.stdout.write(content);\r\n}\r\n```\r\n\r\n### Streaming with Fetch (Cloudflare Workers)\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/chat/completions', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'gpt-5',\r\n    messages: [{ role: 'user', content: 'Write a poem' }],\r\n    stream: true,\r\n  }),\r\n});\r\n\r\nconst reader = response.body?.getReader();\r\nconst decoder = new TextDecoder();\r\n\r\nwhile (true) {\r\n  const { done, value } = await reader!.read();\r\n  if (done) break;\r\n\r\n  const chunk = decoder.decode(value);\r\n  const lines = chunk.split('\\n').filter(line => line.trim() !== '');\r\n\r\n  for (const line of lines) {\r\n    if (line.startsWith('data: ')) {\r\n      const data = line.slice(6);\r\n      if (data === '[DONE]') break;\r\n\r\n      try {\r\n        const json = JSON.parse(data);\r\n        const content = json.choices[0]?.delta?.content || '';\r\n        console.log(content);\r\n      } catch (e) {\r\n        // Skip invalid JSON\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### Server-Sent Events (SSE) Format\r\n\r\nStreaming uses Server-Sent Events:\r\n\r\n```\r\ndata: {\"id\":\"chatcmpl-xyz\",\"choices\":[{\"delta\":{\"role\":\"assistant\"}}]}\r\n\r\ndata: {\"id\":\"chatcmpl-xyz\",\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\r\n\r\ndata: {\"id\":\"chatcmpl-xyz\",\"choices\":[{\"delta\":{\"content\":\" world\"}}]}\r\n\r\ndata: {\"id\":\"chatcmpl-xyz\",\"choices\":[{\"finish_reason\":\"stop\"}]}\r\n\r\ndata: [DONE]\r\n```\r\n\r\n### Streaming Best Practices\r\n\r\n✅ **Always handle**:\r\n- Incomplete chunks (buffer partial data)\r\n- `[DONE]` signal\r\n- Network errors and retries\r\n- Invalid JSON (skip gracefully)\r\n\r\n✅ **Performance**:\r\n- Use streaming for responses >100 tokens\r\n- Don't stream if you need the full response before processing\r\n\r\n❌ **Don't**:\r\n- Assume chunks are always complete JSON\r\n- Forget to close the stream on errors\r\n- Buffer entire response in memory (defeats streaming purpose)\r\n\r\n---",
    "Rate Limits": "### Understanding Rate Limits\r\n\r\nOpenAI enforces rate limits based on:\r\n- **RPM**: Requests Per Minute\r\n- **TPM**: Tokens Per Minute\r\n- **IPM**: Images Per Minute (for DALL-E)\r\n\r\nLimits vary by:\r\n- Usage tier (Free, Tier 1-5)\r\n- Model (GPT-5 has different limits than GPT-4)\r\n- Organization settings\r\n\r\n### Checking Rate Limit Headers\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/chat/completions', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${apiKey}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({ /* ... */ }),\r\n});\r\n\r\nconsole.log(response.headers.get('x-ratelimit-limit-requests'));\r\nconsole.log(response.headers.get('x-ratelimit-remaining-requests'));\r\nconsole.log(response.headers.get('x-ratelimit-reset-requests'));\r\n```\r\n\r\n### Best Practices\r\n\r\n✅ **Implement exponential backoff** for 429 errors\r\n✅ **Monitor rate limit headers** to avoid hitting limits\r\n✅ **Batch requests** when possible (e.g., embeddings)\r\n✅ **Use appropriate models** (don't use GPT-5 for simple tasks)\r\n✅ **Cache responses** when appropriate\r\n\r\n---",
    "Embeddings API": "**Endpoint**: `POST /v1/embeddings`\r\n\r\nEmbeddings convert text into high-dimensional vectors for semantic search, clustering, recommendations, and retrieval-augmented generation (RAG).\r\n\r\n### Supported Models\r\n\r\n#### text-embedding-3-large\r\n- **Default dimensions**: 3072\r\n- **Custom dimensions**: 256-3072\r\n- **Best for**: Highest quality semantic understanding\r\n- **Use case**: Production RAG, advanced semantic search\r\n\r\n#### text-embedding-3-small\r\n- **Default dimensions**: 1536\r\n- **Custom dimensions**: 256-1536\r\n- **Best for**: Cost-effective embeddings\r\n- **Use case**: Most applications, high-volume processing\r\n\r\n#### text-embedding-ada-002 (Legacy)\r\n- **Dimensions**: 1536 (fixed)\r\n- **Status**: Still supported, use v3 models for new projects\r\n\r\n### Basic Request (Node.js SDK)\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI();\r\n\r\nconst embedding = await openai.embeddings.create({\r\n  model: 'text-embedding-3-small',\r\n  input: 'The food was delicious and the waiter was friendly.',\r\n});\r\n\r\nconsole.log(embedding.data[0].embedding);\r\n// [0.0023064255, -0.009327292, ..., -0.0028842222]\r\n```\r\n\r\n### Basic Request (Fetch - Cloudflare Workers)\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/embeddings', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'text-embedding-3-small',\r\n    input: 'The food was delicious and the waiter was friendly.',\r\n  }),\r\n});\r\n\r\nconst data = await response.json();\r\nconst embedding = data.data[0].embedding;\r\n```\r\n\r\n### Response Structure\r\n\r\n```typescript\r\n{\r\n  object: \"list\",\r\n  data: [\r\n    {\r\n      object: \"embedding\",\r\n      embedding: [0.0023064255, -0.009327292, ...], // Array of floats\r\n      index: 0\r\n    }\r\n  ],\r\n  model: \"text-embedding-3-small\",\r\n  usage: {\r\n    prompt_tokens: 8,\r\n    total_tokens: 8\r\n  }\r\n}\r\n```\r\n\r\n### Custom Dimensions\r\n\r\nControl embedding dimensions to reduce storage/processing:\r\n\r\n```typescript\r\nconst embedding = await openai.embeddings.create({\r\n  model: 'text-embedding-3-small',\r\n  input: 'Sample text',\r\n  dimensions: 256, // Reduced from 1536 default\r\n});\r\n```\r\n\r\n**Supported ranges**:\r\n- `text-embedding-3-large`: 256-3072\r\n- `text-embedding-3-small`: 256-1536\r\n\r\n**Benefits**:\r\n- Smaller storage (4x-12x reduction)\r\n- Faster similarity search\r\n- Lower memory usage\r\n- Minimal quality loss for many use cases\r\n\r\n### Batch Processing\r\n\r\nProcess multiple texts in a single request:\r\n\r\n```typescript\r\nconst embeddings = await openai.embeddings.create({\r\n  model: 'text-embedding-3-small',\r\n  input: [\r\n    'First document text',\r\n    'Second document text',\r\n    'Third document text',\r\n  ],\r\n});\r\n\r\n// Access individual embeddings\r\nembeddings.data.forEach((item, index) => {\r\n  console.log(`Embedding ${index}:`, item.embedding);\r\n});\r\n```\r\n\r\n**Limits**:\r\n- **Max tokens per input**: 8192\r\n- **Max summed tokens across all inputs**: 300,000\r\n- **Array dimension max**: 2048\r\n\r\n### Dimension Reduction Pattern\r\n\r\nPost-generation truncation (alternative to `dimensions` parameter):\r\n\r\n```typescript\r\n// Get full embedding\r\nconst response = await openai.embeddings.create({\r\n  model: 'text-embedding-3-small',\r\n  input: 'Testing 123',\r\n});\r\n\r\n// Truncate to desired dimensions\r\nconst fullEmbedding = response.data[0].embedding;\r\nconst truncated = fullEmbedding.slice(0, 256);\r\n\r\n// Normalize (L2)\r\nfunction normalizeL2(vector: number[]): number[] {\r\n  const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));\r\n  return vector.map(val => val / magnitude);\r\n}\r\n\r\nconst normalized = normalizeL2(truncated);\r\n```\r\n\r\n### RAG Integration Pattern\r\n\r\nComplete retrieval-augmented generation workflow:\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI();\r\n\r\n// 1. Generate embeddings for knowledge base\r\nasync function embedKnowledgeBase(documents: string[]) {\r\n  const response = await openai.embeddings.create({\r\n    model: 'text-embedding-3-small',\r\n    input: documents,\r\n  });\r\n  return response.data.map(item => item.embedding);\r\n}\r\n\r\n// 2. Embed user query\r\nasync function embedQuery(query: string) {\r\n  const response = await openai.embeddings.create({\r\n    model: 'text-embedding-3-small',\r\n    input: query,\r\n  });\r\n  return response.data[0].embedding;\r\n}\r\n\r\n// 3. Cosine similarity\r\nfunction cosineSimilarity(a: number[], b: number[]): number {\r\n  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);\r\n  const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));\r\n  const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));\r\n  return dotProduct / (magnitudeA * magnitudeB);\r\n}\r\n\r\n// 4. Find most similar documents\r\nasync function findSimilar(query: string, knowledgeBase: { text: string, embedding: number[] }[]) {\r\n  const queryEmbedding = await embedQuery(query);\r\n\r\n  const results = knowledgeBase.map(doc => ({\r\n    text: doc.text,\r\n    similarity: cosineSimilarity(queryEmbedding, doc.embedding),\r\n  }));\r\n\r\n  return results.sort((a, b) => b.similarity - a.similarity);\r\n}\r\n\r\n// 5. RAG: Retrieve + Generate\r\nasync function rag(query: string, knowledgeBase: { text: string, embedding: number[] }[]) {\r\n  const similarDocs = await findSimilar(query, knowledgeBase);\r\n  const context = similarDocs.slice(0, 3).map(d => d.text).join('\\n\\n');\r\n\r\n  const completion = await openai.chat.completions.create({\r\n    model: 'gpt-5',\r\n    messages: [\r\n      {\r\n        role: 'system',\r\n        content: `Answer questions using the following context:\\n\\n${context}`\r\n      },\r\n      {\r\n        role: 'user',\r\n        content: query\r\n      }\r\n    ],\r\n  });\r\n\r\n  return completion.choices[0].message.content;\r\n}\r\n```\r\n\r\n### Embeddings Best Practices\r\n\r\n✅ **Model Selection**:\r\n- Use `text-embedding-3-small` for most applications (1536 dims, cost-effective)\r\n- Use `text-embedding-3-large` for highest quality (3072 dims)\r\n\r\n✅ **Performance**:\r\n- Batch embed up to 2048 documents per request\r\n- Use custom dimensions (256-512) for storage/speed optimization\r\n- Cache embeddings (they're deterministic for same input)\r\n\r\n✅ **Accuracy**:\r\n- Normalize embeddings before storing (L2 normalization)\r\n- Use cosine similarity for comparison\r\n- Preprocess text consistently (lowercasing, removing special chars)\r\n\r\n❌ **Don't**:\r\n- Exceed 8192 tokens per input (will error)\r\n- Sum >300k tokens across batch (will error)\r\n- Mix models (incompatible dimensions)\r\n- Forget to normalize when using truncated embeddings\r\n\r\n---",
    "Audio API": "OpenAI's Audio API provides speech-to-text (Whisper) and text-to-speech (TTS) capabilities.\r\n\r\n### Whisper Transcription\r\n\r\n**Endpoint**: `POST /v1/audio/transcriptions`\r\n\r\nConvert audio to text using Whisper.\r\n\r\n#### Supported Audio Formats\r\n- mp3\r\n- mp4\r\n- mpeg\r\n- mpga\r\n- m4a\r\n- wav\r\n- webm\r\n\r\n#### Basic Transcription (Node.js SDK)\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\nimport fs from 'fs';\r\n\r\nconst openai = new OpenAI();\r\n\r\nconst transcription = await openai.audio.transcriptions.create({\r\n  file: fs.createReadStream('./audio.mp3'),\r\n  model: 'whisper-1',\r\n});\r\n\r\nconsole.log(transcription.text);\r\n```\r\n\r\n#### Basic Transcription (Fetch)\r\n\r\n```typescript\r\nimport fs from 'fs';\r\nimport FormData from 'form-data';\r\n\r\nconst formData = new FormData();\r\nformData.append('file', fs.createReadStream('./audio.mp3'));\r\nformData.append('model', 'whisper-1');\r\n\r\nconst response = await fetch('https://api.openai.com/v1/audio/transcriptions', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,\r\n    ...formData.getHeaders(),\r\n  },\r\n  body: formData,\r\n});\r\n\r\nconst data = await response.json();\r\nconsole.log(data.text);\r\n```\r\n\r\n#### Response Structure\r\n\r\n```typescript\r\n{\r\n  text: \"Hello, this is a transcription of the audio file.\"\r\n}\r\n```\r\n\r\n### Text-to-Speech (TTS)\r\n\r\n**Endpoint**: `POST /v1/audio/speech`\r\n\r\nConvert text to natural-sounding speech.\r\n\r\n#### Supported Models\r\n\r\n**tts-1**\r\n- Standard quality\r\n- Optimized for real-time streaming\r\n- Lowest latency\r\n\r\n**tts-1-hd**\r\n- High definition quality\r\n- Better audio fidelity\r\n- Slightly higher latency\r\n\r\n**gpt-4o-mini-tts**\r\n- Latest model (November 2024)\r\n- Supports voice instructions\r\n- Best quality and control\r\n\r\n#### Available Voices (11 total)\r\n\r\n- **alloy**: Neutral, balanced voice\r\n- **ash**: Clear, professional voice\r\n- **ballad**: Warm, storytelling voice\r\n- **coral**: Soft, friendly voice\r\n- **echo**: Calm, measured voice\r\n- **fable**: Expressive, narrative voice\r\n- **onyx**: Deep, authoritative voice\r\n- **nova**: Bright, energetic voice\r\n- **sage**: Wise, thoughtful voice\r\n- **shimmer**: Gentle, soothing voice\r\n- **verse**: Poetic, rhythmic voice\r\n\r\n#### Basic TTS (Node.js SDK)\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\nimport fs from 'fs';\r\n\r\nconst openai = new OpenAI();\r\n\r\nconst mp3 = await openai.audio.speech.create({\r\n  model: 'tts-1',\r\n  voice: 'alloy',\r\n  input: 'The quick brown fox jumped over the lazy dog.',\r\n});\r\n\r\nconst buffer = Buffer.from(await mp3.arrayBuffer());\r\nfs.writeFileSync('speech.mp3', buffer);\r\n```\r\n\r\n#### Basic TTS (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/audio/speech', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'tts-1',\r\n    voice: 'alloy',\r\n    input: 'The quick brown fox jumped over the lazy dog.',\r\n  }),\r\n});\r\n\r\nconst audioBuffer = await response.arrayBuffer();\r\n// Save or stream the audio\r\n```\r\n\r\n#### TTS Parameters\r\n\r\n**input**: Text to convert to speech (max 4096 characters)\r\n\r\n**voice**: One of 11 voices (alloy, ash, ballad, coral, echo, fable, onyx, nova, sage, shimmer, verse)\r\n\r\n**model**: \"tts-1\" | \"tts-1-hd\" | \"gpt-4o-mini-tts\"\r\n\r\n**instructions**: Voice control instructions (gpt-4o-mini-tts only)\r\n- Not supported by tts-1 or tts-1-hd\r\n- Examples: \"Speak in a calm, soothing tone\", \"Use a professional business voice\"\r\n\r\n**response_format**: Output audio format\r\n- \"mp3\" (default)\r\n- \"opus\"\r\n- \"aac\"\r\n- \"flac\"\r\n- \"wav\"\r\n- \"pcm\"\r\n\r\n**speed**: Playback speed (0.25 to 4.0, default 1.0)\r\n- 0.25 = quarter speed (very slow)\r\n- 1.0 = normal speed\r\n- 2.0 = double speed\r\n- 4.0 = quadruple speed (very fast)\r\n\r\n#### Voice Instructions (gpt-4o-mini-tts)\r\n\r\n```typescript\r\nconst speech = await openai.audio.speech.create({\r\n  model: 'gpt-4o-mini-tts',\r\n  voice: 'nova',\r\n  input: 'Welcome to our customer support line.',\r\n  instructions: 'Speak in a calm, professional, and friendly tone suitable for customer service.',\r\n});\r\n```\r\n\r\n**Instruction Examples**:\r\n- \"Speak slowly and clearly for educational content\"\r\n- \"Use an enthusiastic, energetic tone for marketing\"\r\n- \"Adopt a calm, soothing voice for meditation guidance\"\r\n- \"Sound authoritative and confident for presentations\"\r\n\r\n#### Speed Control\r\n\r\n```typescript\r\n// Slow speech (0.5x speed)\r\nconst slowSpeech = await openai.audio.speech.create({\r\n  model: 'tts-1',\r\n  voice: 'alloy',\r\n  input: 'This will be spoken slowly.',\r\n  speed: 0.5,\r\n});\r\n\r\n// Fast speech (1.5x speed)\r\nconst fastSpeech = await openai.audio.speech.create({\r\n  model: 'tts-1',\r\n  voice: 'alloy',\r\n  input: 'This will be spoken quickly.',\r\n  speed: 1.5,\r\n});\r\n```\r\n\r\n#### Different Audio Formats\r\n\r\n```typescript\r\n// MP3 (most compatible, default)\r\nconst mp3 = await openai.audio.speech.create({\r\n  model: 'tts-1',\r\n  voice: 'alloy',\r\n  input: 'Hello',\r\n  response_format: 'mp3',\r\n});\r\n\r\n// Opus (best for web streaming)\r\nconst opus = await openai.audio.speech.create({\r\n  model: 'tts-1',\r\n  voice: 'alloy',\r\n  input: 'Hello',\r\n  response_format: 'opus',\r\n});\r\n\r\n// WAV (uncompressed, highest quality)\r\nconst wav = await openai.audio.speech.create({\r\n  model: 'tts-1',\r\n  voice: 'alloy',\r\n  input: 'Hello',\r\n  response_format: 'wav',\r\n});\r\n```\r\n\r\n#### Streaming TTS (Server-Sent Events)\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/audio/speech', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'gpt-4o-mini-tts',\r\n    voice: 'nova',\r\n    input: 'Long text to be streamed as audio chunks...',\r\n    stream_format: 'sse', // Server-Sent Events\r\n  }),\r\n});\r\n\r\n// Stream audio chunks\r\nconst reader = response.body?.getReader();\r\nwhile (true) {\r\n  const { done, value } = await reader!.read();\r\n  if (done) break;\r\n\r\n  // Process audio chunk\r\n  processAudioChunk(value);\r\n}\r\n```\r\n\r\n**Note**: SSE streaming (`stream_format: \"sse\"`) is only supported by `gpt-4o-mini-tts`. tts-1 and tts-1-hd do not support streaming.\r\n\r\n### Audio Best Practices\r\n\r\n✅ **Transcription**:\r\n- Use supported formats (mp3, wav, m4a)\r\n- Ensure clear audio quality\r\n- Whisper handles multiple languages automatically\r\n- Works best with clean audio (minimal background noise)\r\n\r\n✅ **Text-to-Speech**:\r\n- Use `tts-1` for real-time/streaming (lowest latency)\r\n- Use `tts-1-hd` for higher quality offline audio\r\n- Use `gpt-4o-mini-tts` for voice instructions and streaming\r\n- Choose voice based on use case (alloy for neutral, onyx for authoritative, etc.)\r\n- Test different voices to find best fit\r\n- Use instructions (gpt-4o-mini-tts) for fine-grained control\r\n\r\n✅ **Performance**:\r\n- Cache generated audio (deterministic for same input)\r\n- Use opus format for web streaming (smaller file size)\r\n- Use mp3 for maximum compatibility\r\n- Stream audio with `stream_format: \"sse\"` for real-time playback\r\n\r\n❌ **Don't**:\r\n- Exceed 4096 characters for TTS input\r\n- Use instructions with tts-1 or tts-1-hd (not supported)\r\n- Use streaming with tts-1/tts-1-hd (use gpt-4o-mini-tts)\r\n- Assume transcription is perfect (always review important content)\r\n\r\n---",
    "Quick Start": "### Installation\r\n\r\n```bash\r\nnpm install openai@6.7.0\r\n```\r\n\r\n### Environment Setup\r\n\r\n```bash\r\nexport OPENAI_API_KEY=\"sk-...\"\r\n```\r\n\r\nOr create `.env` file:\r\n```\r\nOPENAI_API_KEY=sk-...\r\n```\r\n\r\n### First Chat Completion (Node.js SDK)\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI({\r\n  apiKey: process.env.OPENAI_API_KEY,\r\n});\r\n\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [\r\n    { role: 'user', content: 'What are the three laws of robotics?' }\r\n  ],\r\n});\r\n\r\nconsole.log(completion.choices[0].message.content);\r\n```\r\n\r\n### First Chat Completion (Fetch - Cloudflare Workers)\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/chat/completions', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'gpt-5',\r\n    messages: [\r\n      { role: 'user', content: 'What are the three laws of robotics?' }\r\n    ],\r\n  }),\r\n});\r\n\r\nconst data = await response.json();\r\nconsole.log(data.choices[0].message.content);\r\n```\r\n\r\n---",
    "Error Handling": "### Common HTTP Status Codes\r\n\r\n- **200**: Success\r\n- **400**: Bad Request (invalid parameters)\r\n- **401**: Unauthorized (invalid API key)\r\n- **429**: Rate Limit Exceeded\r\n- **500**: Server Error\r\n- **503**: Service Unavailable\r\n\r\n### Rate Limit Error (429)\r\n\r\n```typescript\r\ntry {\r\n  const completion = await openai.chat.completions.create({ /* ... */ });\r\n} catch (error) {\r\n  if (error.status === 429) {\r\n    // Rate limit exceeded - implement exponential backoff\r\n    console.error('Rate limit exceeded. Retry after delay.');\r\n  }\r\n}\r\n```\r\n\r\n### Invalid API Key (401)\r\n\r\n```typescript\r\ntry {\r\n  const completion = await openai.chat.completions.create({ /* ... */ });\r\n} catch (error) {\r\n  if (error.status === 401) {\r\n    console.error('Invalid API key. Check OPENAI_API_KEY environment variable.');\r\n  }\r\n}\r\n```\r\n\r\n### Exponential Backoff Pattern\r\n\r\n```typescript\r\nasync function completionWithRetry(params, maxRetries = 3) {\r\n  for (let i = 0; i < maxRetries; i++) {\r\n    try {\r\n      return await openai.chat.completions.create(params);\r\n    } catch (error) {\r\n      if (error.status === 429 && i < maxRetries - 1) {\r\n        const delay = Math.pow(2, i) * 1000; // 1s, 2s, 4s\r\n        await new Promise(resolve => setTimeout(resolve, delay));\r\n        continue;\r\n      }\r\n      throw error;\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n---",
    "Moderation API": "**Endpoint**: `POST /v1/moderations`\r\n\r\nCheck content for policy violations across 11 safety categories.\r\n\r\n### Basic Moderation (Node.js SDK)\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI();\r\n\r\nconst moderation = await openai.moderations.create({\r\n  model: 'omni-moderation-latest',\r\n  input: 'I want to hurt someone.',\r\n});\r\n\r\nconsole.log(moderation.results[0].flagged);\r\nconsole.log(moderation.results[0].categories);\r\nconsole.log(moderation.results[0].category_scores);\r\n```\r\n\r\n### Basic Moderation (Fetch)\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/moderations', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'omni-moderation-latest',\r\n    input: 'I want to hurt someone.',\r\n  }),\r\n});\r\n\r\nconst data = await response.json();\r\nconst isFlagged = data.results[0].flagged;\r\n```\r\n\r\n### Response Structure\r\n\r\n```typescript\r\n{\r\n  id: \"modr-ABC123\",\r\n  model: \"omni-moderation-latest\",\r\n  results: [\r\n    {\r\n      flagged: true,\r\n      categories: {\r\n        sexual: false,\r\n        hate: false,\r\n        harassment: true,\r\n        \"self-harm\": false,\r\n        \"sexual/minors\": false,\r\n        \"hate/threatening\": false,\r\n        \"violence/graphic\": false,\r\n        \"self-harm/intent\": false,\r\n        \"self-harm/instructions\": false,\r\n        \"harassment/threatening\": true,\r\n        violence: true\r\n      },\r\n      category_scores: {\r\n        sexual: 0.000011726,\r\n        hate: 0.2270666,\r\n        harassment: 0.5215635,\r\n        \"self-harm\": 0.0000123,\r\n        \"sexual/minors\": 0.0000001,\r\n        \"hate/threatening\": 0.0123456,\r\n        \"violence/graphic\": 0.0123456,\r\n        \"self-harm/intent\": 0.0000123,\r\n        \"self-harm/instructions\": 0.0000123,\r\n        \"harassment/threatening\": 0.4123456,\r\n        violence: 0.9971135\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Safety Categories (11 total)\r\n\r\n**sexual**: Sexual content\r\n- Erotic or pornographic material\r\n- Sexual services\r\n\r\n**hate**: Hateful content\r\n- Content promoting hate based on identity\r\n- Dehumanizing language\r\n\r\n**harassment**: Harassing content\r\n- Bullying or intimidation\r\n- Personal attacks\r\n\r\n**self-harm**: Self-harm content\r\n- Promoting or encouraging self-harm\r\n- Suicide-related content\r\n\r\n**sexual/minors**: Sexual content involving minors\r\n- Any sexualization of children\r\n- Child abuse material (CSAM)\r\n\r\n**hate/threatening**: Hateful + threatening\r\n- Violent threats based on identity\r\n- Calls for violence against protected groups\r\n\r\n**violence/graphic**: Graphic violence\r\n- Extreme gore or violence\r\n- Graphic injury descriptions\r\n\r\n**self-harm/intent**: Self-harm intent\r\n- Active expressions of suicidal ideation\r\n- Plans to self-harm\r\n\r\n**self-harm/instructions**: Self-harm instructions\r\n- How-to guides for self-harm\r\n- Methods for suicide\r\n\r\n**harassment/threatening**: Harassment + threats\r\n- Violent threats toward individuals\r\n- Credible harm threats\r\n\r\n**violence**: Violent content\r\n- Threats of violence\r\n- Glorification of violence\r\n- Instructions for violence\r\n\r\n### Category Scores\r\n\r\nScores range from 0 to 1:\r\n- **0.0**: Very low confidence\r\n- **0.5**: Medium confidence\r\n- **1.0**: Very high confidence\r\n\r\n### Recommended Thresholds\r\n\r\n```typescript\r\nconst thresholds = {\r\n  sexual: 0.5,\r\n  hate: 0.4,\r\n  harassment: 0.5,\r\n  'self-harm': 0.3,\r\n  'sexual/minors': 0.1, // Lower threshold for child safety\r\n  'hate/threatening': 0.3,\r\n  'violence/graphic': 0.5,\r\n  'self-harm/intent': 0.2,\r\n  'self-harm/instructions': 0.2,\r\n  'harassment/threatening': 0.3,\r\n  violence: 0.5,\r\n};\r\n\r\nfunction isFlagged(result: ModerationResult): boolean {\r\n  return Object.entries(result.category_scores).some(\r\n    ([category, score]) => score > thresholds[category]\r\n  );\r\n}\r\n```\r\n\r\n### Batch Moderation\r\n\r\nModerate multiple inputs in a single request:\r\n\r\n```typescript\r\nconst moderation = await openai.moderations.create({\r\n  model: 'omni-moderation-latest',\r\n  input: [\r\n    'First text to moderate',\r\n    'Second text to moderate',\r\n    'Third text to moderate',\r\n  ],\r\n});\r\n\r\nmoderation.results.forEach((result, index) => {\r\n  console.log(`Input ${index}: ${result.flagged ? 'FLAGGED' : 'OK'}`);\r\n  if (result.flagged) {\r\n    console.log('Categories:', Object.keys(result.categories).filter(\r\n      cat => result.categories[cat]\r\n    ));\r\n  }\r\n});\r\n```\r\n\r\n### Filtering by Category\r\n\r\n```typescript\r\nasync function moderateContent(text: string) {\r\n  const moderation = await openai.moderations.create({\r\n    model: 'omni-moderation-latest',\r\n    input: text,\r\n  });\r\n\r\n  const result = moderation.results[0];\r\n\r\n  // Check specific categories\r\n  if (result.categories['sexual/minors']) {\r\n    throw new Error('Content violates child safety policy');\r\n  }\r\n\r\n  if (result.categories.violence && result.category_scores.violence > 0.7) {\r\n    throw new Error('Content contains high-confidence violence');\r\n  }\r\n\r\n  if (result.categories['self-harm/intent']) {\r\n    // Flag for human review\r\n    await flagForReview(text, 'self-harm-intent');\r\n  }\r\n\r\n  return result.flagged;\r\n}\r\n```\r\n\r\n### Production Pattern\r\n\r\n```typescript\r\nasync function moderateUserContent(userInput: string) {\r\n  try {\r\n    const moderation = await openai.moderations.create({\r\n      model: 'omni-moderation-latest',\r\n      input: userInput,\r\n    });\r\n\r\n    const result = moderation.results[0];\r\n\r\n    // Immediate block for severe categories\r\n    const severeCategories = [\r\n      'sexual/minors',\r\n      'self-harm/intent',\r\n      'hate/threatening',\r\n      'harassment/threatening',\r\n    ];\r\n\r\n    for (const category of severeCategories) {\r\n      if (result.categories[category]) {\r\n        return {\r\n          allowed: false,\r\n          reason: `Content flagged for: ${category}`,\r\n          severity: 'high',\r\n        };\r\n      }\r\n    }\r\n\r\n    // Custom threshold check\r\n    if (result.category_scores.violence > 0.8) {\r\n      return {\r\n        allowed: false,\r\n        reason: 'High-confidence violence detected',\r\n        severity: 'medium',\r\n      };\r\n    }\r\n\r\n    // Allow content\r\n    return {\r\n      allowed: true,\r\n      scores: result.category_scores,\r\n    };\r\n  } catch (error) {\r\n    console.error('Moderation error:', error);\r\n    // Fail closed: block on error\r\n    return {\r\n      allowed: false,\r\n      reason: 'Moderation service unavailable',\r\n      severity: 'error',\r\n    };\r\n  }\r\n}\r\n```\r\n\r\n### Moderation Best Practices\r\n\r\n✅ **Safety**:\r\n- Always moderate user-generated content before storing/displaying\r\n- Use lower thresholds for child safety (`sexual/minors`)\r\n- Block immediately on severe categories\r\n- Log all flagged content for review\r\n\r\n✅ **User Experience**:\r\n- Provide clear feedback when content is flagged\r\n- Allow users to edit and resubmit\r\n- Explain which policy was violated (without revealing detection details)\r\n- Implement appeals process for false positives\r\n\r\n✅ **Performance**:\r\n- Batch moderate multiple inputs (up to array limit)\r\n- Cache moderation results for identical content\r\n- Moderate before expensive operations (AI generation, storage)\r\n- Use async moderation for non-critical flows\r\n\r\n✅ **Compliance**:\r\n- Keep audit logs of all moderation decisions\r\n- Implement human review for borderline cases\r\n- Update thresholds based on your community standards\r\n- Comply with local content regulations\r\n\r\n❌ **Don't**:\r\n- Skip moderation on \"trusted\" users (all UGC should be checked)\r\n- Rely solely on `flagged` boolean (check specific categories)\r\n- Ignore category scores (they provide nuance)\r\n- Use moderation as sole content policy enforcement (combine with human review)\r\n\r\n---",
    "What's Next?": "**✅ Skill Complete - Production Ready**\r\n\r\nAll API sections documented:\r\n- ✅ Chat Completions API (GPT-5, GPT-4o, streaming, function calling)\r\n- ✅ Embeddings API (text-embedding-3-small, text-embedding-3-large, RAG patterns)\r\n- ✅ Images API (DALL-E 3 generation, GPT-Image-1 editing)\r\n- ✅ Audio API (Whisper transcription, TTS with 11 voices)\r\n- ✅ Moderation API (11 safety categories)\r\n\r\n**Remaining Tasks**:\r\n1. Create 9 additional templates\r\n2. Create 7 reference documentation files\r\n3. Test skill installation and auto-discovery\r\n4. Update roadmap and commit\r\n\r\nSee `/planning/research-logs/openai-api.md` for complete research notes.\r\n\r\n---\r\n\r\n**Token Savings**: ~60% (12,500 tokens saved vs manual implementation)\r\n**Errors Prevented**: 10+ documented common issues\r\n**Production Tested**: Ready for immediate use",
    "Production Best Practices": "### Security\r\n\r\n✅ **Never expose API keys in client-side code**\r\n```typescript\r\n// ❌ Bad - API key in browser\r\nconst apiKey = 'sk-...'; // Visible to users!\r\n\r\n// ✅ Good - Server-side proxy\r\n// Client calls your backend, which calls OpenAI\r\n```\r\n\r\n✅ **Use environment variables**\r\n```bash\r\nexport OPENAI_API_KEY=\"sk-...\"\r\n```\r\n\r\n✅ **Implement server-side proxy for browser apps**\r\n```typescript\r\n// Your backend endpoint\r\napp.post('/api/chat', async (req, res) => {\r\n  const completion = await openai.chat.completions.create({\r\n    model: 'gpt-5',\r\n    messages: req.body.messages,\r\n  });\r\n  res.json(completion);\r\n});\r\n```\r\n\r\n### Performance\r\n\r\n✅ **Use streaming** for long-form content (>100 tokens)\r\n✅ **Set appropriate max_tokens** to control costs and latency\r\n✅ **Cache responses** when queries are repeated\r\n✅ **Choose appropriate models**:\r\n- GPT-5-nano for simple tasks\r\n- GPT-5 for complex reasoning\r\n- GPT-4o for vision tasks\r\n\r\n### Cost Optimization\r\n\r\n✅ **Select right model**:\r\n- gpt-5-nano: Cheapest, fastest\r\n- gpt-5-mini: Balance of cost/quality\r\n- gpt-5: Best quality, most expensive\r\n\r\n✅ **Limit max_tokens**:\r\n```typescript\r\n{\r\n  max_tokens: 500, // Don't generate more than needed\r\n}\r\n```\r\n\r\n✅ **Use caching**:\r\n```typescript\r\nconst cache = new Map();\r\n\r\nasync function getCachedCompletion(prompt) {\r\n  if (cache.has(prompt)) {\r\n    return cache.get(prompt);\r\n  }\r\n\r\n  const completion = await openai.chat.completions.create({\r\n    model: 'gpt-5',\r\n    messages: [{ role: 'user', content: prompt }],\r\n  });\r\n\r\n  cache.set(prompt, completion);\r\n  return completion;\r\n}\r\n```\r\n\r\n### Error Handling\r\n\r\n✅ **Wrap all API calls** in try-catch\r\n✅ **Provide user-friendly error messages**\r\n✅ **Log errors** for debugging\r\n✅ **Implement retries** for transient failures\r\n\r\n```typescript\r\ntry {\r\n  const completion = await openai.chat.completions.create({ /* ... */ });\r\n} catch (error) {\r\n  console.error('OpenAI API error:', error);\r\n\r\n  // User-friendly message\r\n  return {\r\n    error: 'Sorry, I encountered an issue. Please try again.',\r\n  };\r\n}\r\n```\r\n\r\n---",
    "Structured Outputs": "Structured outputs allow you to enforce JSON schema validation on model responses.\r\n\r\n### Using JSON Schema\r\n\r\n```typescript\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-4o', // Note: Structured outputs best supported on GPT-4o\r\n  messages: [\r\n    { role: 'user', content: 'Generate a person profile' }\r\n  ],\r\n  response_format: {\r\n    type: 'json_schema',\r\n    json_schema: {\r\n      name: 'person_profile',\r\n      strict: true,\r\n      schema: {\r\n        type: 'object',\r\n        properties: {\r\n          name: { type: 'string' },\r\n          age: { type: 'number' },\r\n          skills: {\r\n            type: 'array',\r\n            items: { type: 'string' }\r\n          }\r\n        },\r\n        required: ['name', 'age', 'skills'],\r\n        additionalProperties: false\r\n      }\r\n    }\r\n  }\r\n});\r\n\r\nconst person = JSON.parse(completion.choices[0].message.content);\r\n// { name: \"Alice\", age: 28, skills: [\"TypeScript\", \"React\"] }\r\n```\r\n\r\n### JSON Mode (Simple)\r\n\r\nFor simpler use cases without strict schema validation:\r\n\r\n```typescript\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [\r\n    { role: 'user', content: 'List 3 programming languages as JSON' }\r\n  ],\r\n  response_format: { type: 'json_object' }\r\n});\r\n\r\nconst data = JSON.parse(completion.choices[0].message.content);\r\n```\r\n\r\n**Important**: When using `response_format`, include \"JSON\" in your prompt to guide the model.\r\n\r\n---",
    "Official Documentation": "### Core APIs\r\n- **Chat Completions**: https://platform.openai.com/docs/api-reference/chat/create\r\n- **Embeddings**: https://platform.openai.com/docs/api-reference/embeddings\r\n- **Images**: https://platform.openai.com/docs/api-reference/images\r\n- **Audio**: https://platform.openai.com/docs/api-reference/audio\r\n- **Moderation**: https://platform.openai.com/docs/api-reference/moderations\r\n\r\n### Guides\r\n- **GPT-5 Guide**: https://platform.openai.com/docs/guides/latest-model\r\n- **Function Calling**: https://platform.openai.com/docs/guides/function-calling\r\n- **Structured Outputs**: https://platform.openai.com/docs/guides/structured-outputs\r\n- **Vision**: https://platform.openai.com/docs/guides/vision\r\n- **Rate Limits**: https://platform.openai.com/docs/guides/rate-limits\r\n- **Error Codes**: https://platform.openai.com/docs/guides/error-codes\r\n\r\n### SDKs\r\n- **Node.js SDK**: https://github.com/openai/openai-node\r\n- **Python SDK**: https://github.com/openai/openai-python\r\n\r\n---",
    "Function Calling": "Function calling (also called \"tool calling\") allows models to invoke external functions/tools based on conversation context.\r\n\r\n### Basic Tool Definition\r\n\r\n```typescript\r\nconst tools = [\r\n  {\r\n    type: 'function',\r\n    function: {\r\n      name: 'get_weather',\r\n      description: 'Get the current weather for a location',\r\n      parameters: {\r\n        type: 'object',\r\n        properties: {\r\n          location: {\r\n            type: 'string',\r\n            description: 'City name, e.g., San Francisco'\r\n          },\r\n          unit: {\r\n            type: 'string',\r\n            enum: ['celsius', 'fahrenheit'],\r\n            description: 'Temperature unit'\r\n          }\r\n        },\r\n        required: ['location']\r\n      }\r\n    }\r\n  }\r\n];\r\n```\r\n\r\n### Making a Request with Tools\r\n\r\n```typescript\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: [\r\n    { role: 'user', content: 'What is the weather in San Francisco?' }\r\n  ],\r\n  tools: tools,\r\n});\r\n```\r\n\r\n### Handling Tool Calls\r\n\r\n```typescript\r\nconst message = completion.choices[0].message;\r\n\r\nif (message.tool_calls) {\r\n  // Model wants to call a function\r\n  for (const toolCall of message.tool_calls) {\r\n    if (toolCall.function.name === 'get_weather') {\r\n      const args = JSON.parse(toolCall.function.arguments);\r\n\r\n      // Execute your function\r\n      const weatherData = await getWeather(args.location, args.unit);\r\n\r\n      // Send result back to model\r\n      const followUp = await openai.chat.completions.create({\r\n        model: 'gpt-5',\r\n        messages: [\r\n          ...messages,\r\n          message, // Assistant's tool call\r\n          {\r\n            role: 'tool',\r\n            tool_call_id: toolCall.id,\r\n            content: JSON.stringify(weatherData)\r\n          }\r\n        ],\r\n        tools: tools,\r\n      });\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### Complete Function Calling Flow\r\n\r\n```typescript\r\nasync function chatWithTools(userMessage: string) {\r\n  let messages = [\r\n    { role: 'user', content: userMessage }\r\n  ];\r\n\r\n  while (true) {\r\n    const completion = await openai.chat.completions.create({\r\n      model: 'gpt-5',\r\n      messages: messages,\r\n      tools: tools,\r\n    });\r\n\r\n    const message = completion.choices[0].message;\r\n    messages.push(message);\r\n\r\n    // If no tool calls, we're done\r\n    if (!message.tool_calls) {\r\n      return message.content;\r\n    }\r\n\r\n    // Execute all tool calls\r\n    for (const toolCall of message.tool_calls) {\r\n      const result = await executeFunction(toolCall.function.name, toolCall.function.arguments);\r\n\r\n      messages.push({\r\n        role: 'tool',\r\n        tool_call_id: toolCall.id,\r\n        content: JSON.stringify(result)\r\n      });\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### Multiple Tools\r\n\r\nYou can define multiple tools:\r\n\r\n```typescript\r\nconst tools = [\r\n  {\r\n    type: 'function',\r\n    function: {\r\n      name: 'get_weather',\r\n      description: 'Get weather for a location',\r\n      parameters: { /* schema */ }\r\n    }\r\n  },\r\n  {\r\n    type: 'function',\r\n    function: {\r\n      name: 'search_web',\r\n      description: 'Search the web',\r\n      parameters: { /* schema */ }\r\n    }\r\n  },\r\n  {\r\n    type: 'function',\r\n    function: {\r\n      name: 'calculate',\r\n      description: 'Perform calculations',\r\n      parameters: { /* schema */ }\r\n    }\r\n  }\r\n];\r\n```\r\n\r\nThe model will choose which tool(s) to call based on the conversation.\r\n\r\n---",
    "Relationship to openai-responses": "### openai-api (This Skill)\r\n\r\n**Traditional/stateless API** for:\r\n- ✅ Simple chat completions\r\n- ✅ Embeddings for RAG/search\r\n- ✅ Images (DALL-E 3)\r\n- ✅ Audio (Whisper/TTS)\r\n- ✅ Content moderation\r\n- ✅ One-off text generation\r\n- ✅ Cloudflare Workers / edge deployment\r\n\r\n**Characteristics**:\r\n- Stateless (you manage conversation history)\r\n- No built-in tools\r\n- Maximum flexibility\r\n- Works everywhere (Node.js, browsers, Workers, etc.)\r\n\r\n### openai-responses Skill\r\n\r\n**Stateful/agentic API** for:\r\n- ✅ Automatic conversation state management\r\n- ✅ Preserved reasoning (Chain of Thought) across turns\r\n- ✅ Built-in tools (Code Interpreter, File Search, Web Search, Image Generation)\r\n- ✅ MCP server integration\r\n- ✅ Background mode for long tasks\r\n- ✅ Polymorphic outputs\r\n\r\n**Characteristics**:\r\n- Stateful (OpenAI manages conversation)\r\n- Built-in tools included\r\n- Better for agentic workflows\r\n- Higher-level abstraction\r\n\r\n### When to Use Which?\r\n\r\n| Use Case | Use openai-api | Use openai-responses |\r\n|----------|----------------|---------------------|\r\n| Simple chat | ✅ | ❌ |\r\n| RAG/embeddings | ✅ | ❌ |\r\n| Image generation | ✅ | ✅ |\r\n| Audio processing | ✅ | ❌ |\r\n| Agentic workflows | ❌ | ✅ |\r\n| Multi-turn reasoning | ❌ | ✅ |\r\n| Background tasks | ❌ | ✅ |\r\n| Custom tools only | ✅ | ❌ |\r\n| Built-in + custom tools | ❌ | ✅ |\r\n\r\n**Use both**: Many apps use openai-api for embeddings/images/audio and openai-responses for conversational agents.\r\n\r\n---",
    "Chat Completions API": "**Endpoint**: `POST /v1/chat/completions`\r\n\r\nThe Chat Completions API is the core interface for interacting with OpenAI's language models. It supports conversational AI, text generation, function calling, structured outputs, and vision capabilities.\r\n\r\n### Supported Models\r\n\r\n#### GPT-5 Series (Released August 2025)\r\n- **gpt-5**: Full-featured reasoning model with advanced capabilities\r\n- **gpt-5-mini**: Cost-effective alternative with good performance\r\n- **gpt-5-nano**: Smallest/fastest variant for simple tasks\r\n\r\n#### GPT-4o Series\r\n- **gpt-4o**: Multimodal model with vision capabilities\r\n- **gpt-4-turbo**: Fast GPT-4 variant\r\n\r\n#### GPT-4 Series\r\n- **gpt-4**: Original GPT-4 model\r\n\r\n### Basic Request Structure\r\n\r\n```typescript\r\n{\r\n  model: string,              // Model to use (e.g., \"gpt-5\")\r\n  messages: Message[],        // Conversation history\r\n  reasoning_effort?: string,  // GPT-5 only: \"minimal\" | \"low\" | \"medium\" | \"high\"\r\n  verbosity?: string,         // GPT-5 only: \"low\" | \"medium\" | \"high\"\r\n  temperature?: number,       // NOT supported by GPT-5\r\n  max_tokens?: number,        // Max tokens to generate\r\n  stream?: boolean,           // Enable streaming\r\n  tools?: Tool[],             // Function calling tools\r\n}\r\n```\r\n\r\n### Response Structure\r\n\r\n```typescript\r\n{\r\n  id: string,                 // Unique completion ID\r\n  object: \"chat.completion\",\r\n  created: number,            // Unix timestamp\r\n  model: string,              // Model used\r\n  choices: [{\r\n    index: number,\r\n    message: {\r\n      role: \"assistant\",\r\n      content: string,        // Generated text\r\n      tool_calls?: ToolCall[] // If function calling\r\n    },\r\n    finish_reason: string     // \"stop\" | \"length\" | \"tool_calls\"\r\n  }],\r\n  usage: {\r\n    prompt_tokens: number,\r\n    completion_tokens: number,\r\n    total_tokens: number\r\n  }\r\n}\r\n```\r\n\r\n### Message Roles\r\n\r\nOpenAI supports three message roles:\r\n\r\n1. **system** (formerly \"developer\"): Set behavior and context\r\n2. **user**: User input\r\n3. **assistant**: Model responses\r\n\r\n```typescript\r\nconst messages = [\r\n  {\r\n    role: 'system',\r\n    content: 'You are a helpful assistant that explains complex topics simply.'\r\n  },\r\n  {\r\n    role: 'user',\r\n    content: 'Explain quantum computing to a 10-year-old.'\r\n  }\r\n];\r\n```\r\n\r\n### Multi-turn Conversations\r\n\r\nBuild conversation history by appending messages:\r\n\r\n```typescript\r\nconst messages = [\r\n  { role: 'system', content: 'You are a helpful assistant.' },\r\n  { role: 'user', content: 'What is TypeScript?' },\r\n  { role: 'assistant', content: 'TypeScript is a superset of JavaScript...' },\r\n  { role: 'user', content: 'How do I install it?' }\r\n];\r\n\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-5',\r\n  messages: messages,\r\n});\r\n```\r\n\r\n**Important**: Chat Completions API is **stateless**. You must send full conversation history with each request. For stateful conversations, use the `openai-responses` skill.\r\n\r\n---",
    "Dependencies": "### Package Installation\r\n\r\n```bash\r\nnpm install openai@6.7.0\r\n```\r\n\r\n### TypeScript Types\r\n\r\nFully typed with included TypeScript definitions:\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\nimport type { ChatCompletionMessage, ChatCompletionCreateParams } from 'openai/resources/chat';\r\n```\r\n\r\n### Required Environment Variables\r\n\r\n```bash\r\nOPENAI_API_KEY=sk-...\r\n```\r\n\r\n---",
    "Status": "**✅ Production Ready**:\r\n- ✅ Chat Completions API (GPT-5, GPT-4o, GPT-4 Turbo)\r\n- ✅ Embeddings API (text-embedding-3-small, text-embedding-3-large)\r\n- ✅ Images API (DALL-E 3 generation + GPT-Image-1 editing)\r\n- ✅ Audio API (Whisper transcription + TTS with 11 voices)\r\n- ✅ Moderation API (11 safety categories)\r\n- ✅ Streaming patterns (SSE)\r\n- ✅ Function calling / Tools\r\n- ✅ Structured outputs (JSON schemas)\r\n- ✅ Vision (GPT-4o)\r\n- ✅ Both Node.js SDK and fetch approaches\r\n\r\n---",
    "Vision (GPT-4o)": "GPT-4o supports image understanding alongside text.\r\n\r\n### Image via URL\r\n\r\n```typescript\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-4o',\r\n  messages: [\r\n    {\r\n      role: 'user',\r\n      content: [\r\n        { type: 'text', text: 'What is in this image?' },\r\n        {\r\n          type: 'image_url',\r\n          image_url: {\r\n            url: 'https://example.com/image.jpg'\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n```\r\n\r\n### Image via Base64\r\n\r\n```typescript\r\nimport fs from 'fs';\r\n\r\nconst imageBuffer = fs.readFileSync('./image.jpg');\r\nconst base64Image = imageBuffer.toString('base64');\r\n\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-4o',\r\n  messages: [\r\n    {\r\n      role: 'user',\r\n      content: [\r\n        { type: 'text', text: 'Describe this image in detail' },\r\n        {\r\n          type: 'image_url',\r\n          image_url: {\r\n            url: `data:image/jpeg;base64,${base64Image}`\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n```\r\n\r\n### Multiple Images\r\n\r\n```typescript\r\nconst completion = await openai.chat.completions.create({\r\n  model: 'gpt-4o',\r\n  messages: [\r\n    {\r\n      role: 'user',\r\n      content: [\r\n        { type: 'text', text: 'Compare these two images' },\r\n        { type: 'image_url', image_url: { url: 'https://example.com/image1.jpg' } },\r\n        { type: 'image_url', image_url: { url: 'https://example.com/image2.jpg' } }\r\n      ]\r\n    }\r\n  ]\r\n});\r\n```\r\n\r\n---",
    "Images API": "OpenAI's Images API supports image generation with DALL-E 3 and image editing with GPT-Image-1.\r\n\r\n### Image Generation (DALL-E 3)\r\n\r\n**Endpoint**: `POST /v1/images/generations`\r\n\r\nGenerate images from text prompts using DALL-E 3.\r\n\r\n#### Basic Request (Node.js SDK)\r\n\r\n```typescript\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI();\r\n\r\nconst image = await openai.images.generate({\r\n  model: 'dall-e-3',\r\n  prompt: 'A white siamese cat with striking blue eyes',\r\n  size: '1024x1024',\r\n  quality: 'standard',\r\n  style: 'vivid',\r\n  n: 1,\r\n});\r\n\r\nconsole.log(image.data[0].url);\r\nconsole.log(image.data[0].revised_prompt);\r\n```\r\n\r\n#### Basic Request (Fetch - Cloudflare Workers)\r\n\r\n```typescript\r\nconst response = await fetch('https://api.openai.com/v1/images/generations', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${env.OPENAI_API_KEY}`,\r\n    'Content-Type': 'application/json',\r\n  },\r\n  body: JSON.stringify({\r\n    model: 'dall-e-3',\r\n    prompt: 'A white siamese cat with striking blue eyes',\r\n    size: '1024x1024',\r\n    quality: 'standard',\r\n    style: 'vivid',\r\n  }),\r\n});\r\n\r\nconst data = await response.json();\r\nconst imageUrl = data.data[0].url;\r\n```\r\n\r\n#### Parameters\r\n\r\n**size** - Image dimensions:\r\n- `\"1024x1024\"` (square)\r\n- `\"1024x1536\"` (portrait)\r\n- `\"1536x1024\"` (landscape)\r\n- `\"1024x1792\"` (tall portrait)\r\n- `\"1792x1024\"` (wide landscape)\r\n\r\n**quality** - Rendering quality:\r\n- `\"standard\"`: Normal quality, faster, cheaper\r\n- `\"hd\"`: High definition with finer details, costs more\r\n\r\n**style** - Visual style:\r\n- `\"vivid\"`: Hyper-real, dramatic, high-contrast images\r\n- `\"natural\"`: More natural, less dramatic styling\r\n\r\n**response_format** - Output format:\r\n- `\"url\"`: Returns temporary URL (expires in 1 hour)\r\n- `\"b64_json\"`: Returns base64-encoded image data\r\n\r\n**n** - Number of images:\r\n- DALL-E 3 only supports `n: 1`\r\n- DALL-E 2 supports `n: 1-10`\r\n\r\n#### Response Structure\r\n\r\n```typescript\r\n{\r\n  created: 1700000000,\r\n  data: [\r\n    {\r\n      url: \"https://oaidalleapiprodscus.blob.core.windows.net/...\",\r\n      revised_prompt: \"A pristine white Siamese cat with striking blue eyes, sitting elegantly...\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n**Note**: DALL-E 3 may revise your prompt for safety/quality. The `revised_prompt` field shows what was actually used.\r\n\r\n#### Quality Comparison\r\n\r\n```typescript\r\n// Standard quality (faster, cheaper)\r\nconst standardImage = await openai.images.generate({\r\n  model: 'dall-e-3',\r\n  prompt: 'A futuristic city at sunset',\r\n  quality: 'standard',\r\n});\r\n\r\n// HD quality (finer details, costs more)\r\nconst hdImage = await openai.images.generate({\r\n  model: 'dall-e-3',\r\n  prompt: 'A futuristic city at sunset',\r\n  quality: 'hd',\r\n});\r\n```\r\n\r\n#### Style Comparison\r\n\r\n```typescript\r\n// Vivid style (hyper-real, dramatic)\r\nconst vividImage = await openai.images.generate({\r\n  model: 'dall-e-3',\r\n  prompt: 'A mountain landscape',\r\n  style: 'vivid',\r\n});\r\n\r\n// Natural style (more realistic, less dramatic)\r\nconst naturalImage = await openai.images.generate({\r\n  model: 'dall-e-3',\r\n  prompt: 'A mountain landscape',\r\n  style: 'natural',\r\n});\r\n```\r\n\r\n#### Base64 Output\r\n\r\n```typescript\r\nconst image = await openai.images.generate({\r\n  model: 'dall-e-3',\r\n  prompt: 'A cyberpunk street scene',\r\n  response_format: 'b64_json',\r\n});\r\n\r\nconst base64Data = image.data[0].b64_json;\r\n\r\n// Convert to buffer and save\r\nimport fs from 'fs';\r\nconst buffer = Buffer.from(base64Data, 'base64');\r\nfs.writeFileSync('image.png', buffer);\r\n```\r\n\r\n### Image Editing (GPT-Image-1)\r\n\r\n**Endpoint**: `POST /v1/images/edits`\r\n\r\nEdit or composite images using AI.\r\n\r\n**Important**: This endpoint uses `multipart/form-data`, not JSON.\r\n\r\n#### Basic Edit Request\r\n\r\n```typescript\r\nimport fs from 'fs';\r\nimport FormData from 'form-data';\r\n\r\nconst formData = new FormData();\r\nformData.append('model', 'gpt-image-1');\r\nformData.append('image', fs.createReadStream('./woman.jpg'));\r\nformData.append('image_2', fs.createReadStream('./logo.png'));\r\nformData.append('prompt', 'Add the logo to the woman\\'s top, as if stamped into the fabric.');\r\nformData.append('input_fidelity', 'high');\r\nformData.append('size', '1024x1024');\r\nformData.append('quality', 'auto');\r\n\r\nconst response = await fetch('https://api.openai.com/v1/images/edits', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,\r\n    ...formData.getHeaders(),\r\n  },\r\n  body: formData,\r\n});\r\n\r\nconst data = await response.json();\r\nconst editedImageUrl = data.data[0].url;\r\n```\r\n\r\n#### Edit Parameters\r\n\r\n**model**: `\"gpt-image-1\"` (required)\r\n\r\n**image**: Primary image file (PNG, JPEG, WebP)\r\n\r\n**image_2**: Secondary image for compositing (optional)\r\n\r\n**prompt**: Text description of desired edits\r\n\r\n**input_fidelity**:\r\n- `\"low\"`: More creative freedom\r\n- `\"medium\"`: Balance\r\n- `\"high\"`: Stay closer to original\r\n\r\n**size**: Same options as generation\r\n\r\n**quality**:\r\n- `\"auto\"`: Automatic quality selection\r\n- `\"standard\"`: Normal quality\r\n- `\"high\"`: Higher quality\r\n\r\n**format**: Output format:\r\n- `\"png\"`: PNG (supports transparency)\r\n- `\"jpeg\"`: JPEG (no transparency)\r\n- `\"webp\"`: WebP (smaller file size)\r\n\r\n**background**: Background handling:\r\n- `\"transparent\"`: Transparent background (PNG/WebP only)\r\n- `\"white\"`: White background\r\n- `\"black\"`: Black background\r\n\r\n**output_compression**: JPEG/WebP compression (0-100)\r\n- `0`: Maximum compression (smallest file)\r\n- `100`: Minimum compression (highest quality)\r\n\r\n#### Transparent Background Example\r\n\r\n```typescript\r\nconst formData = new FormData();\r\nformData.append('model', 'gpt-image-1');\r\nformData.append('image', fs.createReadStream('./product.jpg'));\r\nformData.append('prompt', 'Remove the background, keeping only the product.');\r\nformData.append('format', 'png');\r\nformData.append('background', 'transparent');\r\n\r\nconst response = await fetch('https://api.openai.com/v1/images/edits', {\r\n  method: 'POST',\r\n  headers: {\r\n    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,\r\n    ...formData.getHeaders(),\r\n  },\r\n  body: formData,\r\n});\r\n```\r\n\r\n### Images Best Practices\r\n\r\n✅ **Prompting**:\r\n- Be specific about details (colors, composition, style)\r\n- Include artistic style references (\"oil painting\", \"photograph\", \"3D render\")\r\n- Specify lighting (\"golden hour\", \"studio lighting\", \"dramatic shadows\")\r\n- DALL-E 3 may revise prompts; check `revised_prompt`\r\n\r\n✅ **Performance**:\r\n- Use `\"standard\"` quality unless HD details are critical\r\n- Use `\"natural\"` style for realistic images\r\n- Use `\"vivid\"` style for marketing/artistic images\r\n- Cache generated images (they're non-deterministic)\r\n\r\n✅ **Cost Optimization**:\r\n- Standard quality is cheaper than HD\r\n- Smaller sizes cost less\r\n- Use appropriate size for your use case (don't generate 1792x1024 if you need 512x512)\r\n\r\n❌ **Don't**:\r\n- Request multiple images with DALL-E 3 (n=1 only)\r\n- Expect deterministic output (same prompt = different images)\r\n- Use URLs that expire (save images if needed long-term)\r\n- Forget to handle revised prompts (DALL-E 3 modifies for safety)\r\n\r\n---"
  }
}