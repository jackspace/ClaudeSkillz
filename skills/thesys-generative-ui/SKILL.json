{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/ai-provider-setup.md",
      "references/common-errors.md",
      "references/component-api.md",
      "CHANGELOG.md"
    ]
  },
  "content": "Complete skill for building AI-powered interfaces with TheSys C1 Generative UI API. Convert LLM responses into streaming, interactive React components.\r\n\r\n---\r\n\r\n\r\n### 1. Empty Agent Responses\r\n\r\n**Problem**: AI returns empty responses, UI shows nothing.\r\n\r\n**Cause**: Incorrect streaming transformation or response format.\r\n\r\n**Solution**:\r\n```typescript\r\n// ✅ Use transformStream helper\r\nimport { transformStream } from \"@crayonai/stream\";\r\n\r\nconst c1Stream = transformStream(llmStream, (chunk) => {\r\n  return chunk.choices[0]?.delta?.content || \"\"; // Fallback to empty string\r\n}) as ReadableStream<string>;\r\n```\r\n\r\n---\r\n\r\n### 2. Model Not Following System Prompt\r\n\r\n**Problem**: AI ignores instructions in system prompt.\r\n\r\n**Cause**: System prompt is not first in messages array or improperly formatted.\r\n\r\n**Solution**:\r\n```typescript\r\n// ✅ System prompt MUST be first\r\nconst messages = [\r\n  { role: \"system\", content: \"You are a helpful assistant.\" }, // FIRST!\r\n  ...conversationHistory,\r\n  { role: \"user\", content: userPrompt },\r\n];\r\n\r\n// ❌ Wrong - system prompt after user messages\r\nconst messages = [\r\n  { role: \"user\", content: \"Hello\" },\r\n  { role: \"system\", content: \"...\" }, // TOO LATE\r\n];\r\n```\r\n\r\n---\r\n\r\n### 3. Version Compatibility Errors\r\n\r\n**Problem**: `TypeError: Cannot read property 'X' of undefined` or component rendering errors.\r\n\r\n**Cause**: Mismatched SDK versions.\r\n\r\n**Solution**: Check compatibility matrix:\r\n\r\n| C1 Version | @thesysai/genui-sdk | @crayonai/react-ui | @crayonai/react-core |\r\n|------------|---------------------|-------------------|---------------------|\r\n| v-20250930 | ~0.6.40             | ~0.8.42           | ~0.7.6              |\r\n\r\n```bash\r\nnpm install @thesysai/genui-sdk@0.6.40 @crayonai/react-ui@0.8.42 @crayonai/react-core@0.7.6\r\n```\r\n\r\n---\r\n\r\n### 4. Theme Not Applying\r\n\r\n**Problem**: UI components don't match custom theme.\r\n\r\n**Cause**: Missing `ThemeProvider` wrapper.\r\n\r\n**Solution**:\r\n```typescript\r\n// ❌ Wrong\r\n<C1Component c1Response={response} />\r\n\r\n// ✅ Correct\r\n<ThemeProvider theme={customTheme}>\r\n  <C1Component c1Response={response} />\r\n</ThemeProvider>\r\n```\r\n\r\n---\r\n\r\n### 5. Streaming Not Working\r\n\r\n**Problem**: UI doesn't update in real-time, waits for full response.\r\n\r\n**Cause**: Not using streaming or improper response headers.\r\n\r\n**Solution**:\r\n```typescript\r\n// 1. Enable streaming in API call\r\nconst stream = await client.chat.completions.create({\r\n  model: \"c1/openai/gpt-5/v-20250930\",\r\n  messages: [...],\r\n  stream: true, // ✅ IMPORTANT\r\n});\r\n\r\n// 2. Set proper response headers\r\nreturn new NextResponse(responseStream, {\r\n  headers: {\r\n    \"Content-Type\": \"text/event-stream\",\r\n    \"Cache-Control\": \"no-cache, no-transform\",\r\n    \"Connection\": \"keep-alive\",\r\n  },\r\n});\r\n\r\n// 3. Pass isStreaming prop\r\n<C1Component\r\n  c1Response={response}\r\n  isStreaming={true} // ✅ Shows loading indicator\r\n/>\r\n```\r\n\r\n---\r\n\r\n### 6. Tool Calling Failures\r\n\r\n**Problem**: Tools not executing or validation errors.\r\n\r\n**Cause**: Invalid Zod schema or incorrect tool format.\r\n\r\n**Solution**:\r\n```typescript\r\nimport { z } from \"zod\";\r\nimport zodToJsonSchema from \"zod-to-json-schema\";\r\n\r\n// ✅ Proper Zod schema with descriptions\r\nconst toolSchema = z.object({\r\n  query: z.string().describe(\"Search query\"), // DESCRIBE all fields\r\n  limit: z.number().int().min(1).max(100).describe(\"Max results\"),\r\n});\r\n\r\n// ✅ Convert to OpenAI format\r\nconst tool = {\r\n  type: \"function\" as const,\r\n  function: {\r\n    name: \"search_web\",\r\n    description: \"Search the web for information\", // Clear description\r\n    parameters: zodToJsonSchema(toolSchema), // Convert schema\r\n  },\r\n};\r\n\r\n// ✅ Validate incoming tool calls\r\nconst args = toolSchema.parse(JSON.parse(toolCall.function.arguments));\r\n```\r\n\r\n---\r\n\r\n### 7. Thread State Not Persisting\r\n\r\n**Problem**: Threads disappear on page refresh.\r\n\r\n**Cause**: No backend persistence, using in-memory storage.\r\n\r\n**Solution**: Implement database storage (see Production Patterns section).\r\n\r\n---\r\n\r\n### 8. CSS Conflicts\r\n\r\n**Problem**: Styles from C1 components clash with app styles.\r\n\r\n**Cause**: CSS import order or global styles overriding.\r\n\r\n**Solution**:\r\n```typescript\r\n// ✅ Correct import order\r\nimport \"@crayonai/react-ui/styles/index.css\"; // C1 styles FIRST\r\nimport \"./your-app.css\"; // Your styles SECOND\r\n\r\n// In your CSS, use specificity if needed\r\n.your-custom-class .c1-message {\r\n  /* Override specific styles */\r\n}\r\n```\r\n\r\n---\r\n\r\n### 9. TypeScript Type Errors\r\n\r\n**Problem**: TypeScript complains about missing types or incompatible types.\r\n\r\n**Cause**: Outdated package versions or missing type definitions.\r\n\r\n**Solution**:\r\n```bash\r\nnpm install @thesysai/genui-sdk@latest @crayonai/react-ui@latest @crayonai/react-core@latest\r\n\r\n{\r\n  \"compilerOptions\": {\r\n    \"moduleResolution\": \"bundler\", // or \"node16\"\r\n    \"skipLibCheck\": true // Skip type checking for node_modules\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n### 10. CORS Errors with API\r\n\r\n**Problem**: `Access-Control-Allow-Origin` errors when calling backend.\r\n\r\n**Cause**: Missing CORS headers in API responses.\r\n\r\n**Solution**:\r\n```typescript\r\n// Next.js API Route\r\nexport async function POST(req: NextRequest) {\r\n  const response = new NextResponse(stream, {\r\n    headers: {\r\n      \"Content-Type\": \"text/event-stream\",\r\n      \"Access-Control-Allow-Origin\": \"*\", // Or specific domain\r\n      \"Access-Control-Allow-Methods\": \"POST, OPTIONS\",\r\n      \"Access-Control-Allow-Headers\": \"Content-Type\",\r\n    },\r\n  });\r\n\r\n  return response;\r\n}\r\n\r\n// Express\r\napp.use(cors({\r\n  origin: \"http://localhost:5173\", // Your frontend URL\r\n  methods: [\"POST\", \"OPTIONS\"],\r\n}));\r\n```\r\n\r\n---\r\n\r\n### 11. Rate Limiting Issues\r\n\r\n**Problem**: API calls fail with 429 errors, no retry mechanism.\r\n\r\n**Cause**: No backoff logic for rate limits.\r\n\r\n**Solution**:\r\n```typescript\r\nasync function callApiWithRetry(apiCall, maxRetries = 3) {\r\n  for (let i = 0; i < maxRetries; i++) {\r\n    try {\r\n      return await apiCall();\r\n    } catch (error) {\r\n      if (error.status === 429 && i < maxRetries - 1) {\r\n        const waitTime = Math.pow(2, i) * 1000; // Exponential backoff\r\n        await new Promise((resolve) => setTimeout(resolve, waitTime));\r\n        continue;\r\n      }\r\n      throw error;\r\n    }\r\n  }\r\n}\r\n\r\n// Usage\r\nconst response = await callApiWithRetry(() =>\r\n  client.chat.completions.create({...})\r\n);\r\n```\r\n\r\n---\r\n\r\n### 12. Authentication Token Errors\r\n\r\n**Problem**: `401 Unauthorized` even with API key set.\r\n\r\n**Cause**: Environment variable not loaded or incorrect variable name.\r\n\r\n**Solution**:\r\n```bash\r\nTHESYS_API_KEY=your_api_key_here\r\n\r\nif (!process.env.THESYS_API_KEY) {\r\n  throw new Error(\"THESYS_API_KEY is not set\");\r\n}\r\n\r\nVITE_THESYS_API_KEY=your_key # Client-side\r\nTHESYS_API_KEY=your_key      # Server-side\r\n\r\nconst apiKey = import.meta.env.VITE_THESYS_API_KEY;",
  "name": "thesys-generative-ui",
  "id": "thesys-generative-ui",
  "sections": {
    "Templates & Examples": "This skill includes 15+ working templates in the `templates/` directory:\r\n\r\n### Vite + React (5 templates)\r\n1. **`basic-chat.tsx`** - Minimal C1Chat setup with custom backend\r\n2. **`custom-component.tsx`** - Using C1Component with manual state\r\n3. **`tool-calling.tsx`** - Web search + database query tools\r\n4. **`theme-dark-mode.tsx`** - Custom theming with dark mode toggle\r\n5. **`package.json`** - Exact dependency versions\r\n\r\n### Next.js (4 templates)\r\n1. **`app/page.tsx`** - C1Chat page component\r\n2. **`app/api/chat/route.ts`** - Streaming API route handler\r\n3. **`tool-calling-route.ts`** - API route with tool integration\r\n4. **`package.json`** - Next.js dependency setup\r\n\r\n### Cloudflare Workers (3 templates)\r\n1. **`worker-backend.ts`** - Hono API with TheSys proxy\r\n2. **`frontend-setup.tsx`** - React frontend configuration\r\n3. **`wrangler.jsonc`** - Worker deployment config\r\n\r\n### Shared Utilities (3 templates)\r\n1. **`theme-config.ts`** - Reusable theme configurations\r\n2. **`tool-schemas.ts`** - Common Zod schemas for tools\r\n3. **`streaming-utils.ts`** - Helper functions for streaming\r\n\r\n---",
    "Advanced Features": "### Thread Management (Multi-Conversation Support)\r\n\r\nEnable users to have multiple conversation threads with thread switching, history, and persistence.\r\n\r\n#### 1. Define Thread API\r\n\r\nCreate backend endpoints:\r\n- `GET /api/threads` - List all threads\r\n- `POST /api/threads` - Create new thread\r\n- `PUT /api/threads/:id` - Update thread title\r\n- `DELETE /api/threads/:id` - Delete thread\r\n- `GET /api/threads/:id/messages` - Load thread messages\r\n\r\n#### 2. Implement Thread Managers\r\n\r\n```typescript\r\nimport {\r\n  useThreadListManager,\r\n  useThreadManager,\r\n} from \"@thesysai/genui-sdk\";\r\nimport { Thread, Message, UserMessage } from \"@crayonai/react-core\";\r\n\r\nexport default function App() {\r\n  const threadListManager = useThreadListManager({\r\n    // Fetch all threads\r\n    fetchThreadList: async (): Promise<Thread[]> => {\r\n      const response = await fetch(\"/api/threads\");\r\n      return response.json();\r\n    },\r\n\r\n    // Delete thread\r\n    deleteThread: async (threadId: string): Promise<void> => {\r\n      await fetch(`/api/threads/${threadId}`, { method: \"DELETE\" });\r\n    },\r\n\r\n    // Update thread title\r\n    updateThread: async (thread: Thread): Promise<Thread> => {\r\n      const response = await fetch(`/api/threads/${thread.threadId}`, {\r\n        method: \"PUT\",\r\n        headers: { \"Content-Type\": \"application/json\" },\r\n        body: JSON.stringify({ title: thread.title }),\r\n      });\r\n      return response.json();\r\n    },\r\n\r\n    // Create new thread\r\n    createThread: async (firstMessage: UserMessage): Promise<Thread> => {\r\n      const response = await fetch(\"/api/threads\", {\r\n        method: \"POST\",\r\n        headers: { \"Content-Type\": \"application/json\" },\r\n        body: JSON.stringify({\r\n          title: firstMessage.message || \"New Chat\",\r\n        }),\r\n      });\r\n      return response.json();\r\n    },\r\n\r\n    // URL synchronization\r\n    onSwitchToNew: () => {\r\n      window.history.replaceState(null, \"\", window.location.pathname);\r\n    },\r\n    onSelectThread: (threadId: string) => {\r\n      const url = new URL(window.location.href);\r\n      url.searchParams.set(\"threadId\", threadId);\r\n      window.history.replaceState(null, \"\", url.toString());\r\n    },\r\n  });\r\n\r\n  const threadManager = useThreadManager({\r\n    threadListManager,\r\n\r\n    // Load messages for selected thread\r\n    loadThread: async (threadId: string): Promise<Message[]> => {\r\n      const response = await fetch(`/api/threads/${threadId}/messages`);\r\n      return response.json();\r\n    },\r\n\r\n    // Handle message updates (e.g., feedback)\r\n    onUpdateMessage: async ({ message }: { message: Message }) => {\r\n      if (threadListManager.selectedThreadId) {\r\n        await fetch(\r\n          `/api/threads/${threadListManager.selectedThreadId}/message`,\r\n          {\r\n            method: \"PUT\",\r\n            headers: { \"Content-Type\": \"application/json\" },\r\n            body: JSON.stringify(message),\r\n          }\r\n        );\r\n      }\r\n    },\r\n  });\r\n\r\n  return (\r\n    <C1Chat\r\n      threadManager={threadManager}\r\n      threadListManager={threadListManager}\r\n    />\r\n  );\r\n}\r\n```\r\n\r\n---\r\n\r\n### Thinking States (Progress Indicators)\r\n\r\nShow users what the AI is doing during processing (searching web, analyzing data, etc.).\r\n\r\n#### 1. Server-Side: Write Think Items\r\n\r\n```typescript\r\nimport { makeC1Response } from \"@thesysai/genui-sdk/server\";\r\n\r\nexport async function POST(req: NextRequest) {\r\n  const c1Response = makeC1Response();\r\n\r\n  // Initial thinking state\r\n  c1Response.writeThinkItem({\r\n    title: \"Thinking…\",\r\n    description: \"Analyzing your question and planning the response.\",\r\n  });\r\n\r\n  const { prompt } = await req.json();\r\n\r\n  // Update thinking state when calling tools\r\n  const llmStream = await client.beta.chat.completions.runTools({\r\n    model: \"c1/anthropic/claude-sonnet-4/v-20250930\",\r\n    messages: [...],\r\n    tools: [\r\n      getWebSearchTool(() => {\r\n        c1Response.writeThinkItem({\r\n          title: \"Searching the web…\",\r\n          description: \"Finding the most relevant and up-to-date information.\",\r\n        });\r\n      }),\r\n    ],\r\n  });\r\n\r\n  transformStream(\r\n    llmStream,\r\n    (chunk) => {\r\n      const content = chunk.choices[0]?.delta?.content;\r\n      if (content) {\r\n        c1Response.writeContent(content);\r\n      }\r\n      return content;\r\n    },\r\n    {\r\n      onEnd: () => {\r\n        c1Response.end();\r\n      },\r\n    }\r\n  );\r\n\r\n  return new NextResponse(c1Response.responseStream, {\r\n    headers: {\r\n      \"Content-Type\": \"text/event-stream\",\r\n      \"Cache-Control\": \"no-cache, no-transform\",\r\n      \"Connection\": \"keep-alive\",\r\n    },\r\n  });\r\n}\r\n```\r\n\r\n#### 2. Custom Think Component\r\n\r\n```typescript\r\n// CustomThink.tsx\r\nimport { ThinkItem } from \"@crayonai/react-core\";\r\n\r\nexport function CustomThink({ item }: { item: ThinkItem }) {\r\n  return (\r\n    <div className=\"custom-think\">\r\n      <div className=\"spinner\" />\r\n      <div>\r\n        <h4>{item.title}</h4>\r\n        <p>{item.description}</p>\r\n      </div>\r\n    </div>\r\n  );\r\n}\r\n\r\n// In your app\r\n<C1Chat\r\n  apiUrl=\"/api/chat\"\r\n  customizeC1={{ thinkComponent: CustomThink }}\r\n/>\r\n```\r\n\r\n---\r\n\r\n### Message and Thread Sharing\r\n\r\nEnable users to share conversations via public URLs.\r\n\r\n#### 1. Generate Share Links\r\n\r\n```typescript\r\nimport { C1ShareThread } from \"@thesysai/genui-sdk\";\r\n\r\nconst selectedThreadId = threadListManager.selectedThreadId;\r\n\r\n<C1ShareThread\r\n  generateShareLink={\r\n    !selectedThreadId\r\n      ? undefined\r\n      : async () => {\r\n          const baseUrl = window.location.origin;\r\n          return `${baseUrl}/shared/${selectedThreadId}`;\r\n        }\r\n  }\r\n/>\r\n```\r\n\r\n#### 2. Create Shared View Page\r\n\r\n```typescript\r\n// app/shared/[threadId]/page.tsx\r\n\"use client\";\r\n\r\nimport { C1ChatViewer } from \"@thesysai/genui-sdk\";\r\nimport { Message } from \"@crayonai/react-core\";\r\nimport { use, useEffect, useState } from \"react\";\r\nimport \"@crayonai/react-ui/styles/index.css\";\r\n\r\nexport default function ViewSharedThread({\r\n  params,\r\n}: {\r\n  params: Promise<{ threadId: string }>;\r\n}) {\r\n  const { threadId } = use(params);\r\n  const [messages, setMessages] = useState<Message[]>([]);\r\n\r\n  useEffect(() => {\r\n    const fetchMessages = async () => {\r\n      const response = await fetch(`/api/share/${threadId}`);\r\n      const data = await response.json();\r\n      setMessages(data);\r\n    };\r\n    fetchMessages();\r\n  }, [threadId]);\r\n\r\n  if (!messages.length) return <div>Loading...</div>;\r\n\r\n  return <C1ChatViewer messages={messages} />;\r\n}\r\n```\r\n\r\n---",
    "Success Metrics": "- **Token savings**: ~65-70% vs manual implementation\r\n- **Errors prevented**: 12+ documented issues\r\n- **Development speed**: 10x faster (per TheSys)\r\n- **User engagement**: 83% prefer interactive UI\r\n- **Package versions**: Latest stable (Oct 2025)\r\n\r\n---",
    "What is TheSys C1?": "**TheSys C1** is a Generative UI API that transforms Large Language Model (LLM) responses into live, interactive React components instead of plain text. Rather than displaying walls of text, your AI applications can stream forms, charts, tables, search results, and custom UI elements in real-time.\r\n\r\n### Key Innovation\r\n\r\nTraditional LLM applications return text that developers must manually convert into UI:\r\n```\r\nLLM → Text Response → Developer Parses → Manual UI Code → Display\r\n```\r\n\r\nTheSys C1 eliminates this manual step:\r\n```\r\nLLM → C1 API → Interactive React Components → Display\r\n```\r\n\r\n### Real-World Impact\r\n\r\n- **83% more engaging** - Users prefer interactive components over text walls\r\n- **10x faster development** - No manual text-to-UI conversion\r\n- **80% cheaper** - Reduced development time and maintenance\r\n- **Production-ready** - Used by teams building AI-native products\r\n\r\n---",
    "When to Use This Skill": "Use this skill when building:\r\n\r\n1. **Chat Interfaces with Rich UI**\r\n   - Conversational interfaces that need more than text\r\n   - Customer support chatbots with forms and actions\r\n   - AI assistants that show data visualizations\r\n\r\n2. **Data Visualization Applications**\r\n   - Analytics dashboards with AI-generated charts\r\n   - Business intelligence tools with dynamic tables\r\n   - Search interfaces with structured results\r\n\r\n3. **Dynamic Form Generation**\r\n   - E-commerce product configurators\r\n   - Multi-step workflows driven by AI\r\n   - Data collection with intelligent forms\r\n\r\n4. **AI Copilots and Assistants**\r\n   - Developer tools with code snippets and docs\r\n   - Educational platforms with interactive lessons\r\n   - Research tools with citations and references\r\n\r\n5. **Search and Discovery**\r\n   - Semantic search with structured results\r\n   - Document analysis with highlighted findings\r\n   - Knowledge bases with interactive answers\r\n\r\n### This Skill Prevents These Errors\r\n\r\n- ❌ Empty agent responses from incorrect streaming setup\r\n- ❌ Models ignoring system prompts due to message array issues\r\n- ❌ Version compatibility errors between SDK and API\r\n- ❌ Themes not applying without ThemeProvider\r\n- ❌ Streaming failures from improper response transformation\r\n- ❌ Tool calling bugs from invalid Zod schemas\r\n- ❌ Thread state loss from missing persistence\r\n- ❌ CSS conflicts from import order issues\r\n- ❌ TypeScript errors from outdated type definitions\r\n- ❌ CORS failures from missing headers\r\n- ❌ Rate limit crashes without retry logic\r\n- ❌ Authentication token errors from environment issues\r\n\r\n---",
    "Tool Calling with Zod Schemas": "**Tool calling** allows your AI to invoke functions and display interactive UI for data collection, external API calls, and complex workflows.\r\n\r\n### 1. Define Tools with Zod\r\n\r\n```typescript\r\nimport { z } from \"zod\";\r\nimport zodToJsonSchema from \"zod-to-json-schema\";\r\n\r\n// Define the tool schema\r\nconst webSearchSchema = z.object({\r\n  query: z.string().describe(\"The search query\"),\r\n  max_results: z.number().int().min(1).max(10).default(5)\r\n    .describe(\"Maximum number of results to return\"),\r\n});\r\n\r\n// Convert to OpenAI tool format\r\nexport const webSearchTool = {\r\n  type: \"function\" as const,\r\n  function: {\r\n    name: \"web_search\",\r\n    description: \"Search the web for current information\",\r\n    parameters: zodToJsonSchema(webSearchSchema),\r\n  },\r\n};\r\n```\r\n\r\n### 2. More Complex Example: Order Management\r\n\r\n```typescript\r\nimport { z } from \"zod\";\r\n\r\n// Discriminated union for different product types\r\nconst productOrderSchema = z.discriminatedUnion(\"type\", [\r\n  z.object({\r\n    type: z.literal(\"gloves\"),\r\n    size: z.enum([\"S\", \"M\", \"L\", \"XL\"]),\r\n    color: z.string(),\r\n    quantity: z.number().int().min(1),\r\n  }),\r\n  z.object({\r\n    type: z.literal(\"hat\"),\r\n    style: z.enum([\"beanie\", \"baseball\", \"fedora\"]),\r\n    color: z.string(),\r\n    quantity: z.number().int().min(1),\r\n  }),\r\n  z.object({\r\n    type: z.literal(\"scarf\"),\r\n    length: z.enum([\"short\", \"medium\", \"long\"]),\r\n    material: z.enum([\"wool\", \"cotton\", \"silk\"]),\r\n    quantity: z.number().int().min(1),\r\n  }),\r\n]);\r\n\r\nconst createOrderSchema = z.object({\r\n  customer_email: z.string().email(),\r\n  items: z.array(productOrderSchema).min(1),\r\n  shipping_address: z.object({\r\n    street: z.string(),\r\n    city: z.string(),\r\n    state: z.string(),\r\n    zip: z.string(),\r\n  }),\r\n});\r\n\r\nexport const createOrderTool = {\r\n  type: \"function\" as const,\r\n  function: {\r\n    name: \"create_order\",\r\n    description: \"Create a new order for products\",\r\n    parameters: zodToJsonSchema(createOrderSchema),\r\n  },\r\n};\r\n```\r\n\r\n### 3. Implement Tool Execution\r\n\r\n```typescript\r\n// tools.ts\r\nimport { TavilySearchAPIClient } from \"@tavily/core\";\r\n\r\nconst tavily = new TavilySearchAPIClient({\r\n  apiKey: process.env.TAVILY_API_KEY,\r\n});\r\n\r\nexport async function executeWebSearch(query: string, max_results: number) {\r\n  const results = await tavily.search(query, {\r\n    maxResults: max_results,\r\n    includeAnswer: true,\r\n  });\r\n\r\n  return {\r\n    query,\r\n    results: results.results.map((r) => ({\r\n      title: r.title,\r\n      url: r.url,\r\n      snippet: r.content,\r\n    })),\r\n    answer: results.answer,\r\n  };\r\n}\r\n\r\nexport async function executeCreateOrder(orderData: z.infer<typeof createOrderSchema>) {\r\n  // Validate with Zod\r\n  const validated = createOrderSchema.parse(orderData);\r\n\r\n  // Save to database\r\n  const orderId = await saveOrderToDatabase(validated);\r\n\r\n  return {\r\n    success: true,\r\n    orderId,\r\n    message: `Order ${orderId} created successfully`,\r\n  };\r\n}\r\n```\r\n\r\n### 4. Integrate Tools in API Route\r\n\r\n```typescript\r\nimport { NextRequest, NextResponse } from \"next/server\";\r\nimport OpenAI from \"openai\";\r\nimport { transformStream } from \"@crayonai/stream\";\r\nimport { webSearchTool, createOrderTool } from \"./tools\";\r\n\r\nconst client = new OpenAI({\r\n  baseURL: \"https://api.thesys.dev/v1/embed\",\r\n  apiKey: process.env.THESYS_API_KEY,\r\n});\r\n\r\nexport async function POST(req: NextRequest) {\r\n  const { prompt } = await req.json();\r\n\r\n  const llmStream = await client.beta.chat.completions.runTools({\r\n    model: \"c1/anthropic/claude-sonnet-4/v-20250930\",\r\n    messages: [\r\n      {\r\n        role: \"system\",\r\n        content: \"You are a helpful shopping assistant. Use tools to search for products and create orders.\",\r\n      },\r\n      {\r\n        role: \"user\",\r\n        content: prompt,\r\n      },\r\n    ],\r\n    stream: true,\r\n    tools: [webSearchTool, createOrderTool],\r\n    toolChoice: \"auto\", // Let AI decide when to use tools\r\n  });\r\n\r\n  // Handle tool execution\r\n  llmStream.on(\"message\", async (event) => {\r\n    if (event.tool_calls) {\r\n      for (const toolCall of event.tool_calls) {\r\n        if (toolCall.function.name === \"web_search\") {\r\n          const args = JSON.parse(toolCall.function.arguments);\r\n          const result = await executeWebSearch(args.query, args.max_results);\r\n          // Send result back to LLM...\r\n        } else if (toolCall.function.name === \"create_order\") {\r\n          const args = JSON.parse(toolCall.function.arguments);\r\n          const result = await executeCreateOrder(args);\r\n          // Send result back to LLM...\r\n        }\r\n      }\r\n    }\r\n  });\r\n\r\n  const responseStream = transformStream(llmStream, (chunk) => {\r\n    return chunk.choices[0]?.delta?.content || \"\";\r\n  }) as ReadableStream<string>;\r\n\r\n  return new NextResponse(responseStream, {\r\n    headers: {\r\n      \"Content-Type\": \"text/event-stream\",\r\n      \"Cache-Control\": \"no-cache, no-transform\",\r\n      \"Connection\": \"keep-alive\",\r\n    },\r\n  });\r\n}\r\n```\r\n\r\n### 5. Display Tool Results in UI\r\n\r\nThe C1Component automatically renders tool interactions as forms and displays results. You just need to handle the `onAction` callback:\r\n\r\n```typescript\r\n<C1Component\r\n  c1Response={c1Response}\r\n  onAction={async ({ llmFriendlyMessage, rawAction }) => {\r\n    console.log(\"Tool action triggered:\", rawAction);\r\n    // Make API call with llmFriendlyMessage to continue conversation\r\n    await makeApiCall(llmFriendlyMessage);\r\n  }}\r\n/>\r\n```\r\n\r\n---",
    "AI Provider Integration": "TheSys C1 API is **OpenAI-compatible**, meaning it works with any LLM provider that uses OpenAI's API format.\r\n\r\n### OpenAI Integration\r\n\r\n#### Setup\r\n\r\n```bash\r\nnpm install openai\r\n```\r\n\r\n```typescript\r\nimport OpenAI from \"openai\";\r\n\r\nconst client = new OpenAI({\r\n  baseURL: \"https://api.thesys.dev/v1/embed\",\r\n  apiKey: process.env.THESYS_API_KEY, // TheSys API key\r\n});\r\n```\r\n\r\n#### Model Selection\r\n\r\nTheSys supports OpenAI models through C1:\r\n\r\n```typescript\r\n// GPT 5 (Stable - Recommended for Production)\r\nmodel: \"c1/openai/gpt-5/v-20250930\"\r\n\r\n// GPT 4.1 (Experimental)\r\nmodel: \"c1-exp/openai/gpt-4.1/v-20250617\"\r\n```\r\n\r\n#### Complete Example\r\n\r\n```typescript\r\nconst response = await client.chat.completions.create({\r\n  model: \"c1/openai/gpt-5/v-20250930\",\r\n  messages: [\r\n    {\r\n      role: \"system\",\r\n      content: \"You are a helpful assistant that generates interactive UI components.\",\r\n    },\r\n    {\r\n      role: \"user\",\r\n      content: \"Show me a comparison table of the top 3 project management tools.\",\r\n    },\r\n  ],\r\n  stream: true, // Enable streaming\r\n  temperature: 0.7,\r\n  max_tokens: 2000,\r\n});\r\n```\r\n\r\n---\r\n\r\n### Anthropic (Claude) Integration\r\n\r\n#### Setup\r\n\r\nTheSys C1 supports Anthropic's Claude models via OpenAI-compatible endpoint:\r\n\r\n```typescript\r\nimport OpenAI from \"openai\";\r\n\r\nconst client = new OpenAI({\r\n  baseURL: \"https://api.thesys.dev/v1/embed\",\r\n  apiKey: process.env.THESYS_API_KEY,\r\n});\r\n```\r\n\r\n#### Model Selection\r\n\r\n```typescript\r\n// Claude Sonnet 4 (Stable - Recommended for Production)\r\nmodel: \"c1/anthropic/claude-sonnet-4/v-20250930\"\r\n\r\n// Claude 3.5 Haiku (Experimental)\r\nmodel: \"c1-exp/anthropic/claude-3.5-haiku/v-20250709\"\r\n```\r\n\r\n> ⚠️ **Deprecated Models**: Claude 3.5 Sonnet and Claude 3.7 Sonnet are no longer recommended. Use the stable Claude Sonnet 4 version above.\r\n\r\n#### Example with Claude\r\n\r\n```typescript\r\nconst response = await client.chat.completions.create({\r\n  model: \"c1/anthropic/claude-sonnet-4/v-20250930\",\r\n  messages: [\r\n    {\r\n      role: \"system\",\r\n      content: \"You are Claude, an AI assistant that creates interactive interfaces.\",\r\n    },\r\n    {\r\n      role: \"user\",\r\n      content: \"Create a product comparison chart for electric vehicles.\",\r\n    },\r\n  ],\r\n  stream: true,\r\n  temperature: 0.8,\r\n  max_tokens: 4096,\r\n});\r\n```\r\n\r\n---\r\n\r\n### Model Specifications & Pricing\r\n\r\nThe table below shows the current stable and experimental models available via TheSys C1 API:\r\n\r\n| Model | Model ID | Input Price | Output Price | Context | Max Output |\r\n|-------|----------|-------------|--------------|---------|------------|\r\n| **Claude Sonnet 4** | `c1/anthropic/claude-sonnet-4/v-20250930` | $6.00/M | $18.00/M | 180K | 64K |\r\n| **GPT 5** | `c1/openai/gpt-5/v-20250930` | $2.50/M | $12.50/M | 380K | 128K |\r\n| GPT 4.1 (exp) | `c1-exp/openai/gpt-4.1/v-20250617` | $4.00/M | $10.00/M | 1M | 32K |\r\n| Claude 3.5 Haiku (exp) | `c1-exp/anthropic/claude-3.5-haiku/v-20250709` | $1.60/M | $5.00/M | 180K | 8K |\r\n\r\n**Pricing Notes**:\r\n- Costs are per million tokens (M)\r\n- Pricing is based on model name, regardless of endpoint type (embed or visualize)\r\n- Stable models (prefixed with `c1/`) are recommended for production\r\n- Experimental models (prefixed with `c1-exp/`) are for testing and may have different behavior\r\n\r\n> **Model Versions**: Model identifiers include version dates (e.g., `v-20250930`). Always check the [TheSys Playground](https://console.thesys.dev/playground) for the latest stable versions.\r\n\r\n---\r\n\r\n### Cloudflare Workers AI Integration\r\n\r\n#### Setup with Workers AI Binding\r\n\r\n```typescript\r\n// In your Cloudflare Worker\r\nexport default {\r\n  async fetch(request: Request, env: Env) {\r\n    // Use Workers AI directly (cheaper for some use cases)\r\n    const aiResponse = await env.AI.run('@cf/meta/llama-3-8b-instruct', {\r\n      messages: [\r\n        { role: \"system\", content: \"You are a helpful assistant.\" },\r\n        { role: \"user\", content: \"Hello!\" },\r\n      ],\r\n    });\r\n\r\n    // Then transform to C1 format and send to frontend\r\n    // ...\r\n  }\r\n};\r\n```\r\n\r\n#### Hybrid Approach: Workers AI + C1\r\n\r\n```typescript\r\n// Option 1: Use Workers AI for processing, C1 for UI generation\r\nconst thinkingResponse = await env.AI.run('@cf/meta/llama-3-8b-instruct', {\r\n  messages: [{ role: \"user\", content: \"Analyze this data...\" }],\r\n});\r\n\r\n// Then use C1 to generate UI from the analysis\r\nconst c1Response = await fetch(\"https://api.thesys.dev/v1/embed/chat/completions\", {\r\n  method: \"POST\",\r\n  headers: {\r\n    \"Authorization\": `Bearer ${env.THESYS_API_KEY}`,\r\n    \"Content-Type\": \"application/json\",\r\n  },\r\n  body: JSON.stringify({\r\n    model: \"c1/openai/gpt-5/v-20250930\",\r\n    messages: [\r\n      {\r\n        role: \"system\",\r\n        content: \"Generate a chart visualization for this data.\",\r\n      },\r\n      {\r\n        role: \"user\",\r\n        content: thinkingResponse.response,\r\n      },\r\n    ],\r\n  }),\r\n});\r\n```\r\n\r\n---\r\n\r\n### Python Backend Integration\r\n\r\nTheSys provides a Python SDK for backend implementations with FastAPI, Flask, or Django.\r\n\r\n#### Setup\r\n\r\n```bash\r\npip install thesys-genui-sdk openai\r\n```\r\n\r\n#### FastAPI Example\r\n\r\n```python\r\nfrom fastapi import FastAPI\r\nfrom fastapi.responses import StreamingResponse\r\nfrom thesys_genui_sdk import with_c1_response, write_content\r\nimport openai\r\nimport os\r\n\r\napp = FastAPI()\r\n\r\nclient = openai.OpenAI(\r\n    base_url=\"https://api.thesys.dev/v1/embed\",\r\n    api_key=os.getenv(\"THESYS_API_KEY\")\r\n)\r\n\r\n@app.post(\"/api/chat\")\r\n@with_c1_response  # Automatically handles streaming headers\r\nasync def chat_endpoint(request: dict):\r\n    prompt = request.get(\"prompt\")\r\n\r\n    stream = client.chat.completions.create(\r\n        model=\"c1/anthropic/claude-sonnet-4/v-20250930\",\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": prompt}\r\n        ],\r\n        stream=True\r\n    )\r\n\r\n    # Stream chunks to frontend\r\n    async def generate():\r\n        for chunk in stream:\r\n            content = chunk.choices[0].delta.content\r\n            if content:\r\n                yield write_content(content)\r\n\r\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\r\n```\r\n\r\n#### Key Features\r\n\r\n- **`@with_c1_response` decorator**: Automatically sets proper response headers for streaming\r\n- **`write_content` helper**: Formats chunks for C1Component rendering\r\n- **Framework agnostic**: Works with FastAPI, Flask, Django, or any Python web framework\r\n\r\n#### Flask Example\r\n\r\n```python\r\nfrom flask import Flask, request, Response\r\nfrom thesys_genui_sdk import with_c1_response, write_content\r\nimport openai\r\nimport os\r\n\r\napp = Flask(__name__)\r\n\r\nclient = openai.OpenAI(\r\n    base_url=\"https://api.thesys.dev/v1/embed\",\r\n    api_key=os.getenv(\"THESYS_API_KEY\")\r\n)\r\n\r\n@app.route(\"/api/chat\", methods=[\"POST\"])\r\n@with_c1_response\r\ndef chat():\r\n    data = request.get_json()\r\n    prompt = data.get(\"prompt\")\r\n\r\n    stream = client.chat.completions.create(\r\n        model=\"c1/openai/gpt-5/v-20250930\",\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": prompt}\r\n        ],\r\n        stream=True\r\n    )\r\n\r\n    def generate():\r\n        for chunk in stream:\r\n            content = chunk.choices[0].delta.content\r\n            if content:\r\n                yield write_content(content)\r\n\r\n    return Response(generate(), mimetype=\"text/event-stream\")\r\n```\r\n\r\n---\r\n\r\n### Universal Patterns (Any Provider)\r\n\r\n#### Error Handling\r\n\r\n```typescript\r\ntry {\r\n  const response = await client.chat.completions.create({\r\n    model: \"c1/openai/gpt-5/v-20250930\",\r\n    messages: [...],\r\n    stream: true,\r\n  });\r\n\r\n  // Process stream...\r\n} catch (error) {\r\n  if (error.status === 429) {\r\n    // Rate limit - implement exponential backoff\r\n    await new Promise(resolve => setTimeout(resolve, 1000));\r\n    // Retry...\r\n  } else if (error.status === 401) {\r\n    // Invalid API key\r\n    console.error(\"Authentication failed. Check THESYS_API_KEY\");\r\n  } else {\r\n    // Other errors\r\n    console.error(\"API Error:\", error);\r\n  }\r\n}\r\n```\r\n\r\n#### Streaming with transformStream\r\n\r\n```typescript\r\nimport { transformStream } from \"@crayonai/stream\";\r\n\r\nconst llmStream = await client.chat.completions.create({\r\n  model: \"c1/openai/gpt-5/v-20250930\",\r\n  messages: [...],\r\n  stream: true,\r\n});\r\n\r\n// Transform OpenAI stream to C1 stream\r\nconst c1Stream = transformStream(llmStream, (chunk) => {\r\n  return chunk.choices[0]?.delta?.content || \"\";\r\n}) as ReadableStream<string>;\r\n\r\nreturn new Response(c1Stream, {\r\n  headers: {\r\n    \"Content-Type\": \"text/event-stream\",\r\n    \"Cache-Control\": \"no-cache, no-transform\",\r\n    \"Connection\": \"keep-alive\",\r\n  },\r\n});\r\n```\r\n\r\n---",
    "Quick Start by Framework": "### Vite + React Setup\r\n\r\n**Most flexible setup for custom backends (your preferred stack).**\r\n\r\n#### 1. Install Dependencies\r\n\r\n```bash\r\nnpm install @thesysai/genui-sdk @crayonai/react-ui @crayonai/react-core @crayonai/stream\r\nnpm install openai zod\r\n```\r\n\r\n#### 2. Create Chat Component\r\n\r\n**File**: `src/App.tsx`\r\n\r\n```typescript\r\nimport \"@crayonai/react-ui/styles/index.css\";\r\nimport { ThemeProvider, C1Component } from \"@thesysai/genui-sdk\";\r\nimport { useState } from \"react\";\r\n\r\nexport default function App() {\r\n  const [isLoading, setIsLoading] = useState(false);\r\n  const [c1Response, setC1Response] = useState(\"\");\r\n  const [question, setQuestion] = useState(\"\");\r\n\r\n  const makeApiCall = async (query: string) => {\r\n    setIsLoading(true);\r\n    setC1Response(\"\");\r\n\r\n    try {\r\n      const response = await fetch(\"/api/chat\", {\r\n        method: \"POST\",\r\n        headers: { \"Content-Type\": \"application/json\" },\r\n        body: JSON.stringify({ prompt: query }),\r\n      });\r\n\r\n      const data = await response.json();\r\n      setC1Response(data.response);\r\n    } catch (error) {\r\n      console.error(\"Error:\", error);\r\n    } finally {\r\n      setIsLoading(false);\r\n    }\r\n  };\r\n\r\n  return (\r\n    <div className=\"container\">\r\n      <h1>AI Assistant</h1>\r\n\r\n      <form onSubmit={(e) => {\r\n        e.preventDefault();\r\n        makeApiCall(question);\r\n      }}>\r\n        <input\r\n          type=\"text\"\r\n          value={question}\r\n          onChange={(e) => setQuestion(e.target.value)}\r\n          placeholder=\"Ask me anything...\"\r\n        />\r\n        <button type=\"submit\" disabled={isLoading}>\r\n          {isLoading ? \"Processing...\" : \"Send\"}\r\n        </button>\r\n      </form>\r\n\r\n      {c1Response && (\r\n        <ThemeProvider>\r\n          <C1Component\r\n            c1Response={c1Response}\r\n            isStreaming={isLoading}\r\n            updateMessage={(message) => setC1Response(message)}\r\n            onAction={({ llmFriendlyMessage }) => {\r\n              if (!isLoading) {\r\n                makeApiCall(llmFriendlyMessage);\r\n              }\r\n            }}\r\n          />\r\n        </ThemeProvider>\r\n      )}\r\n    </div>\r\n  );\r\n}\r\n```\r\n\r\n#### 3. Configure Backend API (Express Example)\r\n\r\n```typescript\r\nimport express from \"express\";\r\nimport OpenAI from \"openai\";\r\nimport { transformStream } from \"@crayonai/stream\";\r\n\r\nconst app = express();\r\napp.use(express.json());\r\n\r\nconst client = new OpenAI({\r\n  baseURL: \"https://api.thesys.dev/v1/embed\",\r\n  apiKey: process.env.THESYS_API_KEY,\r\n});\r\n\r\napp.post(\"/api/chat\", async (req, res) => {\r\n  const { prompt } = req.body;\r\n\r\n  const stream = await client.chat.completions.create({\r\n    model: \"c1/openai/gpt-5/v-20250930\", // or any C1-compatible model\r\n    messages: [\r\n      { role: \"system\", content: \"You are a helpful assistant.\" },\r\n      { role: \"user\", content: prompt },\r\n    ],\r\n    stream: true,\r\n  });\r\n\r\n  // Transform OpenAI stream to C1 response\r\n  const c1Stream = transformStream(stream, (chunk) => {\r\n    return chunk.choices[0]?.delta?.content || \"\";\r\n  });\r\n\r\n  res.json({ response: await streamToString(c1Stream) });\r\n});\r\n\r\nasync function streamToString(stream: ReadableStream) {\r\n  const reader = stream.getReader();\r\n  let result = \"\";\r\n\r\n  while (true) {\r\n    const { done, value } = await reader.read();\r\n    if (done) break;\r\n    result += value;\r\n  }\r\n\r\n  return result;\r\n}\r\n\r\napp.listen(3000);\r\n```\r\n\r\n---\r\n\r\n### Next.js App Router Setup\r\n\r\n**Most popular framework, full-stack with API routes.**\r\n\r\n#### 1. Install Dependencies\r\n\r\n```bash\r\nnpm install @thesysai/genui-sdk @crayonai/react-ui @crayonai/react-core\r\nnpm install openai\r\n```\r\n\r\n#### 2. Create Chat Page Component\r\n\r\n**File**: `app/page.tsx`\r\n\r\n```typescript\r\n\"use client\";\r\n\r\nimport { C1Chat } from \"@thesysai/genui-sdk\";\r\nimport \"@crayonai/react-ui/styles/index.css\";\r\n\r\nexport default function Home() {\r\n  return (\r\n    <div className=\"min-h-screen\">\r\n      <C1Chat apiUrl=\"/api/chat\" />\r\n    </div>\r\n  );\r\n}\r\n```\r\n\r\n#### 3. Create API Route Handler\r\n\r\n**File**: `app/api/chat/route.ts`\r\n\r\n```typescript\r\nimport { NextRequest, NextResponse } from \"next/server\";\r\nimport OpenAI from \"openai\";\r\nimport { transformStream } from \"@crayonai/stream\";\r\n\r\nconst client = new OpenAI({\r\n  baseURL: \"https://api.thesys.dev/v1/embed\",\r\n  apiKey: process.env.THESYS_API_KEY,\r\n});\r\n\r\nexport async function POST(req: NextRequest) {\r\n  const { prompt } = await req.json();\r\n\r\n  const stream = await client.chat.completions.create({\r\n    model: \"c1/openai/gpt-5/v-20250930\",\r\n    messages: [\r\n      { role: \"system\", content: \"You are a helpful AI assistant.\" },\r\n      { role: \"user\", content: prompt },\r\n    ],\r\n    stream: true,\r\n  });\r\n\r\n  // Transform to C1-compatible stream\r\n  const responseStream = transformStream(stream, (chunk) => {\r\n    return chunk.choices[0]?.delta?.content || \"\";\r\n  }) as ReadableStream<string>;\r\n\r\n  return new NextResponse(responseStream, {\r\n    headers: {\r\n      \"Content-Type\": \"text/event-stream\",\r\n      \"Cache-Control\": \"no-cache, no-transform\",\r\n      \"Connection\": \"keep-alive\",\r\n    },\r\n  });\r\n}\r\n```\r\n\r\n**That's it!** You now have a working Generative UI chat interface.\r\n\r\n---\r\n\r\n### Cloudflare Workers + Static Assets Setup\r\n\r\n**Your stack: Workers backend with Vite+React frontend.**\r\n\r\n#### 1. Create Worker Backend (Hono)\r\n\r\n**File**: `backend/src/index.ts`\r\n\r\n```typescript\r\nimport { Hono } from \"hono\";\r\nimport { cors } from \"hono/cors\";\r\n\r\nconst app = new Hono();\r\n\r\napp.use(\"/*\", cors());\r\n\r\napp.post(\"/api/chat\", async (c) => {\r\n  const { prompt } = await c.req.json();\r\n\r\n  // Use Cloudflare Workers AI or proxy to OpenAI\r\n  const response = await fetch(\"https://api.thesys.dev/v1/embed/chat/completions\", {\r\n    method: \"POST\",\r\n    headers: {\r\n      \"Authorization\": `Bearer ${c.env.THESYS_API_KEY}`,\r\n      \"Content-Type\": \"application/json\",\r\n    },\r\n    body: JSON.stringify({\r\n      model: \"c1/openai/gpt-5/v-20250930\",\r\n      messages: [\r\n        { role: \"system\", content: \"You are a helpful assistant.\" },\r\n        { role: \"user\", content: prompt },\r\n      ],\r\n      stream: false, // or handle streaming\r\n    }),\r\n  });\r\n\r\n  const data = await response.json();\r\n  return c.json(data);\r\n});\r\n\r\nexport default app;\r\n```\r\n\r\n#### 2. Frontend Setup (Same as Vite+React)\r\n\r\nUse the Vite+React example above, but configure API calls to your Worker endpoint.\r\n\r\n#### 3. Wrangler Configuration\r\n\r\n**File**: `wrangler.jsonc`\r\n\r\n```jsonc\r\n{\r\n  \"name\": \"thesys-chat-worker\",\r\n  \"compatibility_date\": \"2025-10-26\",\r\n  \"main\": \"backend/src/index.ts\",\r\n  \"vars\": {\r\n    \"ENVIRONMENT\": \"production\"\r\n  },\r\n  \"assets\": {\r\n    \"directory\": \"dist\",\r\n    \"binding\": \"ASSETS\"\r\n  }\r\n}\r\n```\r\n\r\nAdd `THESYS_API_KEY` as a secret:\r\n```bash\r\nnpx wrangler secret put THESYS_API_KEY\r\n```\r\n\r\n---",
    "Common Errors & Solutions": "npx wrangler secret put THESYS_API_KEY\r\n```\r\n\r\n---",
    "Next Steps": "1. Choose your framework (Vite+React, Next.js, or Cloudflare Workers)\r\n2. Copy the relevant template from `templates/`\r\n3. Set up `THESYS_API_KEY` environment variable\r\n4. Install dependencies with `npm install`\r\n5. Run the development server\r\n6. Customize theming and UI components\r\n7. Add tool calling for advanced features\r\n8. Deploy to production with proper persistence\r\n\r\nFor questions or issues, refer to the `references/common-errors.md` guide or check official TheSys documentation.\r\n\r\n---\r\n\r\n**Last Updated**: 2025-10-26\r\n**Package Version**: @thesysai/genui-sdk@0.6.40\r\n**Production Tested**: ✅ Yes\r\n**Official Standards Compliant**: ✅ Yes",
    "Core Components": "### `<C1Chat>` - Pre-built Chat Component\r\n\r\n**When to use**: Building conversational interfaces with minimal setup.\r\n\r\nThe `C1Chat` component is a fully-featured chat UI with built-in:\r\n- Message history\r\n- Streaming responses\r\n- Thread management\r\n- Loading states\r\n- Error handling\r\n- Responsive design\r\n\r\n#### Basic Usage\r\n\r\n```typescript\r\nimport { C1Chat } from \"@thesysai/genui-sdk\";\r\nimport \"@crayonai/react-ui/styles/index.css\";\r\n\r\nexport default function App() {\r\n  return (\r\n    <C1Chat\r\n      apiUrl=\"/api/chat\"\r\n      agentName=\"My AI Assistant\"\r\n      logoUrl=\"https://example.com/logo.png\"\r\n    />\r\n  );\r\n}\r\n```\r\n\r\n#### Key Props\r\n\r\n- **`apiUrl`** (required) - Backend endpoint for chat completions\r\n- **`agentName`** - Display name for the AI agent\r\n- **`logoUrl`** - Logo/avatar for the agent\r\n- **`theme`** - Custom theme object (see Theming section)\r\n- **`threadManager`** - For multi-thread support (advanced)\r\n- **`threadListManager`** - For thread list UI (advanced)\r\n- **`customizeC1`** - Custom components (footer, thinking states)\r\n\r\n#### With Theme\r\n\r\n```typescript\r\nimport { C1Chat } from \"@thesysai/genui-sdk\";\r\nimport { themePresets } from \"@crayonai/react-ui\";\r\n\r\n<C1Chat\r\n  apiUrl=\"/api/chat\"\r\n  theme={themePresets.candy} // or 'default', or custom object\r\n/>\r\n```\r\n\r\n---\r\n\r\n### `<C1Component>` - Custom Integration Component\r\n\r\n**When to use**: Need full control over state management and UI layout.\r\n\r\nThe `C1Component` is the low-level renderer. You handle:\r\n- Fetching data\r\n- Managing state\r\n- Layout structure\r\n- Error boundaries\r\n\r\n#### Basic Usage\r\n\r\n```typescript\r\nimport { C1Component, ThemeProvider } from \"@thesysai/genui-sdk\";\r\nimport \"@crayonai/react-ui/styles/index.css\";\r\n\r\nconst [c1Response, setC1Response] = useState(\"\");\r\nconst [isStreaming, setIsStreaming] = useState(false);\r\n\r\n// ... fetch logic\r\n\r\nreturn (\r\n  <ThemeProvider>\r\n    <C1Component\r\n      c1Response={c1Response}\r\n      isStreaming={isStreaming}\r\n      updateMessage={(message) => setC1Response(message)}\r\n      onAction={({ llmFriendlyMessage }) => {\r\n        // Handle interactive actions (button clicks, form submissions)\r\n        console.log(\"User action:\", llmFriendlyMessage);\r\n        // Make new API call with llmFriendlyMessage\r\n      }}\r\n    />\r\n  </ThemeProvider>\r\n);\r\n```\r\n\r\n#### Key Props\r\n\r\n- **`c1Response`** (required) - The C1 API response string\r\n- **`isStreaming`** - Whether response is still streaming (shows loading indicator)\r\n- **`updateMessage`** - Callback for response updates during streaming\r\n- **`onAction`** - Callback for user interactions with generated UI\r\n  - `llmFriendlyMessage`: Pre-formatted message to send back to LLM\r\n  - `rawAction`: Raw action data from the component\r\n\r\n#### Important: Must Wrap with ThemeProvider\r\n\r\n```typescript\r\n// ❌ Wrong - theme won't apply\r\n<C1Component c1Response={response} />\r\n\r\n// ✅ Correct\r\n<ThemeProvider>\r\n  <C1Component c1Response={response} />\r\n</ThemeProvider>\r\n```\r\n\r\n---\r\n\r\n### `<ThemeProvider>` - Theming and Customization\r\n\r\n**When to use**: Always wrap `<C1Component>` or customize `<C1Chat>` appearance.\r\n\r\n#### Theme Presets\r\n\r\nTheSys includes pre-built themes:\r\n\r\n```typescript\r\nimport { themePresets } from \"@crayonai/react-ui\";\r\n\r\n// Available presets:\r\n// - themePresets.default\r\n// - themePresets.candy\r\n// ... (check docs for full list)\r\n\r\n<C1Chat theme={themePresets.candy} />\r\n```\r\n\r\n#### Dark Mode Support\r\n\r\n```typescript\r\nimport { useSystemTheme } from \"./hooks/useSystemTheme\"; // custom hook\r\n\r\nexport default function App() {\r\n  const systemTheme = useSystemTheme(); // 'light' | 'dark'\r\n\r\n  return (\r\n    <C1Chat\r\n      apiUrl=\"/api/chat\"\r\n      theme={{ ...themePresets.default, mode: systemTheme }}\r\n    />\r\n  );\r\n}\r\n```\r\n\r\n#### Custom Theme Object\r\n\r\n```typescript\r\nconst customTheme = {\r\n  mode: \"dark\", // 'light' | 'dark' | 'system'\r\n  colors: {\r\n    primary: \"#3b82f6\",\r\n    secondary: \"#8b5cf6\",\r\n    background: \"#1f2937\",\r\n    foreground: \"#f9fafb\",\r\n    // ... more colors\r\n  },\r\n  fonts: {\r\n    body: \"Inter, sans-serif\",\r\n    heading: \"Poppins, sans-serif\",\r\n  },\r\n  borderRadius: \"12px\",\r\n  spacing: {\r\n    base: \"16px\",\r\n  },\r\n};\r\n\r\n<C1Chat theme={customTheme} />\r\n```\r\n\r\n#### CSS Overrides\r\n\r\nCreate a `custom.css` file:\r\n\r\n```css\r\n/* Override specific component styles */\r\n.c1-chat-container {\r\n  max-width: 900px;\r\n  margin: 0 auto;\r\n}\r\n\r\n.c1-message-user {\r\n  background-color: #3b82f6 !important;\r\n}\r\n\r\n.c1-message-assistant {\r\n  background-color: #6b7280 !important;\r\n}\r\n```\r\n\r\nThen import:\r\n\r\n```typescript\r\nimport \"@crayonai/react-ui/styles/index.css\";\r\nimport \"./custom.css\"; // AFTER the default styles\r\n```\r\n\r\n---",
    "Additional Resources": "### Reference Guides\r\n\r\nSee the `references/` directory for detailed guides:\r\n\r\n- **`component-api.md`** - Complete prop reference for all components\r\n- **`ai-provider-setup.md`** - Step-by-step setup for each AI provider\r\n- **`tool-calling-guide.md`** - Comprehensive tool calling patterns\r\n- **`theme-customization.md`** - Theme system deep dive\r\n- **`common-errors.md`** - Expanded error catalog with solutions\r\n\r\n### Scripts\r\n\r\n- **`scripts/install-dependencies.sh`** - Install all required packages\r\n- **`scripts/check-versions.sh`** - Verify package versions\r\n\r\n### Official Documentation\r\n\r\n- TheSys Docs: https://docs.thesys.dev\r\n- C1 Playground: https://console.thesys.dev/playground\r\n- GitHub Examples: Search for \"thesysai\" on GitHub\r\n- Context7: `/websites/thesys_dev`\r\n\r\n---",
    "Production Patterns": "### Message Persistence\r\n\r\n**Don't use in-memory storage in production!**\r\n\r\n```typescript\r\n// ❌ Bad - loses data on restart\r\nconst messageStore = new Map<string, Message[]>();\r\n\r\n// ✅ Good - use a database\r\nimport { db } from \"./database\"; // D1, PostgreSQL, etc.\r\n\r\nexport async function saveMessage(threadId: string, message: Message) {\r\n  await db.insert(messages).values({\r\n    threadId,\r\n    role: message.role,\r\n    content: message.content,\r\n    createdAt: new Date(),\r\n  });\r\n}\r\n\r\nexport async function getThreadMessages(threadId: string): Promise<Message[]> {\r\n  return db.select().from(messages).where(eq(messages.threadId, threadId));\r\n}\r\n```\r\n\r\n### Authentication Integration (Clerk Example)\r\n\r\n```typescript\r\nimport { auth } from \"@clerk/nextjs\";\r\n\r\nexport async function POST(req: NextRequest) {\r\n  const { userId } = auth();\r\n\r\n  if (!userId) {\r\n    return NextResponse.json({ error: \"Unauthorized\" }, { status: 401 });\r\n  }\r\n\r\n  // Proceed with chat logic, scoping to user\r\n  const userThreads = await db\r\n    .select()\r\n    .from(threads)\r\n    .where(eq(threads.userId, userId));\r\n\r\n  // ...\r\n}\r\n```\r\n\r\n### Rate Limiting\r\n\r\n```typescript\r\nimport { Ratelimit } from \"@upstash/ratelimit\";\r\nimport { Redis } from \"@upstash/redis\";\r\n\r\nconst ratelimit = new Ratelimit({\r\n  redis: Redis.fromEnv(),\r\n  limiter: Ratelimit.slidingWindow(10, \"1 m\"), // 10 requests per minute\r\n});\r\n\r\nexport async function POST(req: NextRequest) {\r\n  const { userId } = auth();\r\n  const { success } = await ratelimit.limit(userId);\r\n\r\n  if (!success) {\r\n    return NextResponse.json(\r\n      { error: \"Rate limit exceeded. Please try again later.\" },\r\n      { status: 429 }\r\n    );\r\n  }\r\n\r\n  // Proceed...\r\n}\r\n```\r\n\r\n### Error Boundaries\r\n\r\n```typescript\r\nimport { ErrorBoundary } from \"react-error-boundary\";\r\n\r\nfunction ErrorFallback({ error, resetErrorBoundary }) {\r\n  return (\r\n    <div role=\"alert\">\r\n      <h2>Something went wrong</h2>\r\n      <pre>{error.message}</pre>\r\n      <button onClick={resetErrorBoundary}>Try again</button>\r\n    </div>\r\n  );\r\n}\r\n\r\nexport default function App() {\r\n  return (\r\n    <ErrorBoundary FallbackComponent={ErrorFallback}>\r\n      <C1Chat apiUrl=\"/api/chat\" />\r\n    </ErrorBoundary>\r\n  );\r\n}\r\n```\r\n\r\n### Performance Optimization\r\n\r\n```typescript\r\n// 1. Lazy load C1Chat\r\nimport { lazy, Suspense } from \"react\";\r\n\r\nconst C1Chat = lazy(() =>\r\n  import(\"@thesysai/genui-sdk\").then((mod) => ({ default: mod.C1Chat }))\r\n);\r\n\r\nexport default function App() {\r\n  return (\r\n    <Suspense fallback={<div>Loading chat...</div>}>\r\n      <C1Chat apiUrl=\"/api/chat\" />\r\n    </Suspense>\r\n  );\r\n}\r\n\r\n// 2. Memoize expensive computations\r\nimport { useMemo } from \"react\";\r\n\r\nconst threadListManager = useMemo(\r\n  () =>\r\n    useThreadListManager({\r\n      // ... config\r\n    }),\r\n  [] // Empty deps - only create once\r\n);\r\n```\r\n\r\n---"
  }
}