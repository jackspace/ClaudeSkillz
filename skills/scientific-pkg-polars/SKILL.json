{
  "description": "\"Fast DataFrame library (Apache Arrow). Select, filter, group_by, joins, lazy evaluation, CSV/Parquet I/O, expression API, for high-performance data analysis workflows.\"",
  "references": {
    "files": [
      "references/best_practices.md",
      "references/core_concepts.md",
      "references/io_guide.md",
      "references/operations.md",
      "references/pandas_migration.md",
      "references/transformations.md"
    ]
  },
  "content": "### Installation and Basic Usage\r\n\r\nInstall Polars:\r\n```python\r\npip install polars\r\n```\r\n\r\nBasic DataFrame creation and operations:\r\n```python\r\nimport polars as pl\r\n\r\ndf = pl.DataFrame({\r\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\r\n    \"age\": [25, 30, 35],\r\n    \"city\": [\"NY\", \"LA\", \"SF\"]\r\n})\r\n\r\ndf.select(\"name\", \"age\")\r\n\r\ndf.filter(pl.col(\"age\") > 25)\r\n\r\n\r\n### Expressions\r\n\r\nExpressions are the fundamental building blocks of Polars operations. They describe transformations on data and can be composed, reused, and optimized.\r\n\r\n**Key principles:**\r\n- Use `pl.col(\"column_name\")` to reference columns\r\n- Chain methods to build complex transformations\r\n- Expressions are lazy and only execute within contexts (select, with_columns, filter, group_by)\r\n\r\n**Example:**\r\n```python\r\n\r\n### Select\r\nSelect and manipulate columns:\r\n```python\r\ndf.select(\"name\", \"age\")\r\n\r\ndf.select(\r\n    pl.col(\"name\"),\r\n    (pl.col(\"age\") * 2).alias(\"double_age\")\r\n)\r\n\r\ndf.select(pl.col(\"^.*_id$\"))\r\n```\r\n\r\n### Filter\r\nFilter rows by conditions:\r\n```python\r\ndf.filter(pl.col(\"age\") > 25)\r\n\r\ndf.filter(\r\n    pl.col(\"age\") > 25,\r\n    pl.col(\"city\") == \"NY\"\r\n)\r\n\r\ndf.filter(\r\n    (pl.col(\"age\") > 25) | (pl.col(\"city\") == \"LA\")\r\n)\r\n```\r\n\r\n### With Columns\r\nAdd or modify columns while preserving existing ones:\r\n```python\r\ndf.with_columns(\r\n    age_plus_10=pl.col(\"age\") + 10,\r\n    name_upper=pl.col(\"name\").str.to_uppercase()\r\n)\r\n\r\ndf.with_columns(\r\n    pl.col(\"value\") * 10,\r\n    pl.col(\"value\") * 100,\r\n)\r\n```\r\n\r\n### Group By and Aggregations\r\nGroup data and compute aggregations:\r\n```python\r\ndf.group_by(\"city\").agg(\r\n    pl.col(\"age\").mean().alias(\"avg_age\"),\r\n    pl.len().alias(\"count\")\r\n)\r\n\r\ndf.group_by(\"city\", \"department\").agg(\r\n    pl.col(\"salary\").sum()\r\n)\r\n\r\n\r\n### Aggregation Functions\r\nCommon aggregations within `group_by` context:\r\n- `pl.len()` - count rows\r\n- `pl.col(\"x\").sum()` - sum values\r\n- `pl.col(\"x\").mean()` - average\r\n- `pl.col(\"x\").min()` / `pl.col(\"x\").max()` - extremes\r\n- `pl.first()` / `pl.last()` - first/last values\r\n\r\n### Window Functions with `over()`\r\nApply aggregations while preserving row count:\r\n```python\r\ndf.with_columns(\r\n    avg_age_by_city=pl.col(\"age\").mean().over(\"city\"),\r\n    rank_in_city=pl.col(\"salary\").rank().over(\"city\")\r\n)\r\n\r\n\r\n### Supported Formats\r\nPolars supports reading and writing:\r\n- CSV, Parquet, JSON, Excel\r\n- Databases (via connectors)\r\n- Cloud storage (S3, Azure, GCS)\r\n- Google BigQuery\r\n- Multiple/partitioned files\r\n\r\n### Common I/O Operations\r\n\r\n**CSV:**\r\n```python\r\ndf = pl.read_csv(\"file.csv\")\r\ndf.write_csv(\"output.csv\")\r\n\r\n\r\n### Joins\r\nCombine DataFrames:\r\n```python\r\ndf1.join(df2, on=\"id\", how=\"inner\")\r\n\r\ndf1.join(df2, on=\"id\", how=\"left\")\r\n\r\ndf1.join(df2, left_on=\"user_id\", right_on=\"id\")\r\n```\r\n\r\n### Concatenation\r\nStack DataFrames:\r\n```python\r\npl.concat([df1, df2], how=\"vertical\")\r\n\r\npl.concat([df1, df2], how=\"horizontal\")\r\n\r\npl.concat([df1, df2], how=\"diagonal\")\r\n```\r\n\r\n### Pivot and Unpivot\r\nReshape data:\r\n```python\r\ndf.pivot(values=\"sales\", index=\"date\", columns=\"product\")",
  "name": "polars",
  "id": "scientific-pkg-polars",
  "sections": {
    "Common Operations": "df.group_by(\"city\").agg(\r\n    (pl.col(\"age\") > 30).sum().alias(\"over_30\")\r\n)\r\n```\r\n\r\nFor detailed operation patterns, load `references/operations.md`.",
    "Aggregations and Window Functions": "df.with_columns(\r\n    group_avg=pl.col(\"value\").mean().over(\"category\", \"region\")\r\n)\r\n```\r\n\r\n**Mapping strategies:**\r\n- `group_to_rows` (default): Preserves original row order\r\n- `explode`: Faster but groups rows together\r\n- `join`: Creates list columns",
    "Data I/O": "lf = pl.scan_csv(\"file.csv\")\r\nresult = lf.filter(...).select(...).collect()\r\n```\r\n\r\n**Parquet (recommended for performance):**\r\n```python\r\ndf = pl.read_parquet(\"file.parquet\")\r\ndf.write_parquet(\"output.parquet\")\r\n```\r\n\r\n**JSON:**\r\n```python\r\ndf = pl.read_json(\"file.json\")\r\ndf.write_json(\"output.json\")\r\n```\r\n\r\nFor comprehensive I/O documentation, load `references/io_guide.md`.",
    "Transformations": "df.unpivot(index=\"id\", on=[\"col1\", \"col2\"])\r\n```\r\n\r\nFor detailed transformation examples, load `references/transformations.md`.",
    "Overview": "Polars is a lightning-fast DataFrame library for Python and Rust built on Apache Arrow. Work with Polars' expression-based API, lazy evaluation framework, and high-performance data manipulation capabilities for efficient data processing, pandas migration, and data pipeline optimization.",
    "Pandas Migration": "Polars offers significant performance improvements over pandas with a cleaner API. Key differences:\r\n\r\n### Conceptual Differences\r\n- **No index**: Polars uses integer positions only\r\n- **Strict typing**: No silent type conversions\r\n- **Lazy evaluation**: Available via LazyFrame\r\n- **Parallel by default**: Operations parallelized automatically\r\n\r\n### Common Operation Mappings\r\n\r\n| Operation | Pandas | Polars |\r\n|-----------|--------|--------|\r\n| Select column | `df[\"col\"]` | `df.select(\"col\")` |\r\n| Filter | `df[df[\"col\"] > 10]` | `df.filter(pl.col(\"col\") > 10)` |\r\n| Add column | `df.assign(x=...)` | `df.with_columns(x=...)` |\r\n| Group by | `df.groupby(\"col\").agg(...)` | `df.group_by(\"col\").agg(...)` |\r\n| Window | `df.groupby(\"col\").transform(...)` | `df.with_columns(...).over(\"col\")` |\r\n\r\n### Key Syntax Patterns\r\n\r\n**Pandas sequential (slow):**\r\n```python\r\ndf.assign(\r\n    col_a=lambda df_: df_.value * 10,\r\n    col_b=lambda df_: df_.value * 100\r\n)\r\n```\r\n\r\n**Polars parallel (fast):**\r\n```python\r\ndf.with_columns(\r\n    col_a=pl.col(\"value\") * 10,\r\n    col_b=pl.col(\"value\") * 100,\r\n)\r\n```\r\n\r\nFor comprehensive migration guide, load `references/pandas_migration.md`.",
    "Resources": "This skill includes comprehensive reference documentation:\r\n\r\n### references/\r\n- `core_concepts.md` - Detailed explanations of expressions, lazy evaluation, and type system\r\n- `operations.md` - Comprehensive guide to all common operations with examples\r\n- `pandas_migration.md` - Complete migration guide from pandas to Polars\r\n- `io_guide.md` - Data I/O operations for all supported formats\r\n- `transformations.md` - Joins, concatenation, pivots, and reshaping operations\r\n- `best_practices.md` - Performance optimization tips and common patterns\r\n\r\nLoad these references as needed when users require detailed information about specific topics.",
    "Best Practices": "### Performance Optimization\r\n\r\n1. **Use lazy evaluation for large datasets:**\r\n   ```python\r\n   lf = pl.scan_csv(\"large.csv\")  # Don't use read_csv\r\n   result = lf.filter(...).select(...).collect()\r\n   ```\r\n\r\n2. **Avoid Python functions in hot paths:**\r\n   - Stay within expression API for parallelization\r\n   - Use `.map_elements()` only when necessary\r\n   - Prefer native Polars operations\r\n\r\n3. **Use streaming for very large data:**\r\n   ```python\r\n   lf.collect(streaming=True)\r\n   ```\r\n\r\n4. **Select only needed columns early:**\r\n   ```python\r\n   # Good: Select columns early\r\n   lf.select(\"col1\", \"col2\").filter(...)\r\n\r\n   # Bad: Filter on all columns first\r\n   lf.filter(...).select(\"col1\", \"col2\")\r\n   ```\r\n\r\n5. **Use appropriate data types:**\r\n   - Categorical for low-cardinality strings\r\n   - Appropriate integer sizes (i32 vs i64)\r\n   - Date types for temporal data\r\n\r\n### Expression Patterns\r\n\r\n**Conditional operations:**\r\n```python\r\npl.when(condition).then(value).otherwise(other_value)\r\n```\r\n\r\n**Column operations across multiple columns:**\r\n```python\r\ndf.select(pl.col(\"^.*_value$\") * 2)  # Regex pattern\r\n```\r\n\r\n**Null handling:**\r\n```python\r\npl.col(\"x\").fill_null(0)\r\npl.col(\"x\").is_null()\r\npl.col(\"x\").drop_nulls()\r\n```\r\n\r\nFor additional best practices and patterns, load `references/best_practices.md`.",
    "Core Concepts": "df.select(\r\n    pl.col(\"name\"),\r\n    (pl.col(\"age\") * 12).alias(\"age_in_months\")\r\n)\r\n```\r\n\r\n### Lazy vs Eager Evaluation\r\n\r\n**Eager (DataFrame):** Operations execute immediately\r\n```python\r\ndf = pl.read_csv(\"file.csv\")  # Reads immediately\r\nresult = df.filter(pl.col(\"age\") > 25)  # Executes immediately\r\n```\r\n\r\n**Lazy (LazyFrame):** Operations build a query plan, optimized before execution\r\n```python\r\nlf = pl.scan_csv(\"file.csv\")  # Doesn't read yet\r\nresult = lf.filter(pl.col(\"age\") > 25).select(\"name\", \"age\")\r\ndf = result.collect()  # Now executes optimized query\r\n```\r\n\r\n**When to use lazy:**\r\n- Working with large datasets\r\n- Complex query pipelines\r\n- When only some columns/rows are needed\r\n- Performance is critical\r\n\r\n**Benefits of lazy evaluation:**\r\n- Automatic query optimization\r\n- Predicate pushdown\r\n- Projection pushdown\r\n- Parallel execution\r\n\r\nFor detailed concepts, load `references/core_concepts.md`.",
    "Quick Start": "df.with_columns(\r\n    age_plus_10=pl.col(\"age\") + 10\r\n)\r\n```"
  }
}