{
  "description": "|",
  "metadata": {
    "license": "MIT"
  },
  "references": {
    "files": [
      "references/common-patterns.md",
      "references/s3-compatibility.md",
      "references/workers-api.md"
    ]
  },
  "content": "**Status**: Production Ready ‚úÖ\r\n**Last Updated**: 2025-10-21\r\n**Dependencies**: cloudflare-worker-base (for Worker setup)\r\n**Latest Versions**: wrangler@4.43.0, @cloudflare/workers-types@4.20251014.0, aws4fetch@1.0.20\r\n\r\n---\r\n\r\n\r\n### 1. Create R2 Bucket\r\n\r\n```bash\r\nnpx wrangler r2 bucket create my-bucket\r\n\r\n```\r\n\r\n**Bucket Naming Rules:**\r\n- 3-63 characters\r\n- Lowercase letters, numbers, hyphens only\r\n- Must start/end with letter or number\r\n- Globally unique within your account\r\n\r\n### 2. Configure R2 Binding\r\n\r\nAdd to your `wrangler.jsonc`:\r\n\r\n```jsonc\r\n{\r\n  \"name\": \"my-worker\",\r\n  \"main\": \"src/index.ts\",\r\n  \"compatibility_date\": \"2025-10-11\",\r\n  \"r2_buckets\": [\r\n    {\r\n      \"binding\": \"MY_BUCKET\",          // Available as env.MY_BUCKET in your Worker\r\n      \"bucket_name\": \"my-bucket\",      // Name from wrangler r2 bucket create\r\n      \"preview_bucket_name\": \"my-bucket-preview\"  // Optional: separate bucket for dev\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n**CRITICAL:**\r\n- `binding` is how you access the bucket in code (`env.MY_BUCKET`)\r\n- `bucket_name` is the actual R2 bucket name\r\n- `preview_bucket_name` is optional but recommended for separate dev/prod data\r\n\r\n### 3. Basic Upload/Download\r\n\r\n```typescript\r\n// src/index.ts\r\nimport { Hono } from 'hono';\r\n\r\ntype Bindings = {\r\n  MY_BUCKET: R2Bucket;\r\n};\r\n\r\nconst app = new Hono<{ Bindings: Bindings }>();\r\n\r\n// Upload file\r\napp.put('/upload/:filename', async (c) => {\r\n  const filename = c.req.param('filename');\r\n  const body = await c.req.arrayBuffer();\r\n\r\n  try {\r\n    const object = await c.env.MY_BUCKET.put(filename, body, {\r\n      httpMetadata: {\r\n        contentType: c.req.header('content-type') || 'application/octet-stream',\r\n      },\r\n    });\r\n\r\n    return c.json({\r\n      success: true,\r\n      key: object.key,\r\n      size: object.size,\r\n      etag: object.etag,\r\n    });\r\n  } catch (error: any) {\r\n    console.error('R2 Upload Error:', error.message);\r\n    return c.json({ error: 'Upload failed' }, 500);\r\n  }\r\n});\r\n\r\n// Download file\r\napp.get('/download/:filename', async (c) => {\r\n  const filename = c.req.param('filename');\r\n\r\n  try {\r\n    const object = await c.env.MY_BUCKET.get(filename);\r\n\r\n    if (!object) {\r\n      return c.json({ error: 'File not found' }, 404);\r\n    }\r\n\r\n    return new Response(object.body, {\r\n      headers: {\r\n        'Content-Type': object.httpMetadata?.contentType || 'application/octet-stream',\r\n        'ETag': object.httpEtag,\r\n        'Cache-Control': object.httpMetadata?.cacheControl || 'public, max-age=3600',\r\n      },\r\n    });\r\n  } catch (error: any) {\r\n    console.error('R2 Download Error:', error.message);\r\n    return c.json({ error: 'Download failed' }, 500);\r\n  }\r\n});\r\n\r\nexport default app;\r\n```\r\n\r\n### 4. Deploy and Test\r\n\r\n```bash\r\nnpx wrangler deploy\r\n\r\ncurl -X PUT https://my-worker.workers.dev/upload/test.txt \\\r\n  -H \"Content-Type: text/plain\" \\\r\n  -d \"Hello, R2!\"\r\n\r\n\r\n```bash\r\nwrangler r2 bucket create <BUCKET_NAME>\r\nwrangler r2 bucket list\r\nwrangler r2 bucket delete <BUCKET_NAME>\r\n\r\nwrangler r2 object put <BUCKET_NAME>/<KEY> --file=<FILE_PATH>\r\nwrangler r2 object get <BUCKET_NAME>/<KEY> --file=<OUTPUT_PATH>\r\nwrangler r2 object delete <BUCKET_NAME>/<KEY>",
  "name": "cloudflare-r2",
  "id": "cloudflare-r2",
  "sections": {
    "Presigned URLs": "Presigned URLs allow clients to upload/download objects directly to/from R2 without going through your Worker.\r\n\r\n### When to Use Presigned URLs\r\n\r\n‚úÖ **Use presigned URLs when:**\r\n- Client uploads directly to R2 (saves Worker bandwidth)\r\n- Temporary access to private objects\r\n- Sharing download links with expiry\r\n- Avoiding Worker request size limits\r\n\r\n‚ùå **Don't use presigned URLs when:**\r\n- You need to process/validate uploads\r\n- Files are already public\r\n- Using custom domains (presigned URLs only work with R2 endpoint)\r\n\r\n### Generate Presigned URLs with aws4fetch\r\n\r\n```bash\r\nnpm install aws4fetch\r\n```\r\n\r\n```typescript\r\nimport { AwsClient } from 'aws4fetch';\r\n\r\ninterface Env {\r\n  R2_ACCESS_KEY_ID: string;\r\n  R2_SECRET_ACCESS_KEY: string;\r\n  ACCOUNT_ID: string;\r\n  MY_BUCKET: R2Bucket;\r\n}\r\n\r\nconst app = new Hono<{ Bindings: Env }>();\r\n\r\napp.post('/api/presigned-upload', async (c) => {\r\n  const { filename } = await c.req.json();\r\n\r\n  // Create AWS client for R2\r\n  const r2Client = new AwsClient({\r\n    accessKeyId: c.env.R2_ACCESS_KEY_ID,\r\n    secretAccessKey: c.env.R2_SECRET_ACCESS_KEY,\r\n  });\r\n\r\n  const bucketName = 'my-bucket';\r\n  const accountId = c.env.ACCOUNT_ID;\r\n\r\n  const url = new URL(\r\n    `https://${bucketName}.${accountId}.r2.cloudflarestorage.com/${filename}`\r\n  );\r\n\r\n  // Set expiry (1 hour)\r\n  url.searchParams.set('X-Amz-Expires', '3600');\r\n\r\n  // Sign the URL for PUT\r\n  const signed = await r2Client.sign(\r\n    new Request(url, { method: 'PUT' }),\r\n    { aws: { signQuery: true } }\r\n  );\r\n\r\n  return c.json({\r\n    uploadUrl: signed.url,\r\n    expiresIn: 3600,\r\n  });\r\n});\r\n\r\napp.post('/api/presigned-download', async (c) => {\r\n  const { filename } = await c.req.json();\r\n\r\n  const r2Client = new AwsClient({\r\n    accessKeyId: c.env.R2_ACCESS_KEY_ID,\r\n    secretAccessKey: c.env.R2_SECRET_ACCESS_KEY,\r\n  });\r\n\r\n  const bucketName = 'my-bucket';\r\n  const accountId = c.env.ACCOUNT_ID;\r\n\r\n  const url = new URL(\r\n    `https://${bucketName}.${accountId}.r2.cloudflarestorage.com/${filename}`\r\n  );\r\n\r\n  url.searchParams.set('X-Amz-Expires', '3600');\r\n\r\n  const signed = await r2Client.sign(\r\n    new Request(url, { method: 'GET' }),\r\n    { aws: { signQuery: true } }\r\n  );\r\n\r\n  return c.json({\r\n    downloadUrl: signed.url,\r\n    expiresIn: 3600,\r\n  });\r\n});\r\n```\r\n\r\n### Client-Side Upload with Presigned URL\r\n\r\n```javascript\r\n// 1. Get presigned URL from your Worker\r\nconst response = await fetch('https://my-worker.workers.dev/api/presigned-upload', {\r\n  method: 'POST',\r\n  headers: { 'Content-Type': 'application/json' },\r\n  body: JSON.stringify({ filename: 'photo.jpg' }),\r\n});\r\n\r\nconst { uploadUrl } = await response.json();\r\n\r\n// 2. Upload file directly to R2\r\nconst file = document.querySelector('input[type=\"file\"]').files[0];\r\n\r\nawait fetch(uploadUrl, {\r\n  method: 'PUT',\r\n  body: file,\r\n  headers: {\r\n    'Content-Type': file.type,\r\n  },\r\n});\r\n```\r\n\r\n### Presigned URL Security\r\n\r\n**CRITICAL:**\r\n- ‚ùå **NEVER** expose R2 access keys in client-side code\r\n- ‚úÖ **ALWAYS** generate presigned URLs server-side\r\n- ‚úÖ **ALWAYS** set appropriate expiry times (1-24 hours typical)\r\n- ‚úÖ **CONSIDER** adding authentication before generating URLs\r\n- ‚úÖ **CONSIDER** rate limiting presigned URL generation\r\n\r\n```typescript\r\n// Example with auth check\r\napp.post('/api/presigned-upload', async (c) => {\r\n  // Verify user is authenticated\r\n  const authHeader = c.req.header('Authorization');\r\n  if (!authHeader) {\r\n    return c.json({ error: 'Unauthorized' }, 401);\r\n  }\r\n\r\n  // Validate user has permission\r\n  const userId = await verifyToken(authHeader);\r\n  if (!userId) {\r\n    return c.json({ error: 'Invalid token' }, 401);\r\n  }\r\n\r\n  // Only allow uploads to user's own folder\r\n  const { filename } = await c.req.json();\r\n  const key = `users/${userId}/${filename}`;\r\n\r\n  // Generate presigned URL...\r\n});\r\n```\r\n\r\n---",
    "Performance Optimization": "### Batch Operations\r\n\r\n```typescript\r\n// ‚ùå DON'T: Delete files one by one\r\nfor (const file of filesToDelete) {\r\n  await env.MY_BUCKET.delete(file);\r\n}\r\n\r\n// ‚úÖ DO: Batch delete (up to 1000 keys)\r\nawait env.MY_BUCKET.delete(filesToDelete);\r\n```\r\n\r\n### Range Requests for Large Files\r\n\r\n```typescript\r\n// Download only the first 10MB of a large video\r\nconst object = await env.MY_BUCKET.get('video.mp4', {\r\n  range: { offset: 0, length: 10 * 1024 * 1024 },\r\n});\r\n\r\n// Return range to client\r\nreturn new Response(object.body, {\r\n  status: 206, // Partial Content\r\n  headers: {\r\n    'Content-Type': 'video/mp4',\r\n    'Content-Range': `bytes 0-${10 * 1024 * 1024 - 1}/${object.size}`,\r\n  },\r\n});\r\n```\r\n\r\n### Cache Headers\r\n\r\n```typescript\r\n// Immutable assets (hashed filenames)\r\nawait env.MY_BUCKET.put('static/app.abc123.js', jsData, {\r\n  httpMetadata: {\r\n    contentType: 'application/javascript',\r\n    cacheControl: 'public, max-age=31536000, immutable',\r\n  },\r\n});\r\n\r\n// Dynamic content\r\nawait env.MY_BUCKET.put('api/latest.json', jsonData, {\r\n  httpMetadata: {\r\n    contentType: 'application/json',\r\n    cacheControl: 'public, max-age=60, stale-while-revalidate=300',\r\n  },\r\n});\r\n```\r\n\r\n### Checksums for Data Integrity\r\n\r\n```typescript\r\n// Compute MD5 checksum\r\nconst md5Hash = await crypto.subtle.digest('MD5', fileData);\r\n\r\n// R2 will verify checksum on upload\r\nawait env.MY_BUCKET.put('important.dat', fileData, {\r\n  md5: md5Hash,\r\n});\r\n\r\n// If checksum doesn't match, upload will fail\r\n```\r\n\r\n---",
    "Quick Start (5 Minutes)": "curl https://my-worker.workers.dev/download/test.txt\r\n```\r\n\r\n---",
    "Custom Metadata": "Store arbitrary key-value metadata with objects.\r\n\r\n### Setting Custom Metadata\r\n\r\n```typescript\r\nawait env.MY_BUCKET.put('document.pdf', pdfData, {\r\n  customMetadata: {\r\n    userId: '12345',\r\n    department: 'engineering',\r\n    uploadDate: new Date().toISOString(),\r\n    version: '1.0',\r\n    approved: 'true',\r\n  },\r\n});\r\n```\r\n\r\n### Reading Custom Metadata\r\n\r\n```typescript\r\nconst object = await env.MY_BUCKET.head('document.pdf');\r\n\r\nif (object) {\r\n  console.log(object.customMetadata);\r\n  // {\r\n  //   userId: '12345',\r\n  //   department: 'engineering',\r\n  //   uploadDate: '2025-10-21T10:00:00.000Z',\r\n  //   version: '1.0',\r\n  //   approved: 'true'\r\n  // }\r\n}\r\n```\r\n\r\n### Limitations\r\n\r\n- Max 2KB total size for all custom metadata\r\n- Keys and values must be strings\r\n- Keys are case-insensitive\r\n- No limit on number of keys (within 2KB total)\r\n\r\n---",
    "R2 Workers API": "### Type Definitions\r\n\r\n```typescript\r\n// Add to env.d.ts or worker-configuration.d.ts\r\ninterface Env {\r\n  MY_BUCKET: R2Bucket;\r\n  // ... other bindings\r\n}\r\n\r\n// For Hono\r\ntype Bindings = {\r\n  MY_BUCKET: R2Bucket;\r\n};\r\n\r\nconst app = new Hono<{ Bindings: Bindings }>();\r\n```\r\n\r\n### put() - Upload Objects\r\n\r\n**Signature:**\r\n```typescript\r\nput(key: string, value: ReadableStream | ArrayBuffer | ArrayBufferView | string | Blob, options?: R2PutOptions): Promise<R2Object | null>\r\n```\r\n\r\n**Basic Usage:**\r\n\r\n```typescript\r\n// Upload from request body\r\nawait env.MY_BUCKET.put('path/to/file.txt', request.body);\r\n\r\n// Upload string\r\nawait env.MY_BUCKET.put('config.json', JSON.stringify({ foo: 'bar' }));\r\n\r\n// Upload ArrayBuffer\r\nawait env.MY_BUCKET.put('image.png', await file.arrayBuffer());\r\n```\r\n\r\n**With Metadata:**\r\n\r\n```typescript\r\nconst object = await env.MY_BUCKET.put('document.pdf', fileData, {\r\n  httpMetadata: {\r\n    contentType: 'application/pdf',\r\n    contentLanguage: 'en-US',\r\n    contentDisposition: 'attachment; filename=\"report.pdf\"',\r\n    contentEncoding: 'gzip',\r\n    cacheControl: 'public, max-age=86400',\r\n  },\r\n  customMetadata: {\r\n    userId: '12345',\r\n    uploadDate: new Date().toISOString(),\r\n    version: '1.0',\r\n  },\r\n});\r\n```\r\n\r\n**Conditional Uploads (Prevent Overwrites):**\r\n\r\n```typescript\r\n// Only upload if file doesn't exist\r\nconst object = await env.MY_BUCKET.put('file.txt', data, {\r\n  onlyIf: {\r\n    uploadedBefore: new Date('2020-01-01'), // Any date before R2 existed\r\n  },\r\n});\r\n\r\nif (!object) {\r\n  // File already exists, upload prevented\r\n  return c.json({ error: 'File already exists' }, 409);\r\n}\r\n\r\n// Only upload if etag matches (update specific version)\r\nconst object = await env.MY_BUCKET.put('file.txt', data, {\r\n  onlyIf: {\r\n    etagMatches: existingEtag,\r\n  },\r\n});\r\n```\r\n\r\n**With Checksums:**\r\n\r\n```typescript\r\n// R2 will verify the checksum\r\nconst md5Hash = await crypto.subtle.digest('MD5', fileData);\r\n\r\nawait env.MY_BUCKET.put('file.txt', fileData, {\r\n  md5: md5Hash,\r\n});\r\n```\r\n\r\n### get() - Download Objects\r\n\r\n**Signature:**\r\n```typescript\r\nget(key: string, options?: R2GetOptions): Promise<R2ObjectBody | null>\r\n```\r\n\r\n**Basic Usage:**\r\n\r\n```typescript\r\n// Get full object\r\nconst object = await env.MY_BUCKET.get('file.txt');\r\n\r\nif (!object) {\r\n  return c.json({ error: 'Not found' }, 404);\r\n}\r\n\r\n// Return as response\r\nreturn new Response(object.body, {\r\n  headers: {\r\n    'Content-Type': object.httpMetadata?.contentType || 'application/octet-stream',\r\n    'ETag': object.httpEtag,\r\n  },\r\n});\r\n```\r\n\r\n**Read as Different Formats:**\r\n\r\n```typescript\r\nconst object = await env.MY_BUCKET.get('data.json');\r\n\r\nif (object) {\r\n  const text = await object.text();           // As string\r\n  const json = await object.json();           // As JSON object\r\n  const buffer = await object.arrayBuffer();  // As ArrayBuffer\r\n  const blob = await object.blob();           // As Blob\r\n}\r\n```\r\n\r\n**Range Requests (Partial Downloads):**\r\n\r\n```typescript\r\n// Get first 1MB of file\r\nconst object = await env.MY_BUCKET.get('large-file.mp4', {\r\n  range: { offset: 0, length: 1024 * 1024 },\r\n});\r\n\r\n// Get bytes 100-200\r\nconst object = await env.MY_BUCKET.get('file.bin', {\r\n  range: { offset: 100, length: 100 },\r\n});\r\n\r\n// Get from offset to end\r\nconst object = await env.MY_BUCKET.get('file.bin', {\r\n  range: { offset: 1000 },\r\n});\r\n```\r\n\r\n**Conditional Downloads:**\r\n\r\n```typescript\r\n// Only download if etag matches\r\nconst object = await env.MY_BUCKET.get('file.txt', {\r\n  onlyIf: {\r\n    etagMatches: cachedEtag,\r\n  },\r\n});\r\n\r\nif (!object) {\r\n  // Etag didn't match, file was modified\r\n  return c.json({ error: 'File changed' }, 412);\r\n}\r\n```\r\n\r\n### head() - Get Metadata Only\r\n\r\n**Signature:**\r\n```typescript\r\nhead(key: string): Promise<R2Object | null>\r\n```\r\n\r\n**Usage:**\r\n\r\n```typescript\r\n// Get object metadata without downloading body\r\nconst object = await env.MY_BUCKET.head('file.txt');\r\n\r\nif (object) {\r\n  console.log({\r\n    key: object.key,\r\n    size: object.size,\r\n    etag: object.etag,\r\n    uploaded: object.uploaded,\r\n    contentType: object.httpMetadata?.contentType,\r\n    customMetadata: object.customMetadata,\r\n  });\r\n}\r\n```\r\n\r\n**Use Cases:**\r\n- Check if file exists\r\n- Get file size before downloading\r\n- Check last modified date\r\n- Validate etag for caching\r\n\r\n### delete() - Delete Objects\r\n\r\n**Signature:**\r\n```typescript\r\ndelete(key: string | string[]): Promise<void>\r\n```\r\n\r\n**Single Delete:**\r\n\r\n```typescript\r\n// Delete single object\r\nawait env.MY_BUCKET.delete('file.txt');\r\n\r\n// No error if file doesn't exist (idempotent)\r\n```\r\n\r\n**Bulk Delete (Up to 1000 keys):**\r\n\r\n```typescript\r\n// Delete multiple objects at once\r\nconst keysToDelete = [\r\n  'old-file-1.txt',\r\n  'old-file-2.txt',\r\n  'temp/cache-data.json',\r\n];\r\n\r\nawait env.MY_BUCKET.delete(keysToDelete);\r\n\r\n// Much faster than individual deletes\r\n```\r\n\r\n**Delete with Confirmation:**\r\n\r\n```typescript\r\napp.delete('/files/:filename', async (c) => {\r\n  const filename = c.req.param('filename');\r\n\r\n  // Check if exists first\r\n  const exists = await c.env.MY_BUCKET.head(filename);\r\n\r\n  if (!exists) {\r\n    return c.json({ error: 'File not found' }, 404);\r\n  }\r\n\r\n  await c.env.MY_BUCKET.delete(filename);\r\n\r\n  return c.json({ success: true, deleted: filename });\r\n});\r\n```\r\n\r\n### list() - List Objects\r\n\r\n**Signature:**\r\n```typescript\r\nlist(options?: R2ListOptions): Promise<R2Objects>\r\n```\r\n\r\n**Basic Listing:**\r\n\r\n```typescript\r\n// List all objects (up to 1000)\r\nconst listed = await env.MY_BUCKET.list();\r\n\r\nconsole.log({\r\n  objects: listed.objects,      // Array of R2Object\r\n  truncated: listed.truncated,  // true if more results exist\r\n  cursor: listed.cursor,        // For pagination\r\n});\r\n\r\n// Process objects\r\nfor (const object of listed.objects) {\r\n  console.log(`${object.key}: ${object.size} bytes`);\r\n}\r\n```\r\n\r\n**Pagination:**\r\n\r\n```typescript\r\napp.get('/api/files', async (c) => {\r\n  const cursor = c.req.query('cursor');\r\n\r\n  const listed = await c.env.MY_BUCKET.list({\r\n    limit: 100,\r\n    cursor: cursor || undefined,\r\n  });\r\n\r\n  return c.json({\r\n    files: listed.objects.map(obj => ({\r\n      name: obj.key,\r\n      size: obj.size,\r\n      uploaded: obj.uploaded,\r\n      etag: obj.etag,\r\n    })),\r\n    hasMore: listed.truncated,\r\n    nextCursor: listed.cursor,\r\n  });\r\n});\r\n```\r\n\r\n**Prefix Filtering (List by Directory):**\r\n\r\n```typescript\r\n// List all files in 'images/' folder\r\nconst images = await env.MY_BUCKET.list({\r\n  prefix: 'images/',\r\n});\r\n\r\n// List all user files\r\nconst userFiles = await env.MY_BUCKET.list({\r\n  prefix: `users/${userId}/`,\r\n});\r\n```\r\n\r\n**Delimiter (Folder-like Listing):**\r\n\r\n```typescript\r\n// List only top-level items in 'uploads/'\r\nconst listed = await env.MY_BUCKET.list({\r\n  prefix: 'uploads/',\r\n  delimiter: '/',\r\n});\r\n\r\nconsole.log('Files:', listed.objects);          // Files directly in uploads/\r\nconsole.log('Folders:', listed.delimitedPrefixes); // Sub-folders like uploads/2024/\r\n```\r\n\r\n---",
    "Best Practices Summary": "### ‚úÖ Always Do:\r\n\r\n1. **Set appropriate `contentType`** for all uploads\r\n2. **Use batch delete** for multiple objects (up to 1000)\r\n3. **Set cache headers** (`cacheControl`) for static assets\r\n4. **Use presigned URLs** for large client uploads\r\n5. **Use multipart upload** for files > 100MB\r\n6. **Set CORS policy** before allowing browser uploads\r\n7. **Set expiry times** on presigned URLs (1-24 hours)\r\n8. **Handle errors gracefully** with try/catch\r\n9. **Use `head()`** instead of `get()` when you only need metadata\r\n10. **Use conditional operations** to prevent overwrites\r\n\r\n### ‚ùå Never Do:\r\n\r\n1. **Never expose R2 access keys** in client-side code\r\n2. **Never skip `contentType`** (files will download as binary)\r\n3. **Never delete in loops** (use batch delete)\r\n4. **Never upload without error handling**\r\n5. **Never skip CORS** for browser uploads\r\n6. **Never use multipart** for small files (< 5MB)\r\n7. **Never delete >1000 keys** in single call (will fail)\r\n8. **Never assume uploads succeed** (always check response)\r\n9. **Never skip presigned URL expiry** (security risk)\r\n10. **Never hardcode bucket names** (use bindings)\r\n\r\n---",
    "Error Handling": "### Common R2 Errors\r\n\r\n```typescript\r\ntry {\r\n  await env.MY_BUCKET.put(key, data);\r\n} catch (error: any) {\r\n  const message = error.message;\r\n\r\n  if (message.includes('R2_ERROR')) {\r\n    // Generic R2 error\r\n  } else if (message.includes('exceeded')) {\r\n    // Quota exceeded\r\n  } else if (message.includes('precondition')) {\r\n    // Conditional operation failed\r\n  } else if (message.includes('multipart')) {\r\n    // Multipart upload error\r\n  }\r\n\r\n  console.error('R2 Error:', message);\r\n  return c.json({ error: 'Storage operation failed' }, 500);\r\n}\r\n```\r\n\r\n### Retry Logic\r\n\r\n```typescript\r\nasync function r2WithRetry<T>(\r\n  operation: () => Promise<T>,\r\n  maxRetries = 3\r\n): Promise<T> {\r\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\r\n    try {\r\n      return await operation();\r\n    } catch (error: any) {\r\n      const message = error.message;\r\n\r\n      // Retry on transient errors\r\n      const isRetryable =\r\n        message.includes('network') ||\r\n        message.includes('timeout') ||\r\n        message.includes('temporarily unavailable');\r\n\r\n      if (!isRetryable || attempt === maxRetries - 1) {\r\n        throw error;\r\n      }\r\n\r\n      // Exponential backoff\r\n      const delay = Math.min(1000 * Math.pow(2, attempt), 5000);\r\n      await new Promise(resolve => setTimeout(resolve, delay));\r\n    }\r\n  }\r\n\r\n  throw new Error('Retry logic failed');\r\n}\r\n\r\n// Usage\r\nconst object = await r2WithRetry(() =>\r\n  env.MY_BUCKET.get('important-file.txt')\r\n);\r\n```\r\n\r\n---",
    "Wrangler Commands Reference": "wrangler r2 object list <BUCKET_NAME>\r\nwrangler r2 object list <BUCKET_NAME> --prefix=\"folder/\"\r\n```\r\n\r\n---",
    "Known Issues Prevented": "| Issue | Description | How to Avoid |\r\n|-------|-------------|--------------|\r\n| **CORS errors in browser** | Browser can't upload/download due to missing CORS policy | Configure CORS in bucket settings before browser access |\r\n| **Files download as binary** | Missing content-type causes browsers to download files instead of display | Always set `httpMetadata.contentType` on upload |\r\n| **Presigned URL expiry** | URLs never expire, posing security risk | Always set `X-Amz-Expires` (1-24 hours typical) |\r\n| **Multipart upload limits** | Parts exceed 100MB or >10,000 parts | Keep parts 5MB-100MB, max 10,000 parts per upload |\r\n| **Bulk delete limits** | Trying to delete >1000 keys fails | Chunk deletes into batches of 1000 |\r\n| **Custom metadata overflow** | Metadata exceeds 2KB limit | Keep custom metadata under 2KB total |\r\n\r\n---",
    "CORS Configuration": "Configure CORS to allow browser requests to your R2 bucket.\r\n\r\n### Public Bucket CORS\r\n\r\n```json\r\n{\r\n  \"CORSRules\": [\r\n    {\r\n      \"AllowedOrigins\": [\"https://example.com\"],\r\n      \"AllowedMethods\": [\"GET\", \"HEAD\"],\r\n      \"AllowedHeaders\": [\"*\"],\r\n      \"MaxAgeSeconds\": 3600\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Allow All Origins (Public Assets)\r\n\r\n```json\r\n{\r\n  \"CORSRules\": [\r\n    {\r\n      \"AllowedOrigins\": [\"*\"],\r\n      \"AllowedMethods\": [\"GET\", \"HEAD\"],\r\n      \"AllowedHeaders\": [\"Range\"],\r\n      \"MaxAgeSeconds\": 3600\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Upload with CORS\r\n\r\n```json\r\n{\r\n  \"CORSRules\": [\r\n    {\r\n      \"AllowedOrigins\": [\"https://app.example.com\"],\r\n      \"AllowedMethods\": [\"GET\", \"PUT\", \"POST\", \"DELETE\", \"HEAD\"],\r\n      \"AllowedHeaders\": [\r\n        \"Content-Type\",\r\n        \"Content-MD5\",\r\n        \"x-amz-meta-*\"\r\n      ],\r\n      \"ExposeHeaders\": [\"ETag\"],\r\n      \"MaxAgeSeconds\": 3600\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Apply CORS via Dashboard\r\n\r\n1. Go to Cloudflare Dashboard ‚Üí R2\r\n2. Select your bucket\r\n3. Go to Settings tab\r\n4. Under CORS Policy ‚Üí Add CORS policy\r\n5. Paste JSON configuration\r\n6. Save\r\n\r\n### CORS for Presigned URLs\r\n\r\nWhen using presigned URLs, CORS is handled by R2 directly. Configure CORS on the bucket, not in your Worker.\r\n\r\n---",
    "Official Documentation": "- **R2 Overview**: https://developers.cloudflare.com/r2/\r\n- **Get Started**: https://developers.cloudflare.com/r2/get-started/\r\n- **Workers API**: https://developers.cloudflare.com/r2/api/workers/workers-api-reference/\r\n- **Multipart Upload**: https://developers.cloudflare.com/r2/api/workers/workers-multipart-usage/\r\n- **Presigned URLs**: https://developers.cloudflare.com/r2/api/s3/presigned-urls/\r\n- **CORS Configuration**: https://developers.cloudflare.com/r2/buckets/cors/\r\n- **Public Buckets**: https://developers.cloudflare.com/r2/buckets/public-buckets/\r\n\r\n---\r\n\r\n**Ready to store with R2!** üöÄ",
    "HTTP Metadata": "### Content-Type\r\n\r\n```typescript\r\n// Set content type on upload\r\nawait env.MY_BUCKET.put('image.jpg', imageData, {\r\n  httpMetadata: {\r\n    contentType: 'image/jpeg',\r\n  },\r\n});\r\n\r\n// Will be returned in Content-Type header when downloaded\r\n```\r\n\r\n### Cache-Control\r\n\r\n```typescript\r\n// Set caching headers\r\nawait env.MY_BUCKET.put('static/logo.png', logoData, {\r\n  httpMetadata: {\r\n    contentType: 'image/png',\r\n    cacheControl: 'public, max-age=31536000, immutable',\r\n  },\r\n});\r\n\r\n// For frequently updated content\r\nawait env.MY_BUCKET.put('api/data.json', jsonData, {\r\n  httpMetadata: {\r\n    contentType: 'application/json',\r\n    cacheControl: 'public, max-age=60, must-revalidate',\r\n  },\r\n});\r\n```\r\n\r\n### Content-Disposition\r\n\r\n```typescript\r\n// Force download with specific filename\r\nawait env.MY_BUCKET.put('report.pdf', pdfData, {\r\n  httpMetadata: {\r\n    contentType: 'application/pdf',\r\n    contentDisposition: 'attachment; filename=\"monthly-report.pdf\"',\r\n  },\r\n});\r\n\r\n// Display inline\r\nawait env.MY_BUCKET.put('image.jpg', imageData, {\r\n  httpMetadata: {\r\n    contentType: 'image/jpeg',\r\n    contentDisposition: 'inline',\r\n  },\r\n});\r\n```\r\n\r\n### Content-Encoding\r\n\r\n```typescript\r\n// Indicate gzip compression\r\nawait env.MY_BUCKET.put('data.json.gz', gzippedData, {\r\n  httpMetadata: {\r\n    contentType: 'application/json',\r\n    contentEncoding: 'gzip',\r\n  },\r\n});\r\n```\r\n\r\n---",
    "Multipart Uploads": "For files larger than 100MB or for resumable uploads, use multipart upload API.\r\n\r\n### When to Use Multipart Upload\r\n\r\n‚úÖ **Use multipart when:**\r\n- File size > 100MB\r\n- Need resumable uploads\r\n- Want to parallelize upload\r\n- Uploading from browser/client\r\n\r\n‚ùå **Don't use multipart when:**\r\n- File size < 5MB (overhead not worth it)\r\n- Uploading within Worker (use direct put())\r\n\r\n### Basic Multipart Upload Flow\r\n\r\n```typescript\r\napp.post('/api/upload/start', async (c) => {\r\n  const { filename } = await c.req.json();\r\n\r\n  // Create multipart upload\r\n  const multipart = await c.env.MY_BUCKET.createMultipartUpload(filename, {\r\n    httpMetadata: {\r\n      contentType: 'application/octet-stream',\r\n    },\r\n  });\r\n\r\n  return c.json({\r\n    key: multipart.key,\r\n    uploadId: multipart.uploadId,\r\n  });\r\n});\r\n\r\napp.put('/api/upload/part', async (c) => {\r\n  const { key, uploadId, partNumber } = await c.req.json();\r\n  const body = await c.req.arrayBuffer();\r\n\r\n  // Resume the multipart upload\r\n  const multipart = c.env.MY_BUCKET.resumeMultipartUpload(key, uploadId);\r\n\r\n  // Upload a part\r\n  const uploadedPart = await multipart.uploadPart(partNumber, body);\r\n\r\n  return c.json({\r\n    partNumber: uploadedPart.partNumber,\r\n    etag: uploadedPart.etag,\r\n  });\r\n});\r\n\r\napp.post('/api/upload/complete', async (c) => {\r\n  const { key, uploadId, parts } = await c.req.json();\r\n\r\n  // Resume the multipart upload\r\n  const multipart = c.env.MY_BUCKET.resumeMultipartUpload(key, uploadId);\r\n\r\n  // Complete the upload\r\n  const object = await multipart.complete(parts);\r\n\r\n  return c.json({\r\n    success: true,\r\n    key: object.key,\r\n    size: object.size,\r\n    etag: object.etag,\r\n  });\r\n});\r\n\r\napp.post('/api/upload/abort', async (c) => {\r\n  const { key, uploadId } = await c.req.json();\r\n\r\n  const multipart = c.env.MY_BUCKET.resumeMultipartUpload(key, uploadId);\r\n  await multipart.abort();\r\n\r\n  return c.json({ success: true });\r\n});\r\n```\r\n\r\n### Complete Multipart Upload Example\r\n\r\n```typescript\r\n// Full Worker implementing multipart upload API\r\ninterface Env {\r\n  MY_BUCKET: R2Bucket;\r\n}\r\n\r\nexport default {\r\n  async fetch(request: Request, env: Env): Promise<Response> {\r\n    const url = new URL(request.url);\r\n    const key = url.pathname.slice(1);\r\n    const action = url.searchParams.get('action');\r\n\r\n    switch (request.method) {\r\n      case 'POST': {\r\n        switch (action) {\r\n          case 'mpu-create': {\r\n            // Create multipart upload\r\n            const multipart = await env.MY_BUCKET.createMultipartUpload(key);\r\n            return Response.json({\r\n              key: multipart.key,\r\n              uploadId: multipart.uploadId,\r\n            });\r\n          }\r\n          case 'mpu-complete': {\r\n            // Complete multipart upload\r\n            const uploadId = url.searchParams.get('uploadId')!;\r\n            const multipart = env.MY_BUCKET.resumeMultipartUpload(key, uploadId);\r\n            const parts = await request.json();\r\n            const object = await multipart.complete(parts);\r\n            return Response.json({\r\n              key: object.key,\r\n              etag: object.etag,\r\n              size: object.size,\r\n            });\r\n          }\r\n          default:\r\n            return new Response(`Unknown action: ${action}`, { status: 400 });\r\n        }\r\n      }\r\n      case 'PUT': {\r\n        switch (action) {\r\n          case 'mpu-uploadpart': {\r\n            // Upload a part\r\n            const uploadId = url.searchParams.get('uploadId')!;\r\n            const partNumber = parseInt(url.searchParams.get('partNumber')!);\r\n            const multipart = env.MY_BUCKET.resumeMultipartUpload(key, uploadId);\r\n            const uploadedPart = await multipart.uploadPart(partNumber, request.body!);\r\n            return Response.json({\r\n              partNumber: uploadedPart.partNumber,\r\n              etag: uploadedPart.etag,\r\n            });\r\n          }\r\n          default:\r\n            return new Response(`Unknown action: ${action}`, { status: 400 });\r\n        }\r\n      }\r\n      case 'DELETE': {\r\n        switch (action) {\r\n          case 'mpu-abort': {\r\n            // Abort multipart upload\r\n            const uploadId = url.searchParams.get('uploadId')!;\r\n            const multipart = env.MY_BUCKET.resumeMultipartUpload(key, uploadId);\r\n            await multipart.abort();\r\n            return new Response(null, { status: 204 });\r\n          }\r\n          default:\r\n            return new Response(`Unknown action: ${action}`, { status: 400 });\r\n        }\r\n      }\r\n      default:\r\n        return new Response('Method Not Allowed', { status: 405 });\r\n    }\r\n  },\r\n} satisfies ExportedHandler<Env>;\r\n```\r\n\r\n---"
  }
}